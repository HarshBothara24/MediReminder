{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import json\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset class\n",
    "class MedReminderDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_file)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_file)[0] + \".json\")\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Load or extract labels\n",
    "        with open(label_path, \"r\") as f:\n",
    "            labels = json.load(f)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_prescription_details(image_path):\n",
    "        \"\"\"\n",
    "        Extract text from the image using OCR (pytesseract).\n",
    "        \"\"\"\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_prescription_details(extracted_text):\n",
    "        \"\"\"\n",
    "        Parse extracted text to identify medicines and syrups with their details.\n",
    "        Uses regex for improved accuracy.\n",
    "        \"\"\"\n",
    "        medicines = []\n",
    "        syrups = []\n",
    "        \n",
    "        # Define regex patterns\n",
    "        dosage_pattern = r\"(\\d+(\\.\\d+)?\\s?(mg|g|ml|mcg|unit))\"\n",
    "        frequency_pattern = r\"(\\d+\\s*(x|times)?\\s*(per\\s*day|daily|once|twice|\\d+\\s*times))\"\n",
    "        duration_pattern = r\"(\\d+\\s*(days?|weeks?|months?))\"\n",
    "        \n",
    "        # Medicine and syrup keywords (using regex)\n",
    "        medicine_keywords = r\"(tablet|tab|cap|capsule|pill|medicine)\"\n",
    "        syrup_keywords = r\"(syrup|syp|liquid)\"\n",
    "        \n",
    "        lines = extracted_text.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip lines with irrelevant information\n",
    "            if any(irrelevant in line.lower() for irrelevant in [\"dr.\", \"address\", \"phone\", \"signature\"]):\n",
    "                continue\n",
    "\n",
    "            # Check for medicines using regex\n",
    "            if re.search(medicine_keywords, line.lower()):\n",
    "                medicine = MedReminderDataset.parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, \"medicine\")\n",
    "                if medicine:\n",
    "                    medicines.append(medicine)\n",
    "            \n",
    "            # Check for syrups using regex\n",
    "            elif re.search(syrup_keywords, line.lower()):\n",
    "                syrup = MedReminderDataset.parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, \"syrup\")\n",
    "                if syrup:\n",
    "                    syrups.append(syrup)\n",
    "\n",
    "        return medicines, syrups\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, type_of_med):\n",
    "        \"\"\"\n",
    "        Parse a single line to extract details like name, dosage, frequency, and duration using regex.\n",
    "        Type of medicine or syrup is passed as an argument ('medicine' or 'syrup').\n",
    "        \"\"\"\n",
    "        details = {\"name\": \"\", \"dosage\": \"\", \"frequency\": \"\", \"duration\": \"\", \"type\": type_of_med}\n",
    "        \n",
    "        # Extract the name (first word in the line as name assumption)\n",
    "        tokens = line.split()\n",
    "        if len(tokens) > 0:\n",
    "            details[\"name\"] = tokens[0]  # First token is typically the name\n",
    "\n",
    "        # Use regex to extract dosage, frequency, and duration\n",
    "        dosage_match = re.search(dosage_pattern, line)\n",
    "        if dosage_match:\n",
    "            details[\"dosage\"] = dosage_match.group(0)\n",
    "\n",
    "        frequency_match = re.search(frequency_pattern, line)\n",
    "        if frequency_match:\n",
    "            details[\"frequency\"] = frequency_match.group(0)\n",
    "\n",
    "        duration_match = re.search(duration_pattern, line)\n",
    "        if duration_match:\n",
    "            details[\"duration\"] = duration_match.group(0)\n",
    "\n",
    "        # If the line contains relevant information, return the details\n",
    "        if any(details[key] != \"\" for key in details):\n",
    "            return details\n",
    "        return None\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def custom_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack all images\n",
    "    labels = [item[1] for item in batch]  # Keep labels as-is (list of dictionaries)\n",
    "    return images, labels\n",
    "\n",
    "# Model definition\n",
    "class MedReminderModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MedReminderModel, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Loss calculation (customized for dictionary labels)\n",
    "def calculate_loss(outputs, labels, criterion):\n",
    "    total_loss = 0.0\n",
    "    for i, label_dict in enumerate(labels):\n",
    "        medicines = label_dict.get(\"medicines\", [])\n",
    "        syrups = label_dict.get(\"syrups\", [])\n",
    "        loss = criterion(outputs[i], torch.tensor(len(medicines) + len(syrups), dtype=torch.float32))\n",
    "        total_loss += loss\n",
    "    return total_loss / len(labels)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    image_dir = \"dataset/train/resized_images\"\n",
    "    label_dir = \"dataset/train/labels\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 30\n",
    "    learning_rate = 0.001\n",
    "    num_classes = 3  # Adjust based on your classification categories\n",
    "\n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    dataset = MedReminderDataset(image_dir, label_dir, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    # Model, criterion, and optimizer\n",
    "    model = MedReminderModel(num_classes=num_classes)\n",
    "    criterion = nn.MSELoss()  # Example criterion; adapt for your specific use case\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = calculate_loss(outputs, labels, criterion)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"medi_Rem.pth\")\n",
    "    print(\"Model saved as medi_Rem.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   2%|▏         | 96/5000 [04:21<3:42:47,  2.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 433\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 378\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# Create dataset with caching\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 378\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPrescriptionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/train/resized_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mocr_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True to force OCR reprocessing\u001b[39;49;00m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# Split dataset\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "Cell \u001b[1;32mIn[9], line 51\u001b[0m, in \u001b[0;36mPrescriptionDataset.__init__\u001b[1;34m(self, image_dir, cache_dir, transform, force_reload)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reload \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_cache():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mocr \u001b[38;5;241m=\u001b[39m PaddleOCR(use_angle_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     50\u001b[0m                         use_gpu\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(), show_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_all_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Load cached results\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cache()\n",
      "Cell \u001b[1;32mIn[9], line 85\u001b[0m, in \u001b[0;36mPrescriptionDataset._preprocess_all_images\u001b[1;34m(self, force_reload)\u001b[0m\n\u001b[0;32m     79\u001b[0m thresh \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39madaptiveThreshold(\n\u001b[0;32m     80\u001b[0m     gray, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mADAPTIVE_THRESH_GAUSSIAN_C, \n\u001b[0;32m     81\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Run OCR and process results\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_ocr_result(result)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Cache results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\paddleocr.py:766\u001b[0m, in \u001b[0;36mPaddleOCR.ocr\u001b[1;34m(self, img, det, rec, cls, bin, inv, alpha_color, slice)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[0;32m    765\u001b[0m     img \u001b[38;5;241m=\u001b[39m preprocess_image(img)\n\u001b[1;32m--> 766\u001b[0m     dt_boxes, rec_res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dt_boxes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rec_res:\n\u001b[0;32m    768\u001b[0m         ocr_res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\tools\\infer\\predict_system.py:144\u001b[0m, in \u001b[0;36mTextSystem.__call__\u001b[1;34m(self, img, cls, slice)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img_crop_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m    140\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec crops num: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(img_crop_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time and memory cost may be large.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m     )\n\u001b[1;32m--> 144\u001b[0m rec_res, elapse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_recognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_crop_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m time_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m elapse\n\u001b[0;32m    146\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec_res num  : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, elapsed : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(rec_res), elapse))\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\tools\\infer\\predict_rec.py:796\u001b[0m, in \u001b[0;36mTextRecognizer.__call__\u001b[1;34m(self, img_list)\u001b[0m\n\u001b[0;32m    794\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output_tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_tensors:\n\u001b[1;32m--> 796\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_to_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenchmark:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from paddleocr import PaddleOCR\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('prescription_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Constants\n",
    "MEDICINE_CATEGORIES = ['tablet', 'capsule', 'syrup', 'injection', 'drops', 'cream', 'ointment']\n",
    "NUM_CLASSES = len(MEDICINE_CATEGORIES)\n",
    "\n",
    "class PrescriptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, cache_dir='ocr_cache', transform=None, force_reload=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Create cache directory\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize OCR if needed\n",
    "        if force_reload or not self._check_cache():\n",
    "            self.ocr = PaddleOCR(use_angle_cls=True, lang='en', \n",
    "                                use_gpu=torch.cuda.is_available(), show_log=False)\n",
    "            self._preprocess_all_images(force_reload)\n",
    "        \n",
    "        # Load cached results\n",
    "        self.cached_results = self._load_cache()\n",
    "        logger.info(f\"Loaded {len(self.image_files)} images with cached OCR results\")\n",
    "\n",
    "    def _check_cache(self):\n",
    "        \"\"\"Check if cache exists for all images\"\"\"\n",
    "        return all(os.path.exists(os.path.join(self.cache_dir, f\"{img}.pkl\")) \n",
    "                  for img in self.image_files)\n",
    "\n",
    "    def _preprocess_all_images(self, force_reload):\n",
    "        \"\"\"Preprocess all images and cache OCR results\"\"\"\n",
    "        logger.info(\"Pre-processing images and caching OCR results...\")\n",
    "        for img_file in tqdm(self.image_files, desc=\"Processing images\"):\n",
    "            cache_file = os.path.join(self.cache_dir, f\"{img_file}.pkl\")\n",
    "            \n",
    "            if force_reload or not os.path.exists(cache_file):\n",
    "                try:\n",
    "                    # Process image\n",
    "                    img_path = os.path.join(self.image_dir, img_file)\n",
    "                    image = cv2.imread(img_path)\n",
    "                    if image is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Resize and preprocess\n",
    "                    image = cv2.resize(image, (800, 1000))\n",
    "                    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                    thresh = cv2.adaptiveThreshold(\n",
    "                        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                        cv2.THRESH_BINARY, 11, 2\n",
    "                    )\n",
    "                    \n",
    "                    # Run OCR and process results\n",
    "                    result = self.ocr.ocr(thresh)\n",
    "                    processed_data = self._process_ocr_result(result)\n",
    "                    \n",
    "                    # Cache results\n",
    "                    with open(cache_file, 'wb') as f:\n",
    "                        pickle.dump(processed_data, f)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {img_file}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    def _process_ocr_result(self, result):\n",
    "        \"\"\"Process OCR result into structured data\"\"\"\n",
    "        processed_data = {\n",
    "            'text': [],\n",
    "            'medicine_types': set(),\n",
    "            'confidences': []\n",
    "        }\n",
    "        \n",
    "        for line in result:\n",
    "            for word_info in line:\n",
    "                text = word_info[1][0]\n",
    "                confidence = word_info[1][1]\n",
    "                \n",
    "                if confidence > 0.5:\n",
    "                    processed_data['text'].append(text)\n",
    "                    processed_data['confidences'].append(confidence)\n",
    "                    \n",
    "                    # Check for medicine types\n",
    "                    text_lower = text.lower()\n",
    "                    for category in MEDICINE_CATEGORIES:\n",
    "                        if category in text_lower:\n",
    "                            processed_data['medicine_types'].add(category)\n",
    "        \n",
    "        processed_data['medicine_types'] = list(processed_data['medicine_types'])\n",
    "        processed_data['text'] = ' '.join(processed_data['text'])\n",
    "        return processed_data\n",
    "\n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load all cached OCR results\"\"\"\n",
    "        cached_results = {}\n",
    "        for img_file in self.image_files:\n",
    "            cache_file = os.path.join(self.cache_dir, f\"{img_file}.pkl\")\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    cached_results[img_file] = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading cache for {img_file}: {str(e)}\")\n",
    "                cached_results[img_file] = {\n",
    "                    'text': '',\n",
    "                    'medicine_types': [],\n",
    "                    'confidences': []\n",
    "                }\n",
    "        return cached_results\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "        \n",
    "        try:\n",
    "            # Load and transform image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {img_file}: {str(e)}\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "        \n",
    "        # Get cached OCR result\n",
    "        ocr_data = self.cached_results.get(img_file, {\n",
    "            'text': '',\n",
    "            'medicine_types': [],\n",
    "            'confidences': []\n",
    "        })\n",
    "        \n",
    "        # Create label tensor\n",
    "        label = torch.zeros(NUM_CLASSES)\n",
    "        for med_type in ocr_data['medicine_types']:\n",
    "            if med_type in MEDICINE_CATEGORIES:\n",
    "                label[MEDICINE_CATEGORIES.index(med_type)] = 1\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': ocr_data['text'],\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "class PrescriptionModel(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Image feature extraction (ResNet18)\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove final FC layer\n",
    "        \n",
    "        # Additional layers for classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.resnet(x)\n",
    "        # Classify\n",
    "        return self.classifier(features)\n",
    "\n",
    "class PrescriptionTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, \n",
    "                 learning_rate=1e-4, weight_decay=1e-5):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_path = 'best_model.pth'\n",
    "        \n",
    "        # Create output directory for results\n",
    "        self.output_dir = f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize metrics tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_f1_scores = []\n",
    "        self.val_f1_scores = []\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch in pbar:\n",
    "            # Get data\n",
    "            images = batch['image'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(outputs.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        f1 = self._calculate_f1_score(np.array(all_preds), np.array(all_labels))\n",
    "        \n",
    "        return avg_loss, f1\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc='Validating'):\n",
    "                images = batch['image'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        f1 = self._calculate_f1_score(np.array(all_preds), np.array(all_labels))\n",
    "        \n",
    "        return avg_loss, f1\n",
    "\n",
    "    def _calculate_f1_score(self, predictions, labels, threshold=0.5):\n",
    "        predictions = (predictions > threshold).astype(int)\n",
    "        return f1_score(labels, predictions, average='macro')\n",
    "\n",
    "    def save_checkpoint(self, val_loss, epoch):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(self.output_dir, self.best_model_path))\n",
    "\n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save training metrics\"\"\"\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'train_loss': self.train_losses,\n",
    "            'val_loss': self.val_losses,\n",
    "            'train_f1': self.train_f1_scores,\n",
    "            'val_f1': self.val_f1_scores\n",
    "        })\n",
    "        metrics_df.to_csv(os.path.join(self.output_dir, 'training_metrics.csv'))\n",
    "\n",
    "    def train(self, num_epochs=30, patience=5):\n",
    "        logger.info(\"Starting training...\")\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_f1 = self.train_epoch()\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_f1_scores.append(train_f1)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_f1 = self.validate()\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_f1_scores.append(val_f1)\n",
    "            \n",
    "            # Log metrics\n",
    "            logger.info(\n",
    "                f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}\\n\"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\"\n",
    "            )\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                logger.info(\"Saving best model...\")\n",
    "                self.best_val_loss = val_loss\n",
    "                self.save_checkpoint(val_loss, epoch)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "            \n",
    "            # Save metrics after each epoch\n",
    "            self.save_metrics()\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # Create dataset with caching\n",
    "        logger.info(\"Creating dataset...\")\n",
    "        dataset = PrescriptionDataset(\n",
    "            image_dir=\"dataset/train/resized_images\",\n",
    "            cache_dir=\"ocr_cache\",\n",
    "            transform=transform,\n",
    "            force_reload=False  # Set to True to force OCR reprocessing\n",
    "        )\n",
    "\n",
    "        # Split dataset\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Train size: {train_size}, Validation size: {val_size}\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        logger.info(\"Initializing model...\")\n",
    "        model = PrescriptionModel().to(device)\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = PrescriptionTrainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        trainer.train()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   2%|▏         | 82/5000 [03:23<3:23:46,  2.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 603\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 603\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 554\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    546\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    547\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m    548\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m    549\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], \n\u001b[0;32m    550\u001b[0m                        std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m    551\u001b[0m ])\n\u001b[0;32m    553\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 554\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPrescriptionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/train/resized_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mocr_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True to regenerate labels\u001b[39;49;00m\n\u001b[0;32m    559\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded successfully. Total samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Split dataset\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 59\u001b[0m, in \u001b[0;36mPrescriptionDataset.__init__\u001b[1;34m(self, image_dir, cache_dir, transform, force_reload)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reload \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_cache_exists():\n\u001b[0;32m     58\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing images and caching results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Load cached results\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cached_results()\n",
      "Cell \u001b[1;32mIn[1], line 91\u001b[0m, in \u001b[0;36mPrescriptionDataset._preprocess_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# Process image\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, img_file)\n\u001b[1;32m---> 91\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Cache result\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     cache_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(img_file)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 143\u001b[0m, in \u001b[0;36mPrescriptionDataset._process_single_image\u001b[1;34m(self, image_path)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Perform OCR with correct parameters\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    145\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOCR failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\paddleocr.py:766\u001b[0m, in \u001b[0;36mPaddleOCR.ocr\u001b[1;34m(self, img, det, rec, cls, bin, inv, alpha_color, slice)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[0;32m    765\u001b[0m     img \u001b[38;5;241m=\u001b[39m preprocess_image(img)\n\u001b[1;32m--> 766\u001b[0m     dt_boxes, rec_res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dt_boxes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rec_res:\n\u001b[0;32m    768\u001b[0m         ocr_res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\tools\\infer\\predict_system.py:144\u001b[0m, in \u001b[0;36mTextSystem.__call__\u001b[1;34m(self, img, cls, slice)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img_crop_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m    140\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec crops num: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(img_crop_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time and memory cost may be large.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m     )\n\u001b[1;32m--> 144\u001b[0m rec_res, elapse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_recognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_crop_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m time_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m elapse\n\u001b[0;32m    146\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec_res num  : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, elapsed : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(rec_res), elapse))\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\tools\\infer\\predict_rec.py:796\u001b[0m, in \u001b[0;36mTextRecognizer.__call__\u001b[1;34m(self, img_list)\u001b[0m\n\u001b[0;32m    794\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output_tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_tensors:\n\u001b[1;32m--> 796\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_to_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenchmark:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from paddleocr import PaddleOCR\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('prescription_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class PrescriptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, cache_dir, transform=None, force_reload=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.transform = transform\n",
    "        self.force_reload = force_reload\n",
    "        \n",
    "        # Initialize OCR with correct parameters\n",
    "        self.ocr = PaddleOCR(\n",
    "            use_angle_cls=False,\n",
    "            lang='en',\n",
    "            show_log=False,\n",
    "            use_gpu=True,  # Use GPU if available\n",
    "            enable_mkldnn=True,  # Enable MKL-DNN acceleration\n",
    "            cpu_threads=4  # Limit CPU threads\n",
    "        )\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Get list of image files\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Process images and cache results\n",
    "        if force_reload or not self._check_cache_exists():\n",
    "            logger.info(\"Processing images and caching results...\")\n",
    "            self._preprocess_images()\n",
    "        \n",
    "        # Load cached results\n",
    "        self.cached_results = self._load_cached_results()\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        logger.info(\"\\nDataset Statistics:\")\n",
    "        logger.info(f\"Total images: {len(self.image_files)}\")\n",
    "        logger.info(f\"Cached results: {len(self.cached_results)}\")\n",
    "        \n",
    "        # Print sample label distribution\n",
    "        labels = [result['label'] for result in self.cached_results.values()]\n",
    "        labels = np.array(labels)\n",
    "        logger.info(\"\\nLabel Distribution:\")\n",
    "        for i in range(labels.shape[1]):\n",
    "            positive_count = np.sum(labels[:, i] == 1)\n",
    "            logger.info(f\"Class {i}: {positive_count} positive samples ({positive_count/len(labels)*100:.2f}%)\")\n",
    "\n",
    "    def _check_cache_exists(self):\n",
    "        return all(\n",
    "            os.path.exists(os.path.join(self.cache_dir, f\"{os.path.splitext(img)[0]}.json\"))\n",
    "            for img in self.image_files\n",
    "        )\n",
    "\n",
    "    def _preprocess_images(self):\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for img_file in tqdm(self.image_files, desc=\"Processing images\"):\n",
    "            try:\n",
    "                # Process image\n",
    "                image_path = os.path.join(self.image_dir, img_file)\n",
    "                result = self._process_single_image(image_path)\n",
    "                \n",
    "                # Cache result\n",
    "                cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "                with open(cache_path, 'w') as f:\n",
    "                    json.dump(result, f)\n",
    "                \n",
    "                successful += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {img_file}: {str(e)}\")\n",
    "                failed += 1\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"\\nProcessing complete:\")\n",
    "        logger.info(f\"Successfully processed: {successful}\")\n",
    "        logger.info(f\"Failed to process: {failed}\")\n",
    "        logger.info(f\"Success rate: {(successful/len(self.image_files))*100:.2f}%\")\n",
    "\n",
    "    def _process_single_image(self, image_path):\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                logger.warning(f\"Could not load image: {image_path}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "                \n",
    "            # Resize image to a larger size for better text detection\n",
    "            image = cv2.resize(image, (800, 800))\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply adaptive histogram equalization\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            gray = clahe.apply(gray)\n",
    "            \n",
    "            # Apply Gaussian blur to reduce noise\n",
    "            blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "            \n",
    "            # Apply adaptive thresholding\n",
    "            binary = cv2.adaptiveThreshold(\n",
    "                blurred,\n",
    "                255,\n",
    "                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                cv2.THRESH_BINARY,\n",
    "                11,\n",
    "                2\n",
    "            )\n",
    "            \n",
    "            # Perform OCR with correct parameters\n",
    "            try:\n",
    "                result = self.ocr.ocr(binary, cls=False)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"OCR failed for {image_path}: {str(e)}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "            \n",
    "            # Handle None or empty results\n",
    "            if result is None or len(result) == 0 or not result[0]:\n",
    "                logger.warning(f\"No OCR results for {image_path}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "                \n",
    "            # Extract text and confidence with validation\n",
    "            texts = []\n",
    "            confidences = []\n",
    "            try:\n",
    "                for line in result[0]:\n",
    "                    if isinstance(line, (list, tuple)) and len(line) >= 2:\n",
    "                        text = line[1][0] if isinstance(line[1], (list, tuple)) else str(line[1])\n",
    "                        confidence = line[1][1] if isinstance(line[1], (list, tuple)) and len(line[1]) > 1 else 0.0\n",
    "                        texts.append(text)\n",
    "                        confidences.append(confidence)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error extracting text from OCR result for {image_path}: {str(e)}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "            \n",
    "            # Generate labels based on OCR text\n",
    "            label = self._generate_labels(texts)\n",
    "            \n",
    "            return {\n",
    "                'text': texts,\n",
    "                'confidence': confidences,\n",
    "                'label': label\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return {'text': [], 'label': [0] * 7}\n",
    "\n",
    "    def _normalize_text(self, text):\n",
    "        \"\"\"Normalize text for better matching\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove common prefixes and suffixes\n",
    "        prefixes = ['tab', 'tablet','TAB','Tab','Cap','CAP', 'cap', 'capsule', 'syrup', 'SYR','Syrup','Syr','suspension', 'INJ','Inj','inj', 'injection']\n",
    "        for prefix in prefixes:\n",
    "            if text.startswith(prefix + ' '):\n",
    "                text = text[len(prefix):].strip()\n",
    "        \n",
    "        # Remove special characters and extra spaces\n",
    "        text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def _generate_labels(self, texts):\n",
    "        # Initialize label vector\n",
    "        label = [0] * 7\n",
    "        \n",
    "        # Common medication form indicators with variations\n",
    "        forms = {\n",
    "            0: {  # Tablets\n",
    "                'keywords': ['tablet','Tablet', 'Tab','TAB','tab','PILL','Pill', 'pill'],\n",
    "                'variations': ['tablets', 'tabs', 'pills']\n",
    "            },\n",
    "            1: {  # Capsules\n",
    "                'keywords': ['capsule','Capsule','CAP','Cap', 'cap'],\n",
    "                'variations': ['capsules', 'caps']\n",
    "            },\n",
    "            2: {  # Syrups\n",
    "                'keywords': ['syrup','Syrup','SYR','Syr', 'suspension'],\n",
    "                'variations': ['syrups', 'suspensions']\n",
    "            },\n",
    "            3: {  # Injections\n",
    "                'keywords': ['injection','Injection','INJECTION','INJ', 'inj'],\n",
    "                'variations': ['injections', 'injs']\n",
    "            },\n",
    "            4: {  # Drops\n",
    "                'keywords': ['drops', 'eye drops'],\n",
    "                'variations': ['drop', 'eyedrops']\n",
    "            },\n",
    "            5: {  # Topical\n",
    "                'keywords': ['cream', 'ointment'],\n",
    "                'variations': ['creams', 'ointments']\n",
    "            },\n",
    "            6: {  # Inhalers/Sprays\n",
    "                'keywords': ['inhaler', 'spray'],\n",
    "                'variations': ['inhalers', 'sprays']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Process each text line\n",
    "        for text in texts:\n",
    "            # Normalize text\n",
    "            normalized_text = self._normalize_text(text)\n",
    "            \n",
    "            # Check for medication forms\n",
    "            for form_id, form_info in forms.items():\n",
    "                # Check main keywords\n",
    "                if any(keyword in normalized_text for keyword in form_info['keywords']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "                \n",
    "                # Check variations\n",
    "                if any(variation in normalized_text for variation in form_info['variations']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "                \n",
    "                # Check for exact matches (case-insensitive)\n",
    "                if any(keyword.lower() in text.lower() for keyword in form_info['keywords']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "                \n",
    "                # Check for variations with exact matches\n",
    "                if any(variation.lower() in text.lower() for variation in form_info['variations']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "        \n",
    "        return label\n",
    "\n",
    "    def _load_cached_results(self):\n",
    "        cached_results = {}\n",
    "        for img_file in self.image_files:\n",
    "            cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "            try:\n",
    "                with open(cache_path, 'r') as f:\n",
    "                    cached_results[img_file] = json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading cache for {img_file}: {str(e)}\")\n",
    "                continue\n",
    "        return cached_results\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, img_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get cached result\n",
    "        result = self.cached_results[img_file]\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(result['label'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class PrescriptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use a smaller model initially\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify the final layers\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 7),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class PrescriptionTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, \n",
    "                 learning_rate=1e-3):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=learning_rate,\n",
    "            epochs=30,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.3,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.output_dir = f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        logger.info(f\"\\nStarting training epoch...\")\n",
    "        logger.info(f\"Number of batches: {len(self.train_loader)}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            try:\n",
    "                if batch_idx % 10 == 0:\n",
    "                    logger.info(f\"Processing batch {batch_idx + 1}/{len(self.train_loader)}\")\n",
    "                \n",
    "                images = batch['image'].to(self.device, non_blocking=True)\n",
    "                targets = batch['label'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    logger.info(f\"Input shape: {images.shape}\")\n",
    "                    logger.info(f\"Target shape: {targets.shape}\")\n",
    "                    logger.info(f\"Sample target values: {targets[0]}\")\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                total_loss += loss.item()\n",
    "                pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "                predictions.extend(pred_probs)\n",
    "                labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    logger.info(f\"Current loss: {loss.item():.4f}, LR: {self.scheduler.get_last_lr()[0]:.6f}\")\n",
    "                    logger.info(f\"Sample predictions: {pred_probs[0]}\")\n",
    "                    logger.info(f\"Sample targets: {targets[0].cpu().numpy()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Debug information\n",
    "        logger.info(\"\\nDebug Information:\")\n",
    "        logger.info(f\"Predictions shape: {predictions.shape}\")\n",
    "        logger.info(f\"Labels shape: {labels.shape}\")\n",
    "        logger.info(f\"Predictions range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "        logger.info(f\"Labels range: [{labels.min():.4f}, {labels.max():.4f}]\")\n",
    "        logger.info(f\"Unique values in labels: {np.unique(labels)}\")\n",
    "        \n",
    "        f1 = self._calculate_f1_score(predictions, labels)\n",
    "        logger.info(f\"Calculated F1 score: {f1:.4f}\")\n",
    "        \n",
    "        return avg_loss, f1\n",
    "\n",
    "    def _calculate_f1_score(self, predictions, labels, threshold=0.5):\n",
    "        # Convert probabilities to binary predictions\n",
    "        binary_predictions = (predictions > threshold).astype(int)\n",
    "        \n",
    "        # Debug information\n",
    "        logger.info(\"\\nF1 Score Calculation:\")\n",
    "        logger.info(f\"Binary predictions shape: {binary_predictions.shape}\")\n",
    "        logger.info(f\"Unique values in binary predictions: {np.unique(binary_predictions)}\")\n",
    "        logger.info(f\"Sample binary predictions:\\n{binary_predictions[0]}\")\n",
    "        logger.info(f\"Sample labels:\\n{labels[0]}\")\n",
    "        \n",
    "        # Calculate F1 score for each class\n",
    "        f1_scores = []\n",
    "        for i in range(labels.shape[1]):\n",
    "            class_f1 = f1_score(labels[:, i], binary_predictions[:, i], zero_division=1)\n",
    "            f1_scores.append(class_f1)\n",
    "            logger.info(f\"F1 score for class {i}: {class_f1:.4f}\")\n",
    "        \n",
    "        # Return macro average\n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        for batch in tqdm(self.val_loader, desc='Validating'):\n",
    "            images = batch['image'].to(self.device, non_blocking=True)\n",
    "            targets = batch['label'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            labels.extend(targets.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        f1 = self._calculate_f1_score(np.array(predictions), np.array(labels))\n",
    "        \n",
    "        return avg_loss, f1\n",
    "\n",
    "    def train(self, num_epochs=30, patience=5):\n",
    "        logger.info(\"\\nStarting training...\")\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Initialize metrics tracking\n",
    "        train_losses = []\n",
    "        train_f1s = []\n",
    "        val_losses = []\n",
    "        val_f1s = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_f1 = self.train_epoch()\n",
    "            train_losses.append(train_loss)\n",
    "            train_f1s.append(train_f1)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_f1 = self.validate()\n",
    "            val_losses.append(val_loss)\n",
    "            val_f1s.append(val_f1)\n",
    "            \n",
    "            # Log metrics\n",
    "            logger.info(\n",
    "                f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}\\n\"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\"\n",
    "            )\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                logger.info(\"Saving best model...\")\n",
    "                self.best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_f1': val_f1\n",
    "                }, os.path.join(self.output_dir, 'best_model.pth'))\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Plot training metrics\n",
    "        self._plot_metrics(train_losses, train_f1s, val_losses, val_f1s)\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "\n",
    "    def _plot_metrics(self, train_losses, train_f1s, val_losses, val_f1s):\n",
    "        \"\"\"Plot training and validation metrics\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.title('Loss over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot F1 scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_f1s, label='Train F1')\n",
    "        plt.plot(val_f1s, label='Val F1')\n",
    "        plt.title('F1 Score over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'training_metrics.png'))\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        torch.manual_seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            logger.info(\"Using CPU\")\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        logger.info(\"Loading dataset...\")\n",
    "        dataset = PrescriptionDataset(\n",
    "            image_dir=\"dataset/train/resized_images\",\n",
    "            cache_dir=\"ocr_cache\",\n",
    "            transform=transform,\n",
    "            force_reload=True  # Set to True to regenerate labels\n",
    "        )\n",
    "        logger.info(f\"Dataset loaded successfully. Total samples: {len(dataset)}\")\n",
    "\n",
    "        # Split dataset\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        logger.info(f\"Train size: {train_size}, Val size: {val_size}\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        logger.info(\"Initializing model...\")\n",
    "        model = PrescriptionModel().to(device)\n",
    "        logger.info(\"Model initialized successfully\")\n",
    "\n",
    "        logger.info(\"Initializing trainer...\")\n",
    "        trainer = PrescriptionTrainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(\"Trainer initialized successfully\")\n",
    "\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.82it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4998.28it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5220.69it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5004.54it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4002.96it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.29it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4226.21it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4324.25it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3978.09it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5000.36it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3999.53it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 6526.58it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4003.34it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5003.94it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5022.52it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3999.91it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3949.25it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4005.45it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4137.41it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4993.52it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4721.19it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5157.14it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4041.73it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3994.00it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.85it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3985.65it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4991.44it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.26it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4002.77it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4994.41it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.15it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4993.81it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.86it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.63it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3997.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4003.54it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4002.96it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5000.66it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4003.15it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3997.81it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3332.91it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5005.14it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5004.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4999.47it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.75it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3307.03it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.05it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5003.94it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4887.33it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3995.34it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.67it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4004.30it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.55it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3997.05it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5004.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3998.19it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4998.57it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3997.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.82it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4518.75it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3994.39it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5004.54it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5239.93it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4576.19it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5064.36it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4447.12it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4997.68it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5012.31it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3331.85it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4900.17it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4934.48it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5285.83it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4810.81it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3682.77it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4482.77it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5000.96it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4796.51it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5521.00it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4002.77it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5360.48it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3717.04it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4935.64it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3331.85it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3920.64it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.26it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4479.90it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4951.37it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.55it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3332.65it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4002.20it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4007.94it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5017.11it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.10it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3985.47it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3998.76it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4999.17it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5003.94it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 6661.86it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4719.59it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3795.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4017.92it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.85it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4999.47it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5004.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 2221.62it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4011.96it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3330.66it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3329.21it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3998.00it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 2499.44it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4011.58it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3997.81it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3994.00it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5242.55it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5003.64it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4405.32it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5005.43it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4038.42it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3999.15it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4727.84it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4143.14it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5275.19it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4712.70it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3999.72it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3854.70it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4727.57it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4782.56it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4910.21it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5018.31it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4461.79it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5256.35it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4523.87it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5067.73it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4696.34it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4601.54it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4998.57it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4562.25it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4924.92it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5393.91it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5608.11it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5018.61it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5680.64it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4434.43it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4181.76it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4497.19it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4403.70it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3922.48it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4131.71it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4302.73it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5332.53it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5273.20it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4994.71it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4607.35it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4289.75it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4544.45it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4373.39it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.26it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4278.16it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4278.59it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5003.34it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.29it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5004.84it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.75it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4429.51it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 1614.50it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3999.15it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.86it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.45it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5033.37it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.29it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5000.06it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4967.79it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4573.69it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4007.74it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4988.76it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4040.95it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5034.27it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.29it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4869.74it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4002.20it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5266.25it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4938.25it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.43it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4971.32it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.55it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4995.30it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4928.39it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4980.47it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4973.68it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5000.96it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5003.94it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.15it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4003.54it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4993.81it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4241.38it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.75it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4856.77it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5007.53it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.85it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4633.82it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4209.04it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4752.75it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4001.82it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5435.50it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4971.03it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5018.31it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5001.55it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5083.08it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4412.73it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5210.32it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4497.91it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4756.80it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4980.47it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4766.80it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4510.00it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5577.16it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3999.53it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4957.51it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4728.11it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5550.22it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4152.16it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4687.68it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5000.06it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3996.67it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4188.02it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.45it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4723.58it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4385.51it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4999.77it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5003.34it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5266.58it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5202.24it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4455.63it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 3998.00it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4655.42it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5002.15it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4971.32it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4143.75it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4995.30it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5409.91it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4000.29it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4346.43it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4573.44it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5716.65it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4997.98it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 5543.62it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4972.21it/s]\n",
      "Processing batch (20 images): 100%|██████████| 20/20 [00:00<00:00, 4584.69it/s]\n",
      "Training: 100%|██████████| 250/250 [01:18<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:18<00:00,  3.44it/s]\n",
      "Training: 100%|██████████| 250/250 [01:12<00:00,  3.46it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.78it/s]\n",
      "Training: 100%|██████████| 250/250 [01:13<00:00,  3.41it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.93it/s]\n",
      "Training: 100%|██████████| 250/250 [01:12<00:00,  3.44it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:18<00:00,  3.39it/s]\n",
      "Training: 100%|██████████| 250/250 [01:11<00:00,  3.50it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:15<00:00,  4.04it/s]\n",
      "Training: 100%|██████████| 250/250 [01:08<00:00,  3.64it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:15<00:00,  3.98it/s]\n",
      "Training: 100%|██████████| 250/250 [01:09<00:00,  3.60it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.81it/s]\n",
      "Training: 100%|██████████| 250/250 [01:09<00:00,  3.62it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:15<00:00,  3.95it/s]\n",
      "Training: 100%|██████████| 250/250 [01:09<00:00,  3.60it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:18<00:00,  3.42it/s]\n",
      "Training: 100%|██████████| 250/250 [01:10<00:00,  3.54it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.92it/s]\n",
      "Training: 100%|██████████| 250/250 [01:10<00:00,  3.54it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.92it/s]\n",
      "Training: 100%|██████████| 250/250 [01:09<00:00,  3.61it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.88it/s]\n",
      "Training: 100%|██████████| 250/250 [01:12<00:00,  3.47it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:15<00:00,  4.00it/s]\n",
      "Training: 100%|██████████| 250/250 [01:09<00:00,  3.62it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:15<00:00,  3.95it/s]\n",
      "Training: 100%|██████████| 250/250 [01:10<00:00,  3.55it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.91it/s]\n",
      "Training: 100%|██████████| 250/250 [01:10<00:00,  3.53it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:16<00:00,  3.82it/s]\n",
      "Training: 100%|██████████| 250/250 [01:15<00:00,  3.30it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:18<00:00,  3.33it/s]\n",
      "Training: 100%|██████████| 250/250 [01:16<00:00,  3.26it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:19<00:00,  3.25it/s]\n",
      "Training: 100%|██████████| 250/250 [01:11<00:00,  3.49it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:19<00:00,  3.16it/s]\n",
      "Training: 100%|██████████| 250/250 [01:18<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 63/63 [00:18<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from paddleocr import PaddleOCR\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - [%(levelname)8s] %(filename)s:%(lineno)d - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('prescription_process.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class PrescriptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, cache_dir, transform=None, force_reload=False, \n",
    "                 batch_process=True, batch_size=50, image_size=(400, 600)):\n",
    "        self.image_dir = image_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.transform = transform\n",
    "        self.force_reload = force_reload\n",
    "        self.batch_process = batch_process\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Get list of image files\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Process images and cache results\n",
    "        if force_reload or not self._check_cache_exists():\n",
    "            logger.info(\"Processing images and caching results...\")\n",
    "            self._preprocess_images()\n",
    "            \n",
    "        # Load cached results\n",
    "        self.cached_results = self._load_cached_results()\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        logger.info(\"\\nDataset Statistics:\")\n",
    "        logger.info(f\"Total images: {len(self.image_files)}\")\n",
    "        logger.info(f\"Cached results: {len(self.cached_results)}\")\n",
    "        \n",
    "        # Print sample label distribution\n",
    "        labels = [result['label'] for result in self.cached_results.values()]\n",
    "        labels = np.array(labels)\n",
    "        logger.info(\"\\nLabel Distribution:\")\n",
    "        for i in range(labels.shape[1]):\n",
    "            positive_count = np.sum(labels[:, i] == 1)\n",
    "            logger.info(f\"Class {i}: {positive_count} positive samples ({positive_count/len(labels)*100:.2f}%)\")\n",
    "\n",
    "    def _check_cache_exists(self):\n",
    "        # Check if a sample of files exists to speed up initialization\n",
    "        sample_size = min(100, len(self.image_files))\n",
    "        sample_files = self.image_files[:sample_size]\n",
    "        \n",
    "        return all(\n",
    "            os.path.exists(os.path.join(self.cache_dir, f\"{os.path.splitext(img)[0]}.json\"))\n",
    "            for img in sample_files\n",
    "        )\n",
    "\n",
    "    def _preprocess_images(self):\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        # Initialize OCR only once\n",
    "        ocr = PaddleOCR(\n",
    "            use_angle_cls=False,\n",
    "            lang='en',\n",
    "            show_log=False,\n",
    "            use_gpu=torch.cuda.is_available(),\n",
    "            enable_mkldnn=True,\n",
    "            cpu_threads=4\n",
    "        )\n",
    "        \n",
    "        # Process images in batches\n",
    "        if self.batch_process:\n",
    "            remaining_files = list(self.image_files)\n",
    "            \n",
    "            while remaining_files:\n",
    "                # Take a batch of files\n",
    "                batch_files = remaining_files[:self.batch_size]\n",
    "                remaining_files = remaining_files[self.batch_size:]\n",
    "                \n",
    "                for img_file in tqdm(batch_files, desc=f\"Processing batch ({len(batch_files)} images)\"):\n",
    "                    try:\n",
    "                        # Skip if already cached\n",
    "                        cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "                        if os.path.exists(cache_path):\n",
    "                            successful += 1\n",
    "                            continue\n",
    "                            \n",
    "                        # Process image\n",
    "                        image_path = os.path.join(self.image_dir, img_file)\n",
    "                        result = self._process_single_image(image_path, ocr)\n",
    "                        \n",
    "                        # Cache result\n",
    "                        with open(cache_path, 'w') as f:\n",
    "                            json.dump(result, f)\n",
    "                        \n",
    "                        successful += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing {img_file}: {str(e)}\")\n",
    "                        failed += 1\n",
    "                        continue\n",
    "                \n",
    "                # Force garbage collection between batches\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Process images one by one (original approach)\n",
    "            for img_file in tqdm(self.image_files, desc=\"Processing images\"):\n",
    "                try:\n",
    "                    # Skip if already cached\n",
    "                    cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "                    if os.path.exists(cache_path):\n",
    "                        successful += 1\n",
    "                        continue\n",
    "                        \n",
    "                    # Process image\n",
    "                    image_path = os.path.join(self.image_dir, img_file)\n",
    "                    result = self._process_single_image(image_path, ocr)\n",
    "                    \n",
    "                    # Cache result\n",
    "                    with open(cache_path, 'w') as f:\n",
    "                        json.dump(result, f)\n",
    "                    \n",
    "                    successful += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {img_file}: {str(e)}\")\n",
    "                    failed += 1\n",
    "                    continue\n",
    "        \n",
    "        logger.info(f\"\\nProcessing complete:\")\n",
    "        logger.info(f\"Successfully processed: {successful}\")\n",
    "        logger.info(f\"Failed to process: {failed}\")\n",
    "        logger.info(f\"Success rate: {(successful/(successful+failed))*100:.2f}%\")\n",
    "\n",
    "    def _process_single_image(self, image_path, ocr):\n",
    "        try:\n",
    "            # Load image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                logger.warning(f\"Could not load image: {image_path}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "                \n",
    "            # Use a smaller resize to reduce memory usage\n",
    "            image = cv2.resize(image, self.image_size)\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply simpler preprocessing to save memory\n",
    "            # Skip adaptive histogram equalization (CLAHE) which is memory-intensive\n",
    "            blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "            \n",
    "            # Use simple thresholding instead of adaptive thresholding\n",
    "            _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Perform OCR with optimized parameters\n",
    "            try:\n",
    "                result = ocr.ocr(binary, cls=False)\n",
    "                # Force garbage collection after OCR to free memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"OCR failed for {image_path}: {str(e)}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "            \n",
    "            # Handle None or empty results\n",
    "            if result is None or len(result) == 0 or not result[0]:\n",
    "                logger.warning(f\"No OCR results for {image_path}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "                \n",
    "            # Extract text and confidence with validation\n",
    "            texts = []\n",
    "            confidences = []\n",
    "            try:\n",
    "                for line in result[0]:\n",
    "                    if isinstance(line, (list, tuple)) and len(line) >= 2:\n",
    "                        text = line[1][0] if isinstance(line[1], (list, tuple)) else str(line[1])\n",
    "                        confidence = line[1][1] if isinstance(line[1], (list, tuple)) and len(line[1]) > 1 else 0.0\n",
    "                        texts.append(text)\n",
    "                        confidences.append(confidence)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error extracting text from OCR result for {image_path}: {str(e)}\")\n",
    "                return {'text': [], 'label': [0] * 7}\n",
    "            \n",
    "            # Generate labels based on OCR text\n",
    "            label = self._generate_labels(texts)\n",
    "            \n",
    "            return {\n",
    "                'text': texts,\n",
    "                'confidence': confidences,\n",
    "                'label': label\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return {'text': [], 'label': [0] * 7}\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    def _normalize_text(self, text):\n",
    "        \"\"\"Normalize text for better matching, with caching for performance\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove common prefixes and suffixes\n",
    "        prefixes = ['tab', 'tablet','TAB','Tab','Cap','CAP', 'cap', 'capsule', 'syrup', 'SYR','Syrup','Syr','suspension', 'INJ','Inj','inj', 'injection']\n",
    "        for prefix in prefixes:\n",
    "            if text.startswith(prefix + ' '):\n",
    "                text = text[len(prefix):].strip()\n",
    "        \n",
    "        # Remove special characters and extra spaces\n",
    "        text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def _generate_labels(self, texts):\n",
    "        # Initialize label vector\n",
    "        label = [0] * 7\n",
    "        \n",
    "        # Common medication form indicators with variations\n",
    "        forms = {\n",
    "            0: {  # Tablets\n",
    "                'keywords': ['tablet','Tablet', 'Tab','TAB','tab','PILL','Pill', 'pill'],\n",
    "                'variations': ['tablets', 'tabs', 'pills']\n",
    "            },\n",
    "            1: {  # Capsules\n",
    "                'keywords': ['capsule','Capsule','CAP','Cap', 'cap'],\n",
    "                'variations': ['capsules', 'caps']\n",
    "            },\n",
    "            2: {  # Syrups\n",
    "                'keywords': ['syrup','Syrup','SYR','Syr', 'suspension'],\n",
    "                'variations': ['syrups', 'suspensions']\n",
    "            },\n",
    "            3: {  # Injections\n",
    "                'keywords': ['injection','Injection','INJECTION','INJ', 'inj'],\n",
    "                'variations': ['injections', 'injs']\n",
    "            },\n",
    "            4: {  # Drops\n",
    "                'keywords': ['drops', 'eye drops'],\n",
    "                'variations': ['drop', 'eyedrops']\n",
    "            },\n",
    "            5: {  # Topical\n",
    "                'keywords': ['cream', 'ointment'],\n",
    "                'variations': ['creams', 'ointments']\n",
    "            },\n",
    "            6: {  # Inhalers/Sprays\n",
    "                'keywords': ['inhaler', 'spray'],\n",
    "                'variations': ['inhalers', 'sprays']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Process each text line\n",
    "        for text in texts:\n",
    "            # Normalize text\n",
    "            normalized_text = self._normalize_text(text)\n",
    "            \n",
    "            # Check for medication forms\n",
    "            for form_id, form_info in forms.items():\n",
    "                # Check main keywords\n",
    "                if any(keyword in normalized_text for keyword in form_info['keywords']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "                \n",
    "                # Check variations\n",
    "                if any(variation in normalized_text for variation in form_info['variations']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "                \n",
    "                # Check for exact matches (case-insensitive)\n",
    "                if any(keyword.lower() in text.lower() for keyword in form_info['keywords']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "                \n",
    "                # Check for variations with exact matches\n",
    "                if any(variation.lower() in text.lower() for variation in form_info['variations']):\n",
    "                    label[form_id] = 1\n",
    "                    break\n",
    "        \n",
    "        return label\n",
    "\n",
    "    def _load_cached_results(self):\n",
    "        cached_results = {}\n",
    "        for img_file in self.image_files:\n",
    "            cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "            try:\n",
    "                if os.path.exists(cache_path):\n",
    "                    with open(cache_path, 'r') as f:\n",
    "                        cached_results[img_file] = json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading cache for {img_file}: {str(e)}\")\n",
    "                continue\n",
    "        return cached_results\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, img_file)\n",
    "        \n",
    "        # Load image with PIL and handle memory issue\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            # Return a black image in case of error\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Get cached result\n",
    "        result = self.cached_results.get(img_file, {'label': [0] * 7})\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(result['label'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class PrescriptionModel(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super().__init__()\n",
    "        # Use a smaller model initially\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify the final layers\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class PrescriptionTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, \n",
    "                 learning_rate=1e-3):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.BCELoss()  # Changed from BCEWithLogitsLoss since we have Sigmoid in model\n",
    "        \n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=learning_rate,\n",
    "            epochs=30,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.3,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.output_dir = f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        logger.info(f\"\\nStarting training epoch...\")\n",
    "        logger.info(f\"Number of batches: {len(self.train_loader)}\")\n",
    "        \n",
    "        # Use tqdm for progress tracking\n",
    "        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc=\"Training\")):\n",
    "            try:\n",
    "                if batch_idx % 20 == 0:  # Reduced logging frequency to speed up\n",
    "                    logger.info(f\"Processing batch {batch_idx + 1}/{len(self.train_loader)}\")\n",
    "                \n",
    "                images = batch['image'].to(self.device, non_blocking=True)\n",
    "                targets = batch['label'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    logger.info(f\"Input shape: {images.shape}\")\n",
    "                    logger.info(f\"Target shape: {targets.shape}\")\n",
    "                    logger.info(f\"Sample target values: {targets[0]}\")\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                total_loss += loss.item()\n",
    "                # No need for sigmoid since it's in the model\n",
    "                predictions.extend(outputs.detach().cpu().numpy())\n",
    "                labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "                if batch_idx % 50 == 0:  # Reduced logging frequency\n",
    "                    logger.info(f\"Current loss: {loss.item():.4f}, LR: {self.scheduler.get_last_lr()[0]:.6f}\")\n",
    "                \n",
    "                # Clear GPU memory between batches\n",
    "                if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Debug information\n",
    "        logger.info(\"\\nDebug Information:\")\n",
    "        logger.info(f\"Predictions shape: {predictions.shape}\")\n",
    "        logger.info(f\"Labels shape: {labels.shape}\")\n",
    "        \n",
    "        f1 = self._calculate_f1_score(predictions, labels)\n",
    "        logger.info(f\"Calculated F1 score: {f1:.4f}\")\n",
    "        \n",
    "        return avg_loss, f1\n",
    "\n",
    "    def _calculate_f1_score(self, predictions, labels, threshold=0.5):\n",
    "        # Convert probabilities to binary predictions\n",
    "        binary_predictions = (predictions > threshold).astype(int)\n",
    "        \n",
    "        # Calculate F1 score for each class\n",
    "        f1_scores = []\n",
    "        for i in range(labels.shape[1]):\n",
    "            class_f1 = f1_score(labels[:, i], binary_predictions[:, i], zero_division=1)\n",
    "            f1_scores.append(class_f1)\n",
    "            logger.info(f\"F1 score for class {i}: {class_f1:.4f}\")\n",
    "        \n",
    "        # Return macro average\n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(self.val_loader, desc='Validating')):\n",
    "            try:\n",
    "                images = batch['image'].to(self.device, non_blocking=True)\n",
    "                targets = batch['label'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "                # Clear GPU memory periodically\n",
    "                if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in validation batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        f1 = self._calculate_f1_score(np.array(predictions), np.array(labels))\n",
    "        \n",
    "        return avg_loss, f1\n",
    "\n",
    "    def train(self, num_epochs=20, patience=5):  # Reduced default epochs\n",
    "        logger.info(\"\\nStarting training...\")\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Initialize metrics tracking\n",
    "        train_losses = []\n",
    "        train_f1s = []\n",
    "        val_losses = []\n",
    "        val_f1s = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_f1 = self.train_epoch()\n",
    "            train_losses.append(train_loss)\n",
    "            train_f1s.append(train_f1)\n",
    "            \n",
    "            # Clear memory before validation\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_f1 = self.validate()\n",
    "            val_losses.append(val_loss)\n",
    "            val_f1s.append(val_f1)\n",
    "            \n",
    "            # Log metrics\n",
    "            logger.info(\n",
    "                f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}\\n\"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\"\n",
    "            )\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                logger.info(\"Saving best model...\")\n",
    "                self.best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_f1': val_f1\n",
    "                }, os.path.join(self.output_dir, 'best_model.pth'))\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "            \n",
    "            # Clear memory after each epoch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Plot training metrics\n",
    "        self._plot_metrics(train_losses, train_f1s, val_losses, val_f1s)\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "\n",
    "    def _plot_metrics(self, train_losses, train_f1s, val_losses, val_f1s):\n",
    "        \"\"\"Plot training and validation metrics\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.title('Loss over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot F1 scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_f1s, label='Train F1')\n",
    "        plt.plot(val_f1s, label='Val F1')\n",
    "        plt.title('F1 Score over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'training_metrics.png'))\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Set seeds for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # GPU setup and memory management\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.cuda.empty_cache()\n",
    "            logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            logger.info(\"Using CPU\")\n",
    "        \n",
    "        # Efficient transform pipeline\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        logger.info(\"Loading dataset...\")\n",
    "        dataset = PrescriptionDataset(\n",
    "            image_dir=\"dataset/train/resized_images\",\n",
    "            cache_dir=\"ocr_cache\",\n",
    "            transform=transform,\n",
    "            force_reload=True,       # Set to True to regenerate labels\n",
    "            batch_process=True,      # Process in batches to manage memory\n",
    "            batch_size=20,           # Smaller batch size for more frequent GC\n",
    "            image_size=(400, 600)    # Reduced image size for OCR\n",
    "        )\n",
    "        logger.info(f\"Dataset loaded successfully. Total samples: {len(dataset)}\")\n",
    "\n",
    "        # Split dataset\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        logger.info(f\"Train size: {train_size}, Val size: {val_size}\")\n",
    "\n",
    "        # Configure data loaders with memory optimization\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=16,          # Reduced batch size\n",
    "            shuffle=True,\n",
    "            num_workers=0,          # Avoid additional processes\n",
    "            pin_memory=True         # Faster data transfer to GPU\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=16,          # Reduced batch size\n",
    "            shuffle=False,\n",
    "            num_workers=0,          # Avoid additional processes\n",
    "            pin_memory=True         # Faster data transfer to GPU\n",
    "        )\n",
    "\n",
    "        logger.info(\"Initializing model...\")\n",
    "        model = PrescriptionModel().to(device)\n",
    "        \n",
    "        # Count model parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        logger.info(f\"Model initialized successfully with {total_params/1e6:.2f}M parameters\")\n",
    "\n",
    "        logger.info(\"Initializing trainer...\")\n",
    "        trainer = PrescriptionTrainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            learning_rate=5e-4      # Slightly reduced learning rate\n",
    "        )\n",
    "        logger.info(\"Trainer initialized successfully\")\n",
    "\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train(num_epochs=20, patience=5)  # Reduced epochs for faster training\n",
    "        \n",
    "        # Final cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MedReminderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MedReminderModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjust dimensions for your dataset\n",
    "        self.fc2 = nn.Linear(128, 4 * 5)  # Output 4 values (name, dosage, frequency, duration) for 5 items (medicines + syrups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)  # Flatten the output of the convolution layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # This is a structured output, with 4 fields for each of 5 items (adjust accordingly)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1, Batch 1/157, Loss: 2.6449\n",
      "Epoch 1, Batch 2/157, Loss: 2.9813\n",
      "Epoch 1, Batch 3/157, Loss: 3.0640\n",
      "Epoch 1, Batch 4/157, Loss: 3.0159\n",
      "Epoch 1, Batch 5/157, Loss: 2.8782\n",
      "Epoch 1, Batch 6/157, Loss: 3.1380\n",
      "Epoch 1, Batch 7/157, Loss: 2.7123\n",
      "Epoch 1, Batch 8/157, Loss: 2.6803\n",
      "Epoch 1, Batch 9/157, Loss: 2.8094\n",
      "Epoch 1, Batch 10/157, Loss: 2.8185\n",
      "Epoch 1, Batch 11/157, Loss: 2.9000\n",
      "Epoch 1, Batch 12/157, Loss: 2.7253\n",
      "Epoch 1, Batch 13/157, Loss: 2.7638\n",
      "Epoch 1, Batch 14/157, Loss: 2.7984\n",
      "Epoch 1, Batch 15/157, Loss: 2.7667\n",
      "Epoch 1, Batch 16/157, Loss: 2.6975\n",
      "Epoch 1, Batch 17/157, Loss: 2.7412\n",
      "Epoch 1, Batch 18/157, Loss: 2.8484\n",
      "Epoch 1, Batch 19/157, Loss: 2.7946\n",
      "Epoch 1, Batch 20/157, Loss: 2.7967\n",
      "Epoch 1, Batch 21/157, Loss: 2.7914\n",
      "Epoch 1, Batch 22/157, Loss: 2.6970\n",
      "Epoch 1, Batch 23/157, Loss: 2.8958\n",
      "Epoch 1, Batch 24/157, Loss: 2.8492\n",
      "Epoch 1, Batch 25/157, Loss: 2.7434\n",
      "Epoch 1, Batch 26/157, Loss: 2.7556\n",
      "Epoch 1, Batch 27/157, Loss: 2.8428\n",
      "Epoch 1, Batch 28/157, Loss: 2.8645\n",
      "Epoch 1, Batch 29/157, Loss: 2.8094\n",
      "Epoch 1, Batch 30/157, Loss: 2.8500\n",
      "Epoch 1, Batch 31/157, Loss: 2.8574\n",
      "Epoch 1, Batch 32/157, Loss: 2.7365\n",
      "Epoch 1, Batch 33/157, Loss: 2.8657\n",
      "Epoch 1, Batch 34/157, Loss: 2.7004\n",
      "Epoch 1, Batch 35/157, Loss: 2.8056\n",
      "Epoch 1, Batch 36/157, Loss: 2.7063\n",
      "Epoch 1, Batch 37/157, Loss: 2.7955\n",
      "Epoch 1, Batch 38/157, Loss: 2.8036\n",
      "Epoch 1, Batch 39/157, Loss: 2.7622\n",
      "Epoch 1, Batch 40/157, Loss: 2.7437\n",
      "Epoch 1, Batch 41/157, Loss: 2.7268\n",
      "Epoch 1, Batch 42/157, Loss: 2.7662\n",
      "Epoch 1, Batch 43/157, Loss: 2.7522\n",
      "Epoch 1, Batch 44/157, Loss: 2.7925\n",
      "Epoch 1, Batch 45/157, Loss: 2.6575\n",
      "Epoch 1, Batch 46/157, Loss: 2.7436\n",
      "Epoch 1, Batch 47/157, Loss: 2.6444\n",
      "Epoch 1, Batch 48/157, Loss: 2.7759\n",
      "Epoch 1, Batch 49/157, Loss: 2.6951\n",
      "Epoch 1, Batch 50/157, Loss: 2.7961\n",
      "Epoch 1, Batch 51/157, Loss: 2.8009\n",
      "Epoch 1, Batch 52/157, Loss: 2.6463\n",
      "Epoch 1, Batch 53/157, Loss: 2.6916\n",
      "Epoch 1, Batch 54/157, Loss: 2.8122\n",
      "Epoch 1, Batch 55/157, Loss: 2.6594\n",
      "Epoch 1, Batch 56/157, Loss: 2.7937\n",
      "Epoch 1, Batch 57/157, Loss: 2.7637\n",
      "Epoch 1, Batch 58/157, Loss: 2.6642\n",
      "Epoch 1, Batch 59/157, Loss: 2.7309\n",
      "Epoch 1, Batch 60/157, Loss: 2.8570\n",
      "Epoch 1, Batch 61/157, Loss: 2.7615\n",
      "Epoch 1, Batch 62/157, Loss: 2.7490\n",
      "Epoch 1, Batch 63/157, Loss: 2.6831\n",
      "Epoch 1, Batch 64/157, Loss: 2.6819\n",
      "Epoch 1, Batch 65/157, Loss: 2.7523\n",
      "Epoch 1, Batch 66/157, Loss: 2.7218\n",
      "Epoch 1, Batch 67/157, Loss: 2.7572\n",
      "Epoch 1, Batch 68/157, Loss: 2.6729\n",
      "Epoch 1, Batch 69/157, Loss: 2.6889\n",
      "Epoch 1, Batch 70/157, Loss: 2.7558\n",
      "Epoch 1, Batch 71/157, Loss: 2.7139\n",
      "Epoch 1, Batch 72/157, Loss: 2.7859\n",
      "Epoch 1, Batch 73/157, Loss: 2.7089\n",
      "Epoch 1, Batch 74/157, Loss: 2.7319\n",
      "Epoch 1, Batch 75/157, Loss: 2.7791\n",
      "Epoch 1, Batch 76/157, Loss: 2.8091\n",
      "Epoch 1, Batch 77/157, Loss: 2.6365\n",
      "Epoch 1, Batch 78/157, Loss: 2.7103\n",
      "Epoch 1, Batch 79/157, Loss: 2.7617\n",
      "Epoch 1, Batch 80/157, Loss: 2.7607\n",
      "Epoch 1, Batch 81/157, Loss: 2.7178\n",
      "Epoch 1, Batch 82/157, Loss: 2.8277\n",
      "Epoch 1, Batch 83/157, Loss: 2.7145\n",
      "Epoch 1, Batch 84/157, Loss: 2.7200\n",
      "Epoch 1, Batch 85/157, Loss: 2.6964\n",
      "Epoch 1, Batch 86/157, Loss: 2.6791\n",
      "Epoch 1, Batch 87/157, Loss: 2.7271\n",
      "Epoch 1, Batch 88/157, Loss: 2.7193\n",
      "Epoch 1, Batch 89/157, Loss: 2.6542\n",
      "Epoch 1, Batch 90/157, Loss: 2.7137\n",
      "Epoch 1, Batch 91/157, Loss: 2.8088\n",
      "Epoch 1, Batch 92/157, Loss: 2.7228\n",
      "Epoch 1, Batch 93/157, Loss: 2.6772\n",
      "Epoch 1, Batch 94/157, Loss: 2.6866\n",
      "Epoch 1, Batch 95/157, Loss: 2.7674\n",
      "Epoch 1, Batch 96/157, Loss: 2.8062\n",
      "Epoch 1, Batch 97/157, Loss: 2.7044\n",
      "Epoch 1, Batch 98/157, Loss: 2.7340\n",
      "Epoch 1, Batch 99/157, Loss: 2.7201\n",
      "Epoch 1, Batch 100/157, Loss: 2.6458\n",
      "Epoch 1, Batch 101/157, Loss: 2.7733\n",
      "Epoch 1, Batch 102/157, Loss: 2.8034\n",
      "Epoch 1, Batch 103/157, Loss: 2.8296\n",
      "Epoch 1, Batch 104/157, Loss: 2.7701\n",
      "Epoch 1, Batch 105/157, Loss: 2.7086\n",
      "Epoch 1, Batch 106/157, Loss: 2.7553\n",
      "Epoch 1, Batch 107/157, Loss: 2.7377\n",
      "Epoch 1, Batch 108/157, Loss: 2.7125\n",
      "Epoch 1, Batch 109/157, Loss: 2.8311\n",
      "Epoch 1, Batch 110/157, Loss: 2.7865\n",
      "Epoch 1, Batch 111/157, Loss: 2.7755\n",
      "Epoch 1, Batch 112/157, Loss: 2.6908\n",
      "Epoch 1, Batch 113/157, Loss: 2.8575\n",
      "Epoch 1, Batch 114/157, Loss: 2.7135\n",
      "Epoch 1, Batch 115/157, Loss: 2.7087\n",
      "Epoch 1, Batch 116/157, Loss: 2.7690\n",
      "Epoch 1, Batch 117/157, Loss: 2.6890\n",
      "Epoch 1, Batch 118/157, Loss: 2.7705\n",
      "Epoch 1, Batch 119/157, Loss: 2.7073\n",
      "Epoch 1, Batch 120/157, Loss: 2.8216\n",
      "Epoch 1, Batch 121/157, Loss: 2.7328\n",
      "Epoch 1, Batch 122/157, Loss: 2.7737\n",
      "Epoch 1, Batch 123/157, Loss: 2.7447\n",
      "Epoch 1, Batch 124/157, Loss: 2.6738\n",
      "Epoch 1, Batch 125/157, Loss: 2.6777\n",
      "Epoch 1, Batch 126/157, Loss: 2.7930\n",
      "Epoch 1, Batch 127/157, Loss: 2.7622\n",
      "Epoch 1, Batch 128/157, Loss: 2.7496\n",
      "Epoch 1, Batch 129/157, Loss: 2.7639\n",
      "Epoch 1, Batch 130/157, Loss: 2.8051\n",
      "Epoch 1, Batch 131/157, Loss: 2.7817\n",
      "Epoch 1, Batch 132/157, Loss: 2.6802\n",
      "Epoch 1, Batch 133/157, Loss: 2.7196\n",
      "Epoch 1, Batch 134/157, Loss: 2.7591\n",
      "Epoch 1, Batch 135/157, Loss: 2.7796\n",
      "Epoch 1, Batch 136/157, Loss: 2.6702\n",
      "Epoch 1, Batch 137/157, Loss: 2.8361\n",
      "Epoch 1, Batch 138/157, Loss: 2.7211\n",
      "Epoch 1, Batch 139/157, Loss: 2.7994\n",
      "Epoch 1, Batch 140/157, Loss: 2.8268\n",
      "Epoch 1, Batch 141/157, Loss: 2.7572\n",
      "Epoch 1, Batch 142/157, Loss: 2.6961\n",
      "Epoch 1, Batch 143/157, Loss: 2.7475\n",
      "Epoch 1, Batch 144/157, Loss: 2.7628\n",
      "Epoch 1, Batch 145/157, Loss: 2.7243\n",
      "Epoch 1, Batch 146/157, Loss: 2.7986\n",
      "Epoch 1, Batch 147/157, Loss: 2.7214\n",
      "Epoch 1, Batch 148/157, Loss: 2.7765\n",
      "Epoch 1, Batch 149/157, Loss: 2.7663\n",
      "Epoch 1, Batch 150/157, Loss: 2.7368\n",
      "Epoch 1, Batch 151/157, Loss: 2.7673\n",
      "Epoch 1, Batch 152/157, Loss: 2.7807\n",
      "Epoch 1, Batch 153/157, Loss: 2.7014\n",
      "Epoch 1, Batch 154/157, Loss: 2.7059\n",
      "Epoch 1, Batch 155/157, Loss: 2.7070\n",
      "Epoch 1, Batch 156/157, Loss: 2.8266\n",
      "Epoch 1, Batch 157/157, Loss: 2.7668\n",
      "Epoch 1/50, Average Loss: 2.7598\n",
      "Epoch 2, Batch 1/157, Loss: 2.8319\n",
      "Epoch 2, Batch 2/157, Loss: 2.8056\n",
      "Epoch 2, Batch 3/157, Loss: 2.7899\n",
      "Epoch 2, Batch 4/157, Loss: 2.7025\n",
      "Epoch 2, Batch 5/157, Loss: 2.7400\n",
      "Epoch 2, Batch 6/157, Loss: 2.7817\n",
      "Epoch 2, Batch 7/157, Loss: 2.7806\n",
      "Epoch 2, Batch 8/157, Loss: 2.6760\n",
      "Epoch 2, Batch 9/157, Loss: 2.6960\n",
      "Epoch 2, Batch 10/157, Loss: 2.7315\n",
      "Epoch 2, Batch 11/157, Loss: 2.7688\n",
      "Epoch 2, Batch 12/157, Loss: 2.7765\n",
      "Epoch 2, Batch 13/157, Loss: 2.6387\n",
      "Epoch 2, Batch 14/157, Loss: 2.7726\n",
      "Epoch 2, Batch 15/157, Loss: 2.8509\n",
      "Epoch 2, Batch 16/157, Loss: 2.7116\n",
      "Epoch 2, Batch 17/157, Loss: 2.7287\n",
      "Epoch 2, Batch 18/157, Loss: 2.7215\n",
      "Epoch 2, Batch 19/157, Loss: 2.7706\n",
      "Epoch 2, Batch 20/157, Loss: 2.7132\n",
      "Epoch 2, Batch 21/157, Loss: 2.7589\n",
      "Epoch 2, Batch 22/157, Loss: 2.8163\n",
      "Epoch 2, Batch 23/157, Loss: 2.7534\n",
      "Epoch 2, Batch 24/157, Loss: 2.7970\n",
      "Epoch 2, Batch 25/157, Loss: 2.7029\n",
      "Epoch 2, Batch 26/157, Loss: 2.7091\n",
      "Epoch 2, Batch 27/157, Loss: 2.7147\n",
      "Epoch 2, Batch 28/157, Loss: 2.7692\n",
      "Epoch 2, Batch 29/157, Loss: 2.7866\n",
      "Epoch 2, Batch 30/157, Loss: 2.7768\n",
      "Epoch 2, Batch 31/157, Loss: 2.7349\n",
      "Epoch 2, Batch 32/157, Loss: 2.8255\n",
      "Epoch 2, Batch 33/157, Loss: 2.7012\n",
      "Epoch 2, Batch 34/157, Loss: 2.8196\n",
      "Epoch 2, Batch 35/157, Loss: 2.7507\n",
      "Epoch 2, Batch 36/157, Loss: 2.7864\n",
      "Epoch 2, Batch 37/157, Loss: 2.8101\n",
      "Epoch 2, Batch 38/157, Loss: 2.7246\n",
      "Epoch 2, Batch 39/157, Loss: 2.7904\n",
      "Epoch 2, Batch 40/157, Loss: 2.7643\n",
      "Epoch 2, Batch 41/157, Loss: 2.7304\n",
      "Epoch 2, Batch 42/157, Loss: 2.7328\n",
      "Epoch 2, Batch 43/157, Loss: 2.7704\n",
      "Epoch 2, Batch 44/157, Loss: 2.7007\n",
      "Epoch 2, Batch 45/157, Loss: 2.7577\n",
      "Epoch 2, Batch 46/157, Loss: 2.7689\n",
      "Epoch 2, Batch 47/157, Loss: 2.7819\n",
      "Epoch 2, Batch 48/157, Loss: 2.7256\n",
      "Epoch 2, Batch 49/157, Loss: 2.7563\n",
      "Epoch 2, Batch 50/157, Loss: 2.7541\n",
      "Epoch 2, Batch 51/157, Loss: 2.6475\n",
      "Epoch 2, Batch 52/157, Loss: 2.7776\n",
      "Epoch 2, Batch 53/157, Loss: 2.7928\n",
      "Epoch 2, Batch 54/157, Loss: 2.7471\n",
      "Epoch 2, Batch 55/157, Loss: 2.7233\n",
      "Epoch 2, Batch 56/157, Loss: 2.7133\n",
      "Epoch 2, Batch 57/157, Loss: 2.7565\n",
      "Epoch 2, Batch 58/157, Loss: 2.7139\n",
      "Epoch 2, Batch 59/157, Loss: 2.8893\n",
      "Epoch 2, Batch 60/157, Loss: 2.7816\n",
      "Epoch 2, Batch 61/157, Loss: 2.7214\n",
      "Epoch 2, Batch 62/157, Loss: 2.7922\n",
      "Epoch 2, Batch 63/157, Loss: 2.7454\n",
      "Epoch 2, Batch 64/157, Loss: 2.7155\n",
      "Epoch 2, Batch 65/157, Loss: 2.7491\n",
      "Epoch 2, Batch 66/157, Loss: 2.7431\n",
      "Epoch 2, Batch 67/157, Loss: 2.8484\n",
      "Epoch 2, Batch 68/157, Loss: 2.6618\n",
      "Epoch 2, Batch 69/157, Loss: 2.7457\n",
      "Epoch 2, Batch 70/157, Loss: 2.8038\n",
      "Epoch 2, Batch 71/157, Loss: 2.7667\n",
      "Epoch 2, Batch 72/157, Loss: 2.8809\n",
      "Epoch 2, Batch 73/157, Loss: 2.7451\n",
      "Epoch 2, Batch 74/157, Loss: 2.7322\n",
      "Epoch 2, Batch 75/157, Loss: 2.7868\n",
      "Epoch 2, Batch 76/157, Loss: 2.8043\n",
      "Epoch 2, Batch 77/157, Loss: 2.7545\n",
      "Epoch 2, Batch 78/157, Loss: 2.7018\n",
      "Epoch 2, Batch 79/157, Loss: 2.7521\n",
      "Epoch 2, Batch 80/157, Loss: 2.7551\n",
      "Epoch 2, Batch 81/157, Loss: 2.7555\n",
      "Epoch 2, Batch 82/157, Loss: 2.7143\n",
      "Epoch 2, Batch 83/157, Loss: 2.7619\n",
      "Epoch 2, Batch 84/157, Loss: 2.7181\n",
      "Epoch 2, Batch 85/157, Loss: 2.8067\n",
      "Epoch 2, Batch 86/157, Loss: 2.7412\n",
      "Epoch 2, Batch 87/157, Loss: 2.7587\n",
      "Epoch 2, Batch 88/157, Loss: 2.7368\n",
      "Epoch 2, Batch 89/157, Loss: 2.8249\n",
      "Epoch 2, Batch 90/157, Loss: 2.6969\n",
      "Epoch 2, Batch 91/157, Loss: 2.7846\n",
      "Epoch 2, Batch 92/157, Loss: 2.8038\n",
      "Epoch 2, Batch 93/157, Loss: 2.7906\n",
      "Epoch 2, Batch 94/157, Loss: 2.7712\n",
      "Epoch 2, Batch 95/157, Loss: 2.7375\n",
      "Epoch 2, Batch 96/157, Loss: 2.7868\n",
      "Epoch 2, Batch 97/157, Loss: 2.7014\n",
      "Epoch 2, Batch 98/157, Loss: 2.7335\n",
      "Epoch 2, Batch 99/157, Loss: 2.7297\n",
      "Epoch 2, Batch 100/157, Loss: 2.7791\n",
      "Epoch 2, Batch 101/157, Loss: 2.6774\n",
      "Epoch 2, Batch 102/157, Loss: 2.7253\n",
      "Epoch 2, Batch 103/157, Loss: 2.8917\n",
      "Epoch 2, Batch 104/157, Loss: 2.7988\n",
      "Epoch 2, Batch 105/157, Loss: 2.6468\n",
      "Epoch 2, Batch 106/157, Loss: 2.5844\n",
      "Epoch 2, Batch 107/157, Loss: 2.6637\n",
      "Epoch 2, Batch 108/157, Loss: 2.7626\n",
      "Epoch 2, Batch 109/157, Loss: 2.6835\n",
      "Epoch 2, Batch 110/157, Loss: 2.7661\n",
      "Epoch 2, Batch 111/157, Loss: 2.7619\n",
      "Epoch 2, Batch 112/157, Loss: 2.7584\n",
      "Epoch 2, Batch 113/157, Loss: 2.7758\n",
      "Epoch 2, Batch 114/157, Loss: 2.6926\n",
      "Epoch 2, Batch 115/157, Loss: 2.7549\n",
      "Epoch 2, Batch 116/157, Loss: 2.7558\n",
      "Epoch 2, Batch 117/157, Loss: 2.6607\n",
      "Epoch 2, Batch 118/157, Loss: 2.8156\n",
      "Epoch 2, Batch 119/157, Loss: 2.7903\n",
      "Epoch 2, Batch 120/157, Loss: 2.8034\n",
      "Epoch 2, Batch 121/157, Loss: 2.7226\n",
      "Epoch 2, Batch 122/157, Loss: 2.7286\n",
      "Epoch 2, Batch 123/157, Loss: 2.7624\n",
      "Epoch 2, Batch 124/157, Loss: 2.6823\n",
      "Epoch 2, Batch 125/157, Loss: 2.7650\n",
      "Epoch 2, Batch 126/157, Loss: 2.7182\n",
      "Epoch 2, Batch 127/157, Loss: 2.7265\n",
      "Epoch 2, Batch 128/157, Loss: 2.7508\n",
      "Epoch 2, Batch 129/157, Loss: 2.7554\n",
      "Epoch 2, Batch 130/157, Loss: 2.7236\n",
      "Epoch 2, Batch 131/157, Loss: 2.7738\n",
      "Epoch 2, Batch 132/157, Loss: 2.7505\n",
      "Epoch 2, Batch 133/157, Loss: 2.6538\n",
      "Epoch 2, Batch 134/157, Loss: 2.7906\n",
      "Epoch 2, Batch 135/157, Loss: 2.8137\n",
      "Epoch 2, Batch 136/157, Loss: 2.7360\n",
      "Epoch 2, Batch 137/157, Loss: 2.7412\n",
      "Epoch 2, Batch 138/157, Loss: 2.7681\n",
      "Epoch 2, Batch 139/157, Loss: 2.7600\n",
      "Epoch 2, Batch 140/157, Loss: 2.7647\n",
      "Epoch 2, Batch 141/157, Loss: 2.8341\n",
      "Epoch 2, Batch 142/157, Loss: 2.7750\n",
      "Epoch 2, Batch 143/157, Loss: 2.7546\n",
      "Epoch 2, Batch 144/157, Loss: 2.7063\n",
      "Epoch 2, Batch 145/157, Loss: 2.7068\n",
      "Epoch 2, Batch 146/157, Loss: 2.7251\n",
      "Epoch 2, Batch 147/157, Loss: 2.6359\n",
      "Epoch 2, Batch 148/157, Loss: 2.7026\n",
      "Epoch 2, Batch 149/157, Loss: 2.8276\n",
      "Epoch 2, Batch 150/157, Loss: 2.7584\n",
      "Epoch 2, Batch 151/157, Loss: 2.7768\n",
      "Epoch 2, Batch 152/157, Loss: 2.7499\n",
      "Epoch 2, Batch 153/157, Loss: 2.7103\n",
      "Epoch 2, Batch 154/157, Loss: 2.6918\n",
      "Epoch 2, Batch 155/157, Loss: 2.6847\n",
      "Epoch 2, Batch 156/157, Loss: 2.7485\n",
      "Epoch 2, Batch 157/157, Loss: 3.1980\n",
      "Epoch 2/50, Average Loss: 2.7534\n",
      "Epoch 3, Batch 1/157, Loss: 2.8710\n",
      "Epoch 3, Batch 2/157, Loss: 2.6872\n",
      "Epoch 3, Batch 3/157, Loss: 2.8001\n",
      "Epoch 3, Batch 4/157, Loss: 2.7165\n",
      "Epoch 3, Batch 5/157, Loss: 2.8466\n",
      "Epoch 3, Batch 6/157, Loss: 2.7589\n",
      "Epoch 3, Batch 7/157, Loss: 2.8580\n",
      "Epoch 3, Batch 8/157, Loss: 2.8119\n",
      "Epoch 3, Batch 9/157, Loss: 2.8030\n",
      "Epoch 3, Batch 10/157, Loss: 2.7581\n",
      "Epoch 3, Batch 11/157, Loss: 2.7048\n",
      "Epoch 3, Batch 12/157, Loss: 2.7416\n",
      "Epoch 3, Batch 13/157, Loss: 2.8065\n",
      "Epoch 3, Batch 14/157, Loss: 2.7854\n",
      "Epoch 3, Batch 15/157, Loss: 2.7744\n",
      "Epoch 3, Batch 16/157, Loss: 2.7167\n",
      "Epoch 3, Batch 17/157, Loss: 2.7439\n",
      "Epoch 3, Batch 18/157, Loss: 2.7838\n",
      "Epoch 3, Batch 19/157, Loss: 2.7343\n",
      "Epoch 3, Batch 20/157, Loss: 2.7264\n",
      "Epoch 3, Batch 21/157, Loss: 2.7492\n",
      "Epoch 3, Batch 22/157, Loss: 2.7897\n",
      "Epoch 3, Batch 23/157, Loss: 2.6995\n",
      "Epoch 3, Batch 24/157, Loss: 2.6934\n",
      "Epoch 3, Batch 25/157, Loss: 2.7003\n",
      "Epoch 3, Batch 26/157, Loss: 2.7266\n",
      "Epoch 3, Batch 27/157, Loss: 2.7419\n",
      "Epoch 3, Batch 28/157, Loss: 2.7301\n",
      "Epoch 3, Batch 29/157, Loss: 2.6809\n",
      "Epoch 3, Batch 30/157, Loss: 2.8207\n",
      "Epoch 3, Batch 31/157, Loss: 2.6900\n",
      "Epoch 3, Batch 32/157, Loss: 2.7824\n",
      "Epoch 3, Batch 33/157, Loss: 2.6730\n",
      "Epoch 3, Batch 34/157, Loss: 2.6801\n",
      "Epoch 3, Batch 35/157, Loss: 2.7911\n",
      "Epoch 3, Batch 36/157, Loss: 2.7531\n",
      "Epoch 3, Batch 37/157, Loss: 2.7921\n",
      "Epoch 3, Batch 38/157, Loss: 2.7875\n",
      "Epoch 3, Batch 39/157, Loss: 2.7512\n",
      "Epoch 3, Batch 40/157, Loss: 2.7763\n",
      "Epoch 3, Batch 41/157, Loss: 2.7242\n",
      "Epoch 3, Batch 42/157, Loss: 2.7835\n",
      "Epoch 3, Batch 43/157, Loss: 2.7036\n",
      "Epoch 3, Batch 44/157, Loss: 2.6901\n",
      "Epoch 3, Batch 45/157, Loss: 2.7083\n",
      "Epoch 3, Batch 46/157, Loss: 2.7393\n",
      "Epoch 3, Batch 47/157, Loss: 2.7589\n",
      "Epoch 3, Batch 48/157, Loss: 2.7023\n",
      "Epoch 3, Batch 49/157, Loss: 2.6910\n",
      "Epoch 3, Batch 50/157, Loss: 2.7369\n",
      "Epoch 3, Batch 51/157, Loss: 2.7556\n",
      "Epoch 3, Batch 52/157, Loss: 2.6612\n",
      "Epoch 3, Batch 53/157, Loss: 2.7092\n",
      "Epoch 3, Batch 54/157, Loss: 2.7278\n",
      "Epoch 3, Batch 55/157, Loss: 2.7549\n",
      "Epoch 3, Batch 56/157, Loss: 2.7056\n",
      "Epoch 3, Batch 57/157, Loss: 2.7249\n",
      "Epoch 3, Batch 58/157, Loss: 2.7734\n",
      "Epoch 3, Batch 59/157, Loss: 2.7376\n",
      "Epoch 3, Batch 60/157, Loss: 2.7591\n",
      "Epoch 3, Batch 61/157, Loss: 2.6893\n",
      "Epoch 3, Batch 62/157, Loss: 2.7266\n",
      "Epoch 3, Batch 63/157, Loss: 2.7577\n",
      "Epoch 3, Batch 64/157, Loss: 2.7708\n",
      "Epoch 3, Batch 65/157, Loss: 2.6585\n",
      "Epoch 3, Batch 66/157, Loss: 2.7211\n",
      "Epoch 3, Batch 67/157, Loss: 2.7860\n",
      "Epoch 3, Batch 68/157, Loss: 2.7308\n",
      "Epoch 3, Batch 69/157, Loss: 2.6611\n",
      "Epoch 3, Batch 70/157, Loss: 2.6938\n",
      "Epoch 3, Batch 71/157, Loss: 2.7267\n",
      "Epoch 3, Batch 72/157, Loss: 2.7851\n",
      "Epoch 3, Batch 73/157, Loss: 2.7903\n",
      "Epoch 3, Batch 74/157, Loss: 2.7285\n",
      "Epoch 3, Batch 75/157, Loss: 2.7069\n",
      "Epoch 3, Batch 76/157, Loss: 2.6662\n",
      "Epoch 3, Batch 77/157, Loss: 2.6691\n",
      "Epoch 3, Batch 78/157, Loss: 2.7040\n",
      "Epoch 3, Batch 79/157, Loss: 2.6943\n",
      "Epoch 3, Batch 80/157, Loss: 2.8105\n",
      "Epoch 3, Batch 81/157, Loss: 2.8344\n",
      "Epoch 3, Batch 82/157, Loss: 2.6883\n",
      "Epoch 3, Batch 83/157, Loss: 2.8028\n",
      "Epoch 3, Batch 84/157, Loss: 2.7318\n",
      "Epoch 3, Batch 85/157, Loss: 2.7351\n",
      "Epoch 3, Batch 86/157, Loss: 2.7010\n",
      "Epoch 3, Batch 87/157, Loss: 2.7870\n",
      "Epoch 3, Batch 88/157, Loss: 2.7548\n",
      "Epoch 3, Batch 89/157, Loss: 2.7848\n",
      "Epoch 3, Batch 90/157, Loss: 2.7112\n",
      "Epoch 3, Batch 91/157, Loss: 2.7743\n",
      "Epoch 3, Batch 92/157, Loss: 2.6846\n",
      "Epoch 3, Batch 93/157, Loss: 2.7552\n",
      "Epoch 3, Batch 94/157, Loss: 2.7545\n",
      "Epoch 3, Batch 95/157, Loss: 2.7028\n",
      "Epoch 3, Batch 96/157, Loss: 2.7080\n",
      "Epoch 3, Batch 97/157, Loss: 2.7483\n",
      "Epoch 3, Batch 98/157, Loss: 2.7301\n",
      "Epoch 3, Batch 99/157, Loss: 2.6623\n",
      "Epoch 3, Batch 100/157, Loss: 2.8308\n",
      "Epoch 3, Batch 101/157, Loss: 2.7980\n",
      "Epoch 3, Batch 102/157, Loss: 2.7442\n",
      "Epoch 3, Batch 103/157, Loss: 2.6900\n",
      "Epoch 3, Batch 104/157, Loss: 2.7931\n",
      "Epoch 3, Batch 105/157, Loss: 2.7089\n",
      "Epoch 3, Batch 106/157, Loss: 2.7801\n",
      "Epoch 3, Batch 107/157, Loss: 2.7792\n",
      "Epoch 3, Batch 108/157, Loss: 2.6384\n",
      "Epoch 3, Batch 109/157, Loss: 2.7560\n",
      "Epoch 3, Batch 110/157, Loss: 2.7591\n",
      "Epoch 3, Batch 111/157, Loss: 2.7606\n",
      "Epoch 3, Batch 112/157, Loss: 2.6931\n",
      "Epoch 3, Batch 113/157, Loss: 2.7934\n",
      "Epoch 3, Batch 114/157, Loss: 2.7242\n",
      "Epoch 3, Batch 115/157, Loss: 2.5995\n",
      "Epoch 3, Batch 116/157, Loss: 2.7350\n",
      "Epoch 3, Batch 117/157, Loss: 2.6805\n",
      "Epoch 3, Batch 118/157, Loss: 2.8298\n",
      "Epoch 3, Batch 119/157, Loss: 2.7632\n",
      "Epoch 3, Batch 120/157, Loss: 2.7829\n",
      "Epoch 3, Batch 121/157, Loss: 2.7635\n",
      "Epoch 3, Batch 122/157, Loss: 2.8865\n",
      "Epoch 3, Batch 123/157, Loss: 2.7331\n",
      "Epoch 3, Batch 124/157, Loss: 2.7710\n",
      "Epoch 3, Batch 125/157, Loss: 2.8131\n",
      "Epoch 3, Batch 126/157, Loss: 2.7474\n",
      "Epoch 3, Batch 127/157, Loss: 2.7223\n",
      "Epoch 3, Batch 128/157, Loss: 2.7889\n",
      "Epoch 3, Batch 129/157, Loss: 2.7627\n",
      "Epoch 3, Batch 130/157, Loss: 2.7175\n",
      "Epoch 3, Batch 131/157, Loss: 2.7455\n",
      "Epoch 3, Batch 132/157, Loss: 2.6493\n",
      "Epoch 3, Batch 133/157, Loss: 2.8126\n",
      "Epoch 3, Batch 134/157, Loss: 2.7330\n",
      "Epoch 3, Batch 135/157, Loss: 2.7383\n",
      "Epoch 3, Batch 136/157, Loss: 2.7484\n",
      "Epoch 3, Batch 137/157, Loss: 2.8726\n",
      "Epoch 3, Batch 138/157, Loss: 2.7543\n",
      "Epoch 3, Batch 139/157, Loss: 2.8526\n",
      "Epoch 3, Batch 140/157, Loss: 2.7329\n",
      "Epoch 3, Batch 141/157, Loss: 2.7507\n",
      "Epoch 3, Batch 142/157, Loss: 2.7338\n",
      "Epoch 3, Batch 143/157, Loss: 2.6752\n",
      "Epoch 3, Batch 144/157, Loss: 2.6927\n",
      "Epoch 3, Batch 145/157, Loss: 2.7386\n",
      "Epoch 3, Batch 146/157, Loss: 2.7053\n",
      "Epoch 3, Batch 147/157, Loss: 2.7160\n",
      "Epoch 3, Batch 148/157, Loss: 2.7549\n",
      "Epoch 3, Batch 149/157, Loss: 2.7318\n",
      "Epoch 3, Batch 150/157, Loss: 2.7016\n",
      "Epoch 3, Batch 151/157, Loss: 2.7499\n",
      "Epoch 3, Batch 152/157, Loss: 2.7669\n",
      "Epoch 3, Batch 153/157, Loss: 2.7332\n",
      "Epoch 3, Batch 154/157, Loss: 2.7288\n",
      "Epoch 3, Batch 155/157, Loss: 2.6953\n",
      "Epoch 3, Batch 156/157, Loss: 2.6962\n",
      "Epoch 3, Batch 157/157, Loss: 2.7534\n",
      "Epoch 3/50, Average Loss: 2.7428\n",
      "Epoch 4, Batch 1/157, Loss: 2.7423\n",
      "Epoch 4, Batch 2/157, Loss: 2.7498\n",
      "Epoch 4, Batch 3/157, Loss: 2.7544\n",
      "Epoch 4, Batch 4/157, Loss: 2.7260\n",
      "Epoch 4, Batch 5/157, Loss: 2.7595\n",
      "Epoch 4, Batch 6/157, Loss: 2.6719\n",
      "Epoch 4, Batch 7/157, Loss: 2.7172\n",
      "Epoch 4, Batch 8/157, Loss: 2.8089\n",
      "Epoch 4, Batch 9/157, Loss: 2.7632\n",
      "Epoch 4, Batch 10/157, Loss: 2.7731\n",
      "Epoch 4, Batch 11/157, Loss: 2.6864\n",
      "Epoch 4, Batch 12/157, Loss: 2.7536\n",
      "Epoch 4, Batch 13/157, Loss: 2.7921\n",
      "Epoch 4, Batch 14/157, Loss: 2.7707\n",
      "Epoch 4, Batch 15/157, Loss: 2.7585\n",
      "Epoch 4, Batch 16/157, Loss: 2.6946\n",
      "Epoch 4, Batch 17/157, Loss: 2.7343\n",
      "Epoch 4, Batch 18/157, Loss: 2.6567\n",
      "Epoch 4, Batch 19/157, Loss: 2.7631\n",
      "Epoch 4, Batch 20/157, Loss: 2.7145\n",
      "Epoch 4, Batch 21/157, Loss: 2.6792\n",
      "Epoch 4, Batch 22/157, Loss: 2.8474\n",
      "Epoch 4, Batch 23/157, Loss: 2.8208\n",
      "Epoch 4, Batch 24/157, Loss: 2.7523\n",
      "Epoch 4, Batch 25/157, Loss: 2.6660\n",
      "Epoch 4, Batch 26/157, Loss: 2.6941\n",
      "Epoch 4, Batch 27/157, Loss: 2.8233\n",
      "Epoch 4, Batch 28/157, Loss: 2.7263\n",
      "Epoch 4, Batch 29/157, Loss: 2.7545\n",
      "Epoch 4, Batch 30/157, Loss: 2.7420\n",
      "Epoch 4, Batch 31/157, Loss: 2.6921\n",
      "Epoch 4, Batch 32/157, Loss: 2.7061\n",
      "Epoch 4, Batch 33/157, Loss: 2.8407\n",
      "Epoch 4, Batch 34/157, Loss: 2.8029\n",
      "Epoch 4, Batch 35/157, Loss: 2.7636\n",
      "Epoch 4, Batch 36/157, Loss: 2.7572\n",
      "Epoch 4, Batch 37/157, Loss: 2.7760\n",
      "Epoch 4, Batch 38/157, Loss: 2.6995\n",
      "Epoch 4, Batch 39/157, Loss: 2.7830\n",
      "Epoch 4, Batch 40/157, Loss: 2.7686\n",
      "Epoch 4, Batch 41/157, Loss: 2.7445\n",
      "Epoch 4, Batch 42/157, Loss: 2.6891\n",
      "Epoch 4, Batch 43/157, Loss: 2.7070\n",
      "Epoch 4, Batch 44/157, Loss: 2.7349\n",
      "Epoch 4, Batch 45/157, Loss: 2.6955\n",
      "Epoch 4, Batch 46/157, Loss: 2.7848\n",
      "Epoch 4, Batch 47/157, Loss: 2.8097\n",
      "Epoch 4, Batch 48/157, Loss: 2.7753\n",
      "Epoch 4, Batch 49/157, Loss: 2.7826\n",
      "Epoch 4, Batch 50/157, Loss: 2.6805\n",
      "Epoch 4, Batch 51/157, Loss: 2.7340\n",
      "Epoch 4, Batch 52/157, Loss: 2.7696\n",
      "Epoch 4, Batch 53/157, Loss: 2.7562\n",
      "Epoch 4, Batch 54/157, Loss: 2.6892\n",
      "Epoch 4, Batch 55/157, Loss: 2.6807\n",
      "Epoch 4, Batch 56/157, Loss: 2.7769\n",
      "Epoch 4, Batch 57/157, Loss: 2.7736\n",
      "Epoch 4, Batch 58/157, Loss: 2.7039\n",
      "Epoch 4, Batch 59/157, Loss: 2.7592\n",
      "Epoch 4, Batch 60/157, Loss: 2.7172\n",
      "Epoch 4, Batch 61/157, Loss: 2.7658\n",
      "Epoch 4, Batch 62/157, Loss: 2.6924\n",
      "Epoch 4, Batch 63/157, Loss: 2.7699\n",
      "Epoch 4, Batch 64/157, Loss: 2.7093\n",
      "Epoch 4, Batch 65/157, Loss: 2.7200\n",
      "Epoch 4, Batch 66/157, Loss: 2.7044\n",
      "Epoch 4, Batch 67/157, Loss: 2.8103\n",
      "Epoch 4, Batch 68/157, Loss: 2.7620\n",
      "Epoch 4, Batch 69/157, Loss: 2.8113\n",
      "Epoch 4, Batch 70/157, Loss: 2.6885\n",
      "Epoch 4, Batch 71/157, Loss: 2.8185\n",
      "Epoch 4, Batch 72/157, Loss: 2.7703\n",
      "Epoch 4, Batch 73/157, Loss: 2.8355\n",
      "Epoch 4, Batch 74/157, Loss: 2.7045\n",
      "Epoch 4, Batch 75/157, Loss: 2.7073\n",
      "Epoch 4, Batch 76/157, Loss: 2.8161\n",
      "Epoch 4, Batch 77/157, Loss: 2.7431\n",
      "Epoch 4, Batch 78/157, Loss: 2.7724\n",
      "Epoch 4, Batch 79/157, Loss: 2.7458\n",
      "Epoch 4, Batch 80/157, Loss: 2.6570\n",
      "Epoch 4, Batch 81/157, Loss: 2.7156\n",
      "Epoch 4, Batch 82/157, Loss: 2.8465\n",
      "Epoch 4, Batch 83/157, Loss: 2.7199\n",
      "Epoch 4, Batch 84/157, Loss: 2.8164\n",
      "Epoch 4, Batch 85/157, Loss: 2.7912\n",
      "Epoch 4, Batch 86/157, Loss: 2.8356\n",
      "Epoch 4, Batch 87/157, Loss: 2.7230\n",
      "Epoch 4, Batch 88/157, Loss: 2.7400\n",
      "Epoch 4, Batch 89/157, Loss: 2.7567\n",
      "Epoch 4, Batch 90/157, Loss: 2.6940\n",
      "Epoch 4, Batch 91/157, Loss: 2.7184\n",
      "Epoch 4, Batch 92/157, Loss: 2.7383\n",
      "Epoch 4, Batch 93/157, Loss: 2.7052\n",
      "Epoch 4, Batch 94/157, Loss: 2.7382\n",
      "Epoch 4, Batch 95/157, Loss: 2.6521\n",
      "Epoch 4, Batch 96/157, Loss: 2.6827\n",
      "Epoch 4, Batch 97/157, Loss: 2.6790\n",
      "Epoch 4, Batch 98/157, Loss: 2.7433\n",
      "Epoch 4, Batch 99/157, Loss: 2.6989\n",
      "Epoch 4, Batch 100/157, Loss: 2.7956\n",
      "Epoch 4, Batch 101/157, Loss: 2.7817\n",
      "Epoch 4, Batch 102/157, Loss: 2.8075\n",
      "Epoch 4, Batch 103/157, Loss: 2.6477\n",
      "Epoch 4, Batch 104/157, Loss: 2.8172\n",
      "Epoch 4, Batch 105/157, Loss: 2.7896\n",
      "Epoch 4, Batch 106/157, Loss: 2.7737\n",
      "Epoch 4, Batch 107/157, Loss: 2.7970\n",
      "Epoch 4, Batch 108/157, Loss: 2.7698\n",
      "Epoch 4, Batch 109/157, Loss: 2.7383\n",
      "Epoch 4, Batch 110/157, Loss: 2.7457\n",
      "Epoch 4, Batch 111/157, Loss: 2.6846\n",
      "Epoch 4, Batch 112/157, Loss: 2.7111\n",
      "Epoch 4, Batch 113/157, Loss: 2.8650\n",
      "Epoch 4, Batch 114/157, Loss: 2.7304\n",
      "Epoch 4, Batch 115/157, Loss: 2.8404\n",
      "Epoch 4, Batch 116/157, Loss: 2.6823\n",
      "Epoch 4, Batch 117/157, Loss: 2.9063\n",
      "Epoch 4, Batch 118/157, Loss: 2.6993\n",
      "Epoch 4, Batch 119/157, Loss: 2.7970\n",
      "Epoch 4, Batch 120/157, Loss: 2.8894\n",
      "Epoch 4, Batch 121/157, Loss: 2.7263\n",
      "Epoch 4, Batch 122/157, Loss: 2.7515\n",
      "Epoch 4, Batch 123/157, Loss: 2.7502\n",
      "Epoch 4, Batch 124/157, Loss: 2.8421\n",
      "Epoch 4, Batch 125/157, Loss: 2.7043\n",
      "Epoch 4, Batch 126/157, Loss: 2.7558\n",
      "Epoch 4, Batch 127/157, Loss: 2.6545\n",
      "Epoch 4, Batch 128/157, Loss: 2.6968\n",
      "Epoch 4, Batch 129/157, Loss: 2.7438\n",
      "Epoch 4, Batch 130/157, Loss: 2.7254\n",
      "Epoch 4, Batch 131/157, Loss: 2.7192\n",
      "Epoch 4, Batch 132/157, Loss: 2.6945\n",
      "Epoch 4, Batch 133/157, Loss: 2.7333\n",
      "Epoch 4, Batch 134/157, Loss: 2.7143\n",
      "Epoch 4, Batch 135/157, Loss: 2.7573\n",
      "Epoch 4, Batch 136/157, Loss: 2.7616\n",
      "Epoch 4, Batch 137/157, Loss: 2.7056\n",
      "Epoch 4, Batch 138/157, Loss: 2.7130\n",
      "Epoch 4, Batch 139/157, Loss: 2.6931\n",
      "Epoch 4, Batch 140/157, Loss: 2.7781\n",
      "Epoch 4, Batch 141/157, Loss: 2.6940\n",
      "Epoch 4, Batch 142/157, Loss: 2.7009\n",
      "Epoch 4, Batch 143/157, Loss: 2.7123\n",
      "Epoch 4, Batch 144/157, Loss: 2.8105\n",
      "Epoch 4, Batch 145/157, Loss: 2.7527\n",
      "Epoch 4, Batch 146/157, Loss: 2.7821\n",
      "Epoch 4, Batch 147/157, Loss: 2.7113\n",
      "Epoch 4, Batch 148/157, Loss: 2.6589\n",
      "Epoch 4, Batch 149/157, Loss: 2.7315\n",
      "Epoch 4, Batch 150/157, Loss: 2.6647\n",
      "Epoch 4, Batch 151/157, Loss: 2.7867\n",
      "Epoch 4, Batch 152/157, Loss: 2.7079\n",
      "Epoch 4, Batch 153/157, Loss: 2.7272\n",
      "Epoch 4, Batch 154/157, Loss: 2.7432\n",
      "Epoch 4, Batch 155/157, Loss: 2.6648\n",
      "Epoch 4, Batch 156/157, Loss: 2.7199\n",
      "Epoch 4, Batch 157/157, Loss: 2.9107\n",
      "Epoch 4/50, Average Loss: 2.7453\n",
      "Epoch 5, Batch 1/157, Loss: 2.6788\n",
      "Epoch 5, Batch 2/157, Loss: 2.7651\n",
      "Epoch 5, Batch 3/157, Loss: 2.6363\n",
      "Epoch 5, Batch 4/157, Loss: 2.7593\n",
      "Epoch 5, Batch 5/157, Loss: 2.8128\n",
      "Epoch 5, Batch 6/157, Loss: 2.8337\n",
      "Epoch 5, Batch 7/157, Loss: 2.7455\n",
      "Epoch 5, Batch 8/157, Loss: 2.7967\n",
      "Epoch 5, Batch 9/157, Loss: 2.7756\n",
      "Epoch 5, Batch 10/157, Loss: 2.6321\n",
      "Epoch 5, Batch 11/157, Loss: 2.7134\n",
      "Epoch 5, Batch 12/157, Loss: 2.7550\n",
      "Epoch 5, Batch 13/157, Loss: 2.6540\n",
      "Epoch 5, Batch 14/157, Loss: 2.7455\n",
      "Epoch 5, Batch 15/157, Loss: 2.7171\n",
      "Epoch 5, Batch 16/157, Loss: 2.8171\n",
      "Epoch 5, Batch 17/157, Loss: 2.6906\n",
      "Epoch 5, Batch 18/157, Loss: 2.7102\n",
      "Epoch 5, Batch 19/157, Loss: 2.7228\n",
      "Epoch 5, Batch 20/157, Loss: 2.7435\n",
      "Epoch 5, Batch 21/157, Loss: 2.6780\n",
      "Epoch 5, Batch 22/157, Loss: 2.7652\n",
      "Epoch 5, Batch 23/157, Loss: 2.8586\n",
      "Epoch 5, Batch 24/157, Loss: 2.7520\n",
      "Epoch 5, Batch 25/157, Loss: 2.7590\n",
      "Epoch 5, Batch 26/157, Loss: 2.7002\n",
      "Epoch 5, Batch 27/157, Loss: 2.7980\n",
      "Epoch 5, Batch 28/157, Loss: 2.7250\n",
      "Epoch 5, Batch 29/157, Loss: 2.7673\n",
      "Epoch 5, Batch 30/157, Loss: 2.7650\n",
      "Epoch 5, Batch 31/157, Loss: 2.7045\n",
      "Epoch 5, Batch 32/157, Loss: 2.7178\n",
      "Epoch 5, Batch 33/157, Loss: 2.7760\n",
      "Epoch 5, Batch 34/157, Loss: 2.7684\n",
      "Epoch 5, Batch 35/157, Loss: 2.7726\n",
      "Epoch 5, Batch 36/157, Loss: 2.7930\n",
      "Epoch 5, Batch 37/157, Loss: 2.7621\n",
      "Epoch 5, Batch 38/157, Loss: 2.7112\n",
      "Epoch 5, Batch 39/157, Loss: 2.7728\n",
      "Epoch 5, Batch 40/157, Loss: 2.7025\n",
      "Epoch 5, Batch 41/157, Loss: 2.8040\n",
      "Epoch 5, Batch 42/157, Loss: 2.7000\n",
      "Epoch 5, Batch 43/157, Loss: 2.7718\n",
      "Epoch 5, Batch 44/157, Loss: 2.7721\n",
      "Epoch 5, Batch 45/157, Loss: 2.7783\n",
      "Epoch 5, Batch 46/157, Loss: 2.6914\n",
      "Epoch 5, Batch 47/157, Loss: 2.6786\n",
      "Epoch 5, Batch 48/157, Loss: 2.7789\n",
      "Epoch 5, Batch 49/157, Loss: 2.7129\n",
      "Epoch 5, Batch 50/157, Loss: 2.6383\n",
      "Epoch 5, Batch 51/157, Loss: 2.7271\n",
      "Epoch 5, Batch 52/157, Loss: 2.8003\n",
      "Epoch 5, Batch 53/157, Loss: 2.7482\n",
      "Epoch 5, Batch 54/157, Loss: 2.7816\n",
      "Epoch 5, Batch 55/157, Loss: 2.7048\n",
      "Epoch 5, Batch 56/157, Loss: 2.7610\n",
      "Epoch 5, Batch 57/157, Loss: 2.8021\n",
      "Epoch 5, Batch 58/157, Loss: 2.6820\n",
      "Epoch 5, Batch 59/157, Loss: 2.6886\n",
      "Epoch 5, Batch 60/157, Loss: 2.7480\n",
      "Epoch 5, Batch 61/157, Loss: 2.7125\n",
      "Epoch 5, Batch 62/157, Loss: 2.7910\n",
      "Epoch 5, Batch 63/157, Loss: 2.7809\n",
      "Epoch 5, Batch 64/157, Loss: 2.7294\n",
      "Epoch 5, Batch 65/157, Loss: 2.7477\n",
      "Epoch 5, Batch 66/157, Loss: 2.7173\n",
      "Epoch 5, Batch 67/157, Loss: 2.7827\n",
      "Epoch 5, Batch 68/157, Loss: 2.7117\n",
      "Epoch 5, Batch 69/157, Loss: 2.7271\n",
      "Epoch 5, Batch 70/157, Loss: 2.7421\n",
      "Epoch 5, Batch 71/157, Loss: 2.6977\n",
      "Epoch 5, Batch 72/157, Loss: 2.6786\n",
      "Epoch 5, Batch 73/157, Loss: 2.7276\n",
      "Epoch 5, Batch 74/157, Loss: 2.7123\n",
      "Epoch 5, Batch 75/157, Loss: 2.7279\n",
      "Epoch 5, Batch 76/157, Loss: 2.7211\n",
      "Epoch 5, Batch 77/157, Loss: 2.8297\n",
      "Epoch 5, Batch 78/157, Loss: 2.7361\n",
      "Epoch 5, Batch 79/157, Loss: 2.7981\n",
      "Epoch 5, Batch 80/157, Loss: 2.7890\n",
      "Epoch 5, Batch 81/157, Loss: 2.8138\n",
      "Epoch 5, Batch 82/157, Loss: 2.7345\n",
      "Epoch 5, Batch 83/157, Loss: 2.7407\n",
      "Epoch 5, Batch 84/157, Loss: 2.7225\n",
      "Epoch 5, Batch 85/157, Loss: 2.6971\n",
      "Epoch 5, Batch 86/157, Loss: 2.7847\n",
      "Epoch 5, Batch 87/157, Loss: 2.7456\n",
      "Epoch 5, Batch 88/157, Loss: 2.6951\n",
      "Epoch 5, Batch 89/157, Loss: 2.7206\n",
      "Epoch 5, Batch 90/157, Loss: 2.6683\n",
      "Epoch 5, Batch 91/157, Loss: 2.8032\n",
      "Epoch 5, Batch 92/157, Loss: 2.7214\n",
      "Epoch 5, Batch 93/157, Loss: 2.7468\n",
      "Epoch 5, Batch 94/157, Loss: 2.7477\n",
      "Epoch 5, Batch 95/157, Loss: 2.7789\n",
      "Epoch 5, Batch 96/157, Loss: 2.8393\n",
      "Epoch 5, Batch 97/157, Loss: 2.7103\n",
      "Epoch 5, Batch 98/157, Loss: 2.7616\n",
      "Epoch 5, Batch 99/157, Loss: 2.7922\n",
      "Epoch 5, Batch 100/157, Loss: 2.7746\n",
      "Epoch 5, Batch 101/157, Loss: 2.7383\n",
      "Epoch 5, Batch 102/157, Loss: 2.7436\n",
      "Epoch 5, Batch 103/157, Loss: 2.6984\n",
      "Epoch 5, Batch 104/157, Loss: 2.7205\n",
      "Epoch 5, Batch 105/157, Loss: 2.7072\n",
      "Epoch 5, Batch 106/157, Loss: 2.7023\n",
      "Epoch 5, Batch 107/157, Loss: 2.7473\n",
      "Epoch 5, Batch 108/157, Loss: 2.8308\n",
      "Epoch 5, Batch 109/157, Loss: 2.7921\n",
      "Epoch 5, Batch 110/157, Loss: 2.7973\n",
      "Epoch 5, Batch 111/157, Loss: 2.7713\n",
      "Epoch 5, Batch 112/157, Loss: 2.7126\n",
      "Epoch 5, Batch 113/157, Loss: 2.6936\n",
      "Epoch 5, Batch 114/157, Loss: 2.7471\n",
      "Epoch 5, Batch 115/157, Loss: 2.7260\n",
      "Epoch 5, Batch 116/157, Loss: 2.7672\n",
      "Epoch 5, Batch 117/157, Loss: 2.6829\n",
      "Epoch 5, Batch 118/157, Loss: 2.6794\n",
      "Epoch 5, Batch 119/157, Loss: 2.7373\n",
      "Epoch 5, Batch 120/157, Loss: 2.7392\n",
      "Epoch 5, Batch 121/157, Loss: 2.7420\n",
      "Epoch 5, Batch 122/157, Loss: 2.7153\n",
      "Epoch 5, Batch 123/157, Loss: 2.8505\n",
      "Epoch 5, Batch 124/157, Loss: 2.7669\n",
      "Epoch 5, Batch 125/157, Loss: 2.8629\n",
      "Epoch 5, Batch 126/157, Loss: 2.7098\n",
      "Epoch 5, Batch 127/157, Loss: 2.7057\n",
      "Epoch 5, Batch 128/157, Loss: 2.6250\n",
      "Epoch 5, Batch 129/157, Loss: 2.7184\n",
      "Epoch 5, Batch 130/157, Loss: 2.7210\n",
      "Epoch 5, Batch 131/157, Loss: 2.7474\n",
      "Epoch 5, Batch 132/157, Loss: 2.7170\n",
      "Epoch 5, Batch 133/157, Loss: 2.7246\n",
      "Epoch 5, Batch 134/157, Loss: 2.7304\n",
      "Epoch 5, Batch 135/157, Loss: 2.7645\n",
      "Epoch 5, Batch 136/157, Loss: 2.7750\n",
      "Epoch 5, Batch 137/157, Loss: 2.8291\n",
      "Epoch 5, Batch 138/157, Loss: 2.6894\n",
      "Epoch 5, Batch 139/157, Loss: 2.7937\n",
      "Epoch 5, Batch 140/157, Loss: 2.8021\n",
      "Epoch 5, Batch 141/157, Loss: 2.7887\n",
      "Epoch 5, Batch 142/157, Loss: 2.6879\n",
      "Epoch 5, Batch 143/157, Loss: 2.6797\n",
      "Epoch 5, Batch 144/157, Loss: 2.7607\n",
      "Epoch 5, Batch 145/157, Loss: 2.7641\n",
      "Epoch 5, Batch 146/157, Loss: 2.7436\n",
      "Epoch 5, Batch 147/157, Loss: 2.7975\n",
      "Epoch 5, Batch 148/157, Loss: 2.6524\n",
      "Epoch 5, Batch 149/157, Loss: 2.7265\n",
      "Epoch 5, Batch 150/157, Loss: 2.7414\n",
      "Epoch 5, Batch 151/157, Loss: 2.7527\n",
      "Epoch 5, Batch 152/157, Loss: 2.7702\n",
      "Epoch 5, Batch 153/157, Loss: 2.7712\n",
      "Epoch 5, Batch 154/157, Loss: 2.6825\n",
      "Epoch 5, Batch 155/157, Loss: 2.7266\n",
      "Epoch 5, Batch 156/157, Loss: 2.7537\n",
      "Epoch 5, Batch 157/157, Loss: 2.8747\n",
      "Epoch 5/50, Average Loss: 2.7438\n",
      "Epoch 6, Batch 1/157, Loss: 2.7993\n",
      "Epoch 6, Batch 2/157, Loss: 2.7926\n",
      "Epoch 6, Batch 3/157, Loss: 2.6887\n",
      "Epoch 6, Batch 4/157, Loss: 2.7390\n",
      "Epoch 6, Batch 5/157, Loss: 2.6670\n",
      "Epoch 6, Batch 6/157, Loss: 2.8715\n",
      "Epoch 6, Batch 7/157, Loss: 2.7458\n",
      "Epoch 6, Batch 8/157, Loss: 2.7604\n",
      "Epoch 6, Batch 9/157, Loss: 2.8739\n",
      "Epoch 6, Batch 10/157, Loss: 2.6801\n",
      "Epoch 6, Batch 11/157, Loss: 2.7930\n",
      "Epoch 6, Batch 12/157, Loss: 2.7963\n",
      "Epoch 6, Batch 13/157, Loss: 2.6883\n",
      "Epoch 6, Batch 14/157, Loss: 2.7192\n",
      "Epoch 6, Batch 15/157, Loss: 2.6937\n",
      "Epoch 6, Batch 16/157, Loss: 2.7068\n",
      "Epoch 6, Batch 17/157, Loss: 2.7336\n",
      "Epoch 6, Batch 18/157, Loss: 2.6618\n",
      "Epoch 6, Batch 19/157, Loss: 2.8580\n",
      "Epoch 6, Batch 20/157, Loss: 2.8114\n",
      "Epoch 6, Batch 21/157, Loss: 2.7811\n",
      "Epoch 6, Batch 22/157, Loss: 2.7127\n",
      "Epoch 6, Batch 23/157, Loss: 2.8046\n",
      "Epoch 6, Batch 24/157, Loss: 2.7482\n",
      "Epoch 6, Batch 25/157, Loss: 2.7129\n",
      "Epoch 6, Batch 26/157, Loss: 2.6303\n",
      "Epoch 6, Batch 27/157, Loss: 2.8179\n",
      "Epoch 6, Batch 28/157, Loss: 2.7282\n",
      "Epoch 6, Batch 29/157, Loss: 2.6910\n",
      "Epoch 6, Batch 30/157, Loss: 2.8263\n",
      "Epoch 6, Batch 31/157, Loss: 2.6667\n",
      "Epoch 6, Batch 32/157, Loss: 2.7807\n",
      "Epoch 6, Batch 33/157, Loss: 2.7752\n",
      "Epoch 6, Batch 34/157, Loss: 2.6962\n",
      "Epoch 6, Batch 35/157, Loss: 2.7967\n",
      "Epoch 6, Batch 36/157, Loss: 2.7039\n",
      "Epoch 6, Batch 37/157, Loss: 2.7405\n",
      "Epoch 6, Batch 38/157, Loss: 2.7143\n",
      "Epoch 6, Batch 39/157, Loss: 2.7401\n",
      "Epoch 6, Batch 40/157, Loss: 2.8180\n",
      "Epoch 6, Batch 41/157, Loss: 2.7572\n",
      "Epoch 6, Batch 42/157, Loss: 2.6900\n",
      "Epoch 6, Batch 43/157, Loss: 2.7001\n",
      "Epoch 6, Batch 44/157, Loss: 2.6588\n",
      "Epoch 6, Batch 45/157, Loss: 2.7537\n",
      "Epoch 6, Batch 46/157, Loss: 2.7269\n",
      "Epoch 6, Batch 47/157, Loss: 2.7154\n",
      "Epoch 6, Batch 48/157, Loss: 2.7919\n",
      "Epoch 6, Batch 49/157, Loss: 2.6868\n",
      "Epoch 6, Batch 50/157, Loss: 2.8203\n",
      "Epoch 6, Batch 51/157, Loss: 2.7002\n",
      "Epoch 6, Batch 52/157, Loss: 2.8035\n",
      "Epoch 6, Batch 53/157, Loss: 2.6728\n",
      "Epoch 6, Batch 54/157, Loss: 2.7563\n",
      "Epoch 6, Batch 55/157, Loss: 2.7582\n",
      "Epoch 6, Batch 56/157, Loss: 2.7378\n",
      "Epoch 6, Batch 57/157, Loss: 2.8166\n",
      "Epoch 6, Batch 58/157, Loss: 2.6922\n",
      "Epoch 6, Batch 59/157, Loss: 2.7210\n",
      "Epoch 6, Batch 60/157, Loss: 2.7717\n",
      "Epoch 6, Batch 61/157, Loss: 2.7021\n",
      "Epoch 6, Batch 62/157, Loss: 2.7311\n",
      "Epoch 6, Batch 63/157, Loss: 2.7217\n",
      "Epoch 6, Batch 64/157, Loss: 2.7878\n",
      "Epoch 6, Batch 65/157, Loss: 2.7078\n",
      "Epoch 6, Batch 66/157, Loss: 2.7386\n",
      "Epoch 6, Batch 67/157, Loss: 2.6707\n",
      "Epoch 6, Batch 68/157, Loss: 2.6593\n",
      "Epoch 6, Batch 69/157, Loss: 2.7401\n",
      "Epoch 6, Batch 70/157, Loss: 2.7883\n",
      "Epoch 6, Batch 71/157, Loss: 2.7947\n",
      "Epoch 6, Batch 72/157, Loss: 2.7752\n",
      "Epoch 6, Batch 73/157, Loss: 2.6755\n",
      "Epoch 6, Batch 74/157, Loss: 2.6832\n",
      "Epoch 6, Batch 75/157, Loss: 2.7562\n",
      "Epoch 6, Batch 76/157, Loss: 2.7618\n",
      "Epoch 6, Batch 77/157, Loss: 2.7511\n",
      "Epoch 6, Batch 78/157, Loss: 2.7108\n",
      "Epoch 6, Batch 79/157, Loss: 2.7915\n",
      "Epoch 6, Batch 80/157, Loss: 2.7482\n",
      "Epoch 6, Batch 81/157, Loss: 2.8297\n",
      "Epoch 6, Batch 82/157, Loss: 2.7326\n",
      "Epoch 6, Batch 83/157, Loss: 2.7494\n",
      "Epoch 6, Batch 84/157, Loss: 2.7499\n",
      "Epoch 6, Batch 85/157, Loss: 2.7889\n",
      "Epoch 6, Batch 86/157, Loss: 2.7306\n",
      "Epoch 6, Batch 87/157, Loss: 2.7411\n",
      "Epoch 6, Batch 88/157, Loss: 2.7621\n",
      "Epoch 6, Batch 89/157, Loss: 2.7774\n",
      "Epoch 6, Batch 90/157, Loss: 2.7740\n",
      "Epoch 6, Batch 91/157, Loss: 2.7255\n",
      "Epoch 6, Batch 92/157, Loss: 2.7815\n",
      "Epoch 6, Batch 93/157, Loss: 2.7515\n",
      "Epoch 6, Batch 94/157, Loss: 2.7727\n",
      "Epoch 6, Batch 95/157, Loss: 2.7504\n",
      "Epoch 6, Batch 96/157, Loss: 2.7440\n",
      "Epoch 6, Batch 97/157, Loss: 2.7747\n",
      "Epoch 6, Batch 98/157, Loss: 2.6947\n",
      "Epoch 6, Batch 99/157, Loss: 2.6688\n",
      "Epoch 6, Batch 100/157, Loss: 2.7482\n",
      "Epoch 6, Batch 101/157, Loss: 2.7712\n",
      "Epoch 6, Batch 102/157, Loss: 2.6930\n",
      "Epoch 6, Batch 103/157, Loss: 2.7393\n",
      "Epoch 6, Batch 104/157, Loss: 2.7523\n",
      "Epoch 6, Batch 105/157, Loss: 2.7023\n",
      "Epoch 6, Batch 106/157, Loss: 2.6923\n",
      "Epoch 6, Batch 107/157, Loss: 2.7598\n",
      "Epoch 6, Batch 108/157, Loss: 2.8071\n",
      "Epoch 6, Batch 109/157, Loss: 2.7110\n",
      "Epoch 6, Batch 110/157, Loss: 2.6954\n",
      "Epoch 6, Batch 111/157, Loss: 2.7020\n",
      "Epoch 6, Batch 112/157, Loss: 2.7088\n",
      "Epoch 6, Batch 113/157, Loss: 2.7504\n",
      "Epoch 6, Batch 114/157, Loss: 2.6965\n",
      "Epoch 6, Batch 115/157, Loss: 2.7182\n",
      "Epoch 6, Batch 116/157, Loss: 2.7339\n",
      "Epoch 6, Batch 117/157, Loss: 2.7129\n",
      "Epoch 6, Batch 118/157, Loss: 2.7286\n",
      "Epoch 6, Batch 119/157, Loss: 2.7011\n",
      "Epoch 6, Batch 120/157, Loss: 2.7222\n",
      "Epoch 6, Batch 121/157, Loss: 2.7938\n",
      "Epoch 6, Batch 122/157, Loss: 2.7424\n",
      "Epoch 6, Batch 123/157, Loss: 2.6906\n",
      "Epoch 6, Batch 124/157, Loss: 2.7748\n",
      "Epoch 6, Batch 125/157, Loss: 2.7830\n",
      "Epoch 6, Batch 126/157, Loss: 2.6450\n",
      "Epoch 6, Batch 127/157, Loss: 2.8150\n",
      "Epoch 6, Batch 128/157, Loss: 2.8289\n",
      "Epoch 6, Batch 129/157, Loss: 2.7566\n",
      "Epoch 6, Batch 130/157, Loss: 2.6829\n",
      "Epoch 6, Batch 131/157, Loss: 2.6724\n",
      "Epoch 6, Batch 132/157, Loss: 2.7374\n",
      "Epoch 6, Batch 133/157, Loss: 2.7789\n",
      "Epoch 6, Batch 134/157, Loss: 2.7870\n",
      "Epoch 6, Batch 135/157, Loss: 2.6773\n",
      "Epoch 6, Batch 136/157, Loss: 2.6889\n",
      "Epoch 6, Batch 137/157, Loss: 2.7326\n",
      "Epoch 6, Batch 138/157, Loss: 2.7304\n",
      "Epoch 6, Batch 139/157, Loss: 2.7344\n",
      "Epoch 6, Batch 140/157, Loss: 2.8201\n",
      "Epoch 6, Batch 141/157, Loss: 2.6544\n",
      "Epoch 6, Batch 142/157, Loss: 2.8886\n",
      "Epoch 6, Batch 143/157, Loss: 2.7411\n",
      "Epoch 6, Batch 144/157, Loss: 2.7372\n",
      "Epoch 6, Batch 145/157, Loss: 2.6459\n",
      "Epoch 6, Batch 146/157, Loss: 2.6744\n",
      "Epoch 6, Batch 147/157, Loss: 2.7132\n",
      "Epoch 6, Batch 148/157, Loss: 2.7158\n",
      "Epoch 6, Batch 149/157, Loss: 2.7733\n",
      "Epoch 6, Batch 150/157, Loss: 2.6962\n",
      "Epoch 6, Batch 151/157, Loss: 2.6672\n",
      "Epoch 6, Batch 152/157, Loss: 2.8928\n",
      "Epoch 6, Batch 153/157, Loss: 2.7124\n",
      "Epoch 6, Batch 154/157, Loss: 2.7896\n",
      "Epoch 6, Batch 155/157, Loss: 2.8041\n",
      "Epoch 6, Batch 156/157, Loss: 2.7904\n",
      "Epoch 6, Batch 157/157, Loss: 2.8002\n",
      "Epoch 6/50, Average Loss: 2.7421\n",
      "Epoch 7, Batch 1/157, Loss: 2.7100\n",
      "Epoch 7, Batch 2/157, Loss: 2.6925\n",
      "Epoch 7, Batch 3/157, Loss: 2.7184\n",
      "Epoch 7, Batch 4/157, Loss: 2.7890\n",
      "Epoch 7, Batch 5/157, Loss: 2.7593\n",
      "Epoch 7, Batch 6/157, Loss: 2.7379\n",
      "Epoch 7, Batch 7/157, Loss: 2.7902\n",
      "Epoch 7, Batch 8/157, Loss: 2.7721\n",
      "Epoch 7, Batch 9/157, Loss: 2.7458\n",
      "Epoch 7, Batch 10/157, Loss: 2.7822\n",
      "Epoch 7, Batch 11/157, Loss: 2.7274\n",
      "Epoch 7, Batch 12/157, Loss: 2.7142\n",
      "Epoch 7, Batch 13/157, Loss: 2.6860\n",
      "Epoch 7, Batch 14/157, Loss: 2.7826\n",
      "Epoch 7, Batch 15/157, Loss: 2.7398\n",
      "Epoch 7, Batch 16/157, Loss: 2.7476\n",
      "Epoch 7, Batch 17/157, Loss: 2.7325\n",
      "Epoch 7, Batch 18/157, Loss: 2.7283\n",
      "Epoch 7, Batch 19/157, Loss: 2.6998\n",
      "Epoch 7, Batch 20/157, Loss: 2.7796\n",
      "Epoch 7, Batch 21/157, Loss: 2.7436\n",
      "Epoch 7, Batch 22/157, Loss: 2.7382\n",
      "Epoch 7, Batch 23/157, Loss: 2.6896\n",
      "Epoch 7, Batch 24/157, Loss: 2.7403\n",
      "Epoch 7, Batch 25/157, Loss: 2.7241\n",
      "Epoch 7, Batch 26/157, Loss: 2.6257\n",
      "Epoch 7, Batch 27/157, Loss: 2.7987\n",
      "Epoch 7, Batch 28/157, Loss: 2.6936\n",
      "Epoch 7, Batch 29/157, Loss: 2.7549\n",
      "Epoch 7, Batch 30/157, Loss: 2.6868\n",
      "Epoch 7, Batch 31/157, Loss: 2.8095\n",
      "Epoch 7, Batch 32/157, Loss: 2.7713\n",
      "Epoch 7, Batch 33/157, Loss: 2.7615\n",
      "Epoch 7, Batch 34/157, Loss: 2.7465\n",
      "Epoch 7, Batch 35/157, Loss: 2.7413\n",
      "Epoch 7, Batch 36/157, Loss: 2.7089\n",
      "Epoch 7, Batch 37/157, Loss: 2.7667\n",
      "Epoch 7, Batch 38/157, Loss: 2.7867\n",
      "Epoch 7, Batch 39/157, Loss: 2.6872\n",
      "Epoch 7, Batch 40/157, Loss: 2.7071\n",
      "Epoch 7, Batch 41/157, Loss: 2.6775\n",
      "Epoch 7, Batch 42/157, Loss: 2.6854\n",
      "Epoch 7, Batch 43/157, Loss: 2.7323\n",
      "Epoch 7, Batch 44/157, Loss: 2.8197\n",
      "Epoch 7, Batch 45/157, Loss: 2.7380\n",
      "Epoch 7, Batch 46/157, Loss: 2.7285\n",
      "Epoch 7, Batch 47/157, Loss: 2.7961\n",
      "Epoch 7, Batch 48/157, Loss: 2.7685\n",
      "Epoch 7, Batch 49/157, Loss: 2.6809\n",
      "Epoch 7, Batch 50/157, Loss: 2.6135\n",
      "Epoch 7, Batch 51/157, Loss: 2.7498\n",
      "Epoch 7, Batch 52/157, Loss: 2.6835\n",
      "Epoch 7, Batch 53/157, Loss: 2.8020\n",
      "Epoch 7, Batch 54/157, Loss: 2.7619\n",
      "Epoch 7, Batch 55/157, Loss: 2.7180\n",
      "Epoch 7, Batch 56/157, Loss: 2.7443\n",
      "Epoch 7, Batch 57/157, Loss: 2.6819\n",
      "Epoch 7, Batch 58/157, Loss: 2.7592\n",
      "Epoch 7, Batch 59/157, Loss: 2.7470\n",
      "Epoch 7, Batch 60/157, Loss: 2.7622\n",
      "Epoch 7, Batch 61/157, Loss: 2.7236\n",
      "Epoch 7, Batch 62/157, Loss: 2.6844\n",
      "Epoch 7, Batch 63/157, Loss: 2.7226\n",
      "Epoch 7, Batch 64/157, Loss: 2.7845\n",
      "Epoch 7, Batch 65/157, Loss: 2.6865\n",
      "Epoch 7, Batch 66/157, Loss: 2.7104\n",
      "Epoch 7, Batch 67/157, Loss: 2.6967\n",
      "Epoch 7, Batch 68/157, Loss: 2.7407\n",
      "Epoch 7, Batch 69/157, Loss: 2.8013\n",
      "Epoch 7, Batch 70/157, Loss: 2.6603\n",
      "Epoch 7, Batch 71/157, Loss: 2.6776\n",
      "Epoch 7, Batch 72/157, Loss: 2.7360\n",
      "Epoch 7, Batch 73/157, Loss: 2.7482\n",
      "Epoch 7, Batch 74/157, Loss: 2.7042\n",
      "Epoch 7, Batch 75/157, Loss: 2.8066\n",
      "Epoch 7, Batch 76/157, Loss: 2.7494\n",
      "Epoch 7, Batch 77/157, Loss: 2.7425\n",
      "Epoch 7, Batch 78/157, Loss: 2.7049\n",
      "Epoch 7, Batch 79/157, Loss: 2.7236\n",
      "Epoch 7, Batch 80/157, Loss: 2.6763\n",
      "Epoch 7, Batch 81/157, Loss: 2.7376\n",
      "Epoch 7, Batch 82/157, Loss: 2.7386\n",
      "Epoch 7, Batch 83/157, Loss: 2.7034\n",
      "Epoch 7, Batch 84/157, Loss: 2.6507\n",
      "Epoch 7, Batch 85/157, Loss: 2.6614\n",
      "Epoch 7, Batch 86/157, Loss: 2.7135\n",
      "Epoch 7, Batch 87/157, Loss: 2.7768\n",
      "Epoch 7, Batch 88/157, Loss: 2.7159\n",
      "Epoch 7, Batch 89/157, Loss: 2.7616\n",
      "Epoch 7, Batch 90/157, Loss: 2.7064\n",
      "Epoch 7, Batch 91/157, Loss: 2.7868\n",
      "Epoch 7, Batch 92/157, Loss: 2.9201\n",
      "Epoch 7, Batch 93/157, Loss: 2.7233\n",
      "Epoch 7, Batch 94/157, Loss: 2.6814\n",
      "Epoch 7, Batch 95/157, Loss: 2.6944\n",
      "Epoch 7, Batch 96/157, Loss: 2.7587\n",
      "Epoch 7, Batch 97/157, Loss: 2.6904\n",
      "Epoch 7, Batch 98/157, Loss: 2.7155\n",
      "Epoch 7, Batch 99/157, Loss: 2.7775\n",
      "Epoch 7, Batch 100/157, Loss: 2.7556\n",
      "Epoch 7, Batch 101/157, Loss: 2.7256\n",
      "Epoch 7, Batch 102/157, Loss: 2.6758\n",
      "Epoch 7, Batch 103/157, Loss: 2.7049\n",
      "Epoch 7, Batch 104/157, Loss: 2.7146\n",
      "Epoch 7, Batch 105/157, Loss: 2.7263\n",
      "Epoch 7, Batch 106/157, Loss: 2.7590\n",
      "Epoch 7, Batch 107/157, Loss: 2.7143\n",
      "Epoch 7, Batch 108/157, Loss: 2.7570\n",
      "Epoch 7, Batch 109/157, Loss: 2.7971\n",
      "Epoch 7, Batch 110/157, Loss: 2.7169\n",
      "Epoch 7, Batch 111/157, Loss: 2.7120\n",
      "Epoch 7, Batch 112/157, Loss: 2.6615\n",
      "Epoch 7, Batch 113/157, Loss: 2.7910\n",
      "Epoch 7, Batch 114/157, Loss: 2.7436\n",
      "Epoch 7, Batch 115/157, Loss: 2.8195\n",
      "Epoch 7, Batch 116/157, Loss: 2.6593\n",
      "Epoch 7, Batch 117/157, Loss: 2.7153\n",
      "Epoch 7, Batch 118/157, Loss: 2.7143\n",
      "Epoch 7, Batch 119/157, Loss: 2.7364\n",
      "Epoch 7, Batch 120/157, Loss: 2.7338\n",
      "Epoch 7, Batch 121/157, Loss: 2.6804\n",
      "Epoch 7, Batch 122/157, Loss: 2.7567\n",
      "Epoch 7, Batch 123/157, Loss: 2.6932\n",
      "Epoch 7, Batch 124/157, Loss: 2.7271\n",
      "Epoch 7, Batch 125/157, Loss: 2.7179\n",
      "Epoch 7, Batch 126/157, Loss: 2.7106\n",
      "Epoch 7, Batch 127/157, Loss: 2.7480\n",
      "Epoch 7, Batch 128/157, Loss: 2.7698\n",
      "Epoch 7, Batch 129/157, Loss: 2.6959\n",
      "Epoch 7, Batch 130/157, Loss: 2.7085\n",
      "Epoch 7, Batch 131/157, Loss: 2.6792\n",
      "Epoch 7, Batch 132/157, Loss: 2.6518\n",
      "Epoch 7, Batch 133/157, Loss: 2.7408\n",
      "Epoch 7, Batch 134/157, Loss: 2.7164\n",
      "Epoch 7, Batch 135/157, Loss: 2.7926\n",
      "Epoch 7, Batch 136/157, Loss: 2.5966\n",
      "Epoch 7, Batch 137/157, Loss: 2.7086\n",
      "Epoch 7, Batch 138/157, Loss: 2.7810\n",
      "Epoch 7, Batch 139/157, Loss: 2.7135\n",
      "Epoch 7, Batch 140/157, Loss: 2.8148\n",
      "Epoch 7, Batch 141/157, Loss: 2.8016\n",
      "Epoch 7, Batch 142/157, Loss: 2.7463\n",
      "Epoch 7, Batch 143/157, Loss: 2.7638\n",
      "Epoch 7, Batch 144/157, Loss: 2.7790\n",
      "Epoch 7, Batch 145/157, Loss: 2.7631\n",
      "Epoch 7, Batch 146/157, Loss: 2.7074\n",
      "Epoch 7, Batch 147/157, Loss: 2.7953\n",
      "Epoch 7, Batch 148/157, Loss: 2.6872\n",
      "Epoch 7, Batch 149/157, Loss: 2.6903\n",
      "Epoch 7, Batch 150/157, Loss: 2.6992\n",
      "Epoch 7, Batch 151/157, Loss: 2.7323\n",
      "Epoch 7, Batch 152/157, Loss: 2.8564\n",
      "Epoch 7, Batch 153/157, Loss: 2.7842\n",
      "Epoch 7, Batch 154/157, Loss: 2.7761\n",
      "Epoch 7, Batch 155/157, Loss: 2.8300\n",
      "Epoch 7, Batch 156/157, Loss: 2.7567\n",
      "Epoch 7, Batch 157/157, Loss: 2.7566\n",
      "Epoch 7/50, Average Loss: 2.7339\n",
      "Epoch 8, Batch 1/157, Loss: 2.7212\n",
      "Epoch 8, Batch 2/157, Loss: 2.7902\n",
      "Epoch 8, Batch 3/157, Loss: 2.6548\n",
      "Epoch 8, Batch 4/157, Loss: 2.7699\n",
      "Epoch 8, Batch 5/157, Loss: 2.7721\n",
      "Epoch 8, Batch 6/157, Loss: 2.7936\n",
      "Epoch 8, Batch 7/157, Loss: 2.7050\n",
      "Epoch 8, Batch 8/157, Loss: 2.7596\n",
      "Epoch 8, Batch 9/157, Loss: 2.8376\n",
      "Epoch 8, Batch 10/157, Loss: 2.7068\n",
      "Epoch 8, Batch 11/157, Loss: 2.6998\n",
      "Epoch 8, Batch 12/157, Loss: 2.7549\n",
      "Epoch 8, Batch 13/157, Loss: 2.8171\n",
      "Epoch 8, Batch 14/157, Loss: 2.6889\n",
      "Epoch 8, Batch 15/157, Loss: 2.8374\n",
      "Epoch 8, Batch 16/157, Loss: 2.7925\n",
      "Epoch 8, Batch 17/157, Loss: 2.7443\n",
      "Epoch 8, Batch 18/157, Loss: 2.7703\n",
      "Epoch 8, Batch 19/157, Loss: 2.7489\n",
      "Epoch 8, Batch 20/157, Loss: 2.7554\n",
      "Epoch 8, Batch 21/157, Loss: 2.7271\n",
      "Epoch 8, Batch 22/157, Loss: 2.7326\n",
      "Epoch 8, Batch 23/157, Loss: 2.6857\n",
      "Epoch 8, Batch 24/157, Loss: 2.7391\n",
      "Epoch 8, Batch 25/157, Loss: 2.7600\n",
      "Epoch 8, Batch 26/157, Loss: 2.7024\n",
      "Epoch 8, Batch 27/157, Loss: 2.6747\n",
      "Epoch 8, Batch 28/157, Loss: 2.7584\n",
      "Epoch 8, Batch 29/157, Loss: 2.6993\n",
      "Epoch 8, Batch 30/157, Loss: 2.7182\n",
      "Epoch 8, Batch 31/157, Loss: 2.7852\n",
      "Epoch 8, Batch 32/157, Loss: 2.7116\n",
      "Epoch 8, Batch 33/157, Loss: 2.7087\n",
      "Epoch 8, Batch 34/157, Loss: 2.6895\n",
      "Epoch 8, Batch 35/157, Loss: 2.7392\n",
      "Epoch 8, Batch 36/157, Loss: 2.7470\n",
      "Epoch 8, Batch 37/157, Loss: 2.7788\n",
      "Epoch 8, Batch 38/157, Loss: 2.7757\n",
      "Epoch 8, Batch 39/157, Loss: 2.6984\n",
      "Epoch 8, Batch 40/157, Loss: 2.7795\n",
      "Epoch 8, Batch 41/157, Loss: 2.7174\n",
      "Epoch 8, Batch 42/157, Loss: 2.7194\n",
      "Epoch 8, Batch 43/157, Loss: 2.7516\n",
      "Epoch 8, Batch 44/157, Loss: 2.7165\n",
      "Epoch 8, Batch 45/157, Loss: 2.6715\n",
      "Epoch 8, Batch 46/157, Loss: 2.7229\n",
      "Epoch 8, Batch 47/157, Loss: 2.7687\n",
      "Epoch 8, Batch 48/157, Loss: 2.7910\n",
      "Epoch 8, Batch 49/157, Loss: 2.7789\n",
      "Epoch 8, Batch 50/157, Loss: 2.7349\n",
      "Epoch 8, Batch 51/157, Loss: 2.7825\n",
      "Epoch 8, Batch 52/157, Loss: 2.6759\n",
      "Epoch 8, Batch 53/157, Loss: 2.6917\n",
      "Epoch 8, Batch 54/157, Loss: 2.7230\n",
      "Epoch 8, Batch 55/157, Loss: 2.7170\n",
      "Epoch 8, Batch 56/157, Loss: 2.7439\n",
      "Epoch 8, Batch 57/157, Loss: 2.7248\n",
      "Epoch 8, Batch 58/157, Loss: 2.7266\n",
      "Epoch 8, Batch 59/157, Loss: 2.7415\n",
      "Epoch 8, Batch 60/157, Loss: 2.7581\n",
      "Epoch 8, Batch 61/157, Loss: 2.7426\n",
      "Epoch 8, Batch 62/157, Loss: 2.7219\n",
      "Epoch 8, Batch 63/157, Loss: 2.7645\n",
      "Epoch 8, Batch 64/157, Loss: 2.6760\n",
      "Epoch 8, Batch 65/157, Loss: 2.6997\n",
      "Epoch 8, Batch 66/157, Loss: 2.7271\n",
      "Epoch 8, Batch 67/157, Loss: 2.7052\n",
      "Epoch 8, Batch 68/157, Loss: 2.7521\n",
      "Epoch 8, Batch 69/157, Loss: 2.7932\n",
      "Epoch 8, Batch 70/157, Loss: 2.7285\n",
      "Epoch 8, Batch 71/157, Loss: 2.8205\n",
      "Epoch 8, Batch 72/157, Loss: 2.7443\n",
      "Epoch 8, Batch 73/157, Loss: 2.7095\n",
      "Epoch 8, Batch 74/157, Loss: 2.7564\n",
      "Epoch 8, Batch 75/157, Loss: 2.8099\n",
      "Epoch 8, Batch 76/157, Loss: 2.6902\n",
      "Epoch 8, Batch 77/157, Loss: 2.7648\n",
      "Epoch 8, Batch 78/157, Loss: 2.7063\n",
      "Epoch 8, Batch 79/157, Loss: 2.6973\n",
      "Epoch 8, Batch 80/157, Loss: 2.8288\n",
      "Epoch 8, Batch 81/157, Loss: 2.7774\n",
      "Epoch 8, Batch 82/157, Loss: 2.6642\n",
      "Epoch 8, Batch 83/157, Loss: 2.7441\n",
      "Epoch 8, Batch 84/157, Loss: 2.8177\n",
      "Epoch 8, Batch 85/157, Loss: 2.7747\n",
      "Epoch 8, Batch 86/157, Loss: 2.6964\n",
      "Epoch 8, Batch 87/157, Loss: 2.7391\n",
      "Epoch 8, Batch 88/157, Loss: 2.7429\n",
      "Epoch 8, Batch 89/157, Loss: 2.7093\n",
      "Epoch 8, Batch 90/157, Loss: 2.8109\n",
      "Epoch 8, Batch 91/157, Loss: 2.6390\n",
      "Epoch 8, Batch 92/157, Loss: 2.7870\n",
      "Epoch 8, Batch 93/157, Loss: 2.7274\n",
      "Epoch 8, Batch 94/157, Loss: 2.7963\n",
      "Epoch 8, Batch 95/157, Loss: 2.7275\n",
      "Epoch 8, Batch 96/157, Loss: 2.7350\n",
      "Epoch 8, Batch 97/157, Loss: 2.6958\n",
      "Epoch 8, Batch 98/157, Loss: 2.7372\n",
      "Epoch 8, Batch 99/157, Loss: 2.7036\n",
      "Epoch 8, Batch 100/157, Loss: 2.7227\n",
      "Epoch 8, Batch 101/157, Loss: 2.6906\n",
      "Epoch 8, Batch 102/157, Loss: 2.7607\n",
      "Epoch 8, Batch 103/157, Loss: 2.7844\n",
      "Epoch 8, Batch 104/157, Loss: 2.7335\n",
      "Epoch 8, Batch 105/157, Loss: 2.7059\n",
      "Epoch 8, Batch 106/157, Loss: 2.7303\n",
      "Epoch 8, Batch 107/157, Loss: 2.7132\n",
      "Epoch 8, Batch 108/157, Loss: 2.6640\n",
      "Epoch 8, Batch 109/157, Loss: 2.7713\n",
      "Epoch 8, Batch 110/157, Loss: 2.7200\n",
      "Epoch 8, Batch 111/157, Loss: 2.7719\n",
      "Epoch 8, Batch 112/157, Loss: 2.7415\n",
      "Epoch 8, Batch 113/157, Loss: 2.8007\n",
      "Epoch 8, Batch 114/157, Loss: 2.7591\n",
      "Epoch 8, Batch 115/157, Loss: 2.7324\n",
      "Epoch 8, Batch 116/157, Loss: 2.7409\n",
      "Epoch 8, Batch 117/157, Loss: 2.7739\n",
      "Epoch 8, Batch 118/157, Loss: 2.7095\n",
      "Epoch 8, Batch 119/157, Loss: 2.7467\n",
      "Epoch 8, Batch 120/157, Loss: 2.7116\n",
      "Epoch 8, Batch 121/157, Loss: 2.6748\n",
      "Epoch 8, Batch 122/157, Loss: 2.7203\n",
      "Epoch 8, Batch 123/157, Loss: 2.7427\n",
      "Epoch 8, Batch 124/157, Loss: 2.6747\n",
      "Epoch 8, Batch 125/157, Loss: 2.7332\n",
      "Epoch 8, Batch 126/157, Loss: 2.7104\n",
      "Epoch 8, Batch 127/157, Loss: 2.8230\n",
      "Epoch 8, Batch 128/157, Loss: 2.7791\n",
      "Epoch 8, Batch 129/157, Loss: 2.7160\n",
      "Epoch 8, Batch 130/157, Loss: 2.7173\n",
      "Epoch 8, Batch 131/157, Loss: 2.7487\n",
      "Epoch 8, Batch 132/157, Loss: 2.8061\n",
      "Epoch 8, Batch 133/157, Loss: 2.6566\n",
      "Epoch 8, Batch 134/157, Loss: 2.7762\n",
      "Epoch 8, Batch 135/157, Loss: 2.6729\n",
      "Epoch 8, Batch 136/157, Loss: 2.7372\n",
      "Epoch 8, Batch 137/157, Loss: 2.7278\n",
      "Epoch 8, Batch 138/157, Loss: 2.7380\n",
      "Epoch 8, Batch 139/157, Loss: 2.7853\n",
      "Epoch 8, Batch 140/157, Loss: 2.7214\n",
      "Epoch 8, Batch 141/157, Loss: 2.7617\n",
      "Epoch 8, Batch 142/157, Loss: 2.7159\n",
      "Epoch 8, Batch 143/157, Loss: 2.7549\n",
      "Epoch 8, Batch 144/157, Loss: 2.8179\n",
      "Epoch 8, Batch 145/157, Loss: 2.7187\n",
      "Epoch 8, Batch 146/157, Loss: 2.7638\n",
      "Epoch 8, Batch 147/157, Loss: 2.7498\n",
      "Epoch 8, Batch 148/157, Loss: 2.6991\n",
      "Epoch 8, Batch 149/157, Loss: 2.7690\n",
      "Epoch 8, Batch 150/157, Loss: 2.7164\n",
      "Epoch 8, Batch 151/157, Loss: 2.8503\n",
      "Epoch 8, Batch 152/157, Loss: 2.6863\n",
      "Epoch 8, Batch 153/157, Loss: 2.7564\n",
      "Epoch 8, Batch 154/157, Loss: 2.7681\n",
      "Epoch 8, Batch 155/157, Loss: 2.7393\n",
      "Epoch 8, Batch 156/157, Loss: 2.8012\n",
      "Epoch 8, Batch 157/157, Loss: 2.7760\n",
      "Epoch 8/50, Average Loss: 2.7404\n",
      "Epoch 9, Batch 1/157, Loss: 2.7669\n",
      "Epoch 9, Batch 2/157, Loss: 2.6591\n",
      "Epoch 9, Batch 3/157, Loss: 2.7884\n",
      "Epoch 9, Batch 4/157, Loss: 2.7157\n",
      "Epoch 9, Batch 5/157, Loss: 2.8254\n",
      "Epoch 9, Batch 6/157, Loss: 2.8105\n",
      "Epoch 9, Batch 7/157, Loss: 2.7345\n",
      "Epoch 9, Batch 8/157, Loss: 2.7201\n",
      "Epoch 9, Batch 9/157, Loss: 2.7305\n",
      "Epoch 9, Batch 10/157, Loss: 2.7583\n",
      "Epoch 9, Batch 11/157, Loss: 2.6928\n",
      "Epoch 9, Batch 12/157, Loss: 2.7399\n",
      "Epoch 9, Batch 13/157, Loss: 2.7519\n",
      "Epoch 9, Batch 14/157, Loss: 2.6879\n",
      "Epoch 9, Batch 15/157, Loss: 2.7746\n",
      "Epoch 9, Batch 16/157, Loss: 2.8847\n",
      "Epoch 9, Batch 17/157, Loss: 2.8405\n",
      "Epoch 9, Batch 18/157, Loss: 2.7308\n",
      "Epoch 9, Batch 19/157, Loss: 2.7577\n",
      "Epoch 9, Batch 20/157, Loss: 2.8607\n",
      "Epoch 9, Batch 21/157, Loss: 2.7817\n",
      "Epoch 9, Batch 22/157, Loss: 2.7192\n",
      "Epoch 9, Batch 23/157, Loss: 2.7044\n",
      "Epoch 9, Batch 24/157, Loss: 2.7656\n",
      "Epoch 9, Batch 25/157, Loss: 2.6870\n",
      "Epoch 9, Batch 26/157, Loss: 2.7662\n",
      "Epoch 9, Batch 27/157, Loss: 2.6937\n",
      "Epoch 9, Batch 28/157, Loss: 2.8186\n",
      "Epoch 9, Batch 29/157, Loss: 2.7143\n",
      "Epoch 9, Batch 30/157, Loss: 2.7808\n",
      "Epoch 9, Batch 31/157, Loss: 2.7594\n",
      "Epoch 9, Batch 32/157, Loss: 2.7712\n",
      "Epoch 9, Batch 33/157, Loss: 2.7011\n",
      "Epoch 9, Batch 34/157, Loss: 2.7423\n",
      "Epoch 9, Batch 35/157, Loss: 2.7267\n",
      "Epoch 9, Batch 36/157, Loss: 2.6154\n",
      "Epoch 9, Batch 37/157, Loss: 2.7407\n",
      "Epoch 9, Batch 38/157, Loss: 2.7285\n",
      "Epoch 9, Batch 39/157, Loss: 2.7368\n",
      "Epoch 9, Batch 40/157, Loss: 2.6979\n",
      "Epoch 9, Batch 41/157, Loss: 2.6390\n",
      "Epoch 9, Batch 42/157, Loss: 2.8637\n",
      "Epoch 9, Batch 43/157, Loss: 2.7151\n",
      "Epoch 9, Batch 44/157, Loss: 2.7370\n",
      "Epoch 9, Batch 45/157, Loss: 2.7470\n",
      "Epoch 9, Batch 46/157, Loss: 2.7886\n",
      "Epoch 9, Batch 47/157, Loss: 2.6995\n",
      "Epoch 9, Batch 48/157, Loss: 2.6765\n",
      "Epoch 9, Batch 49/157, Loss: 2.7619\n",
      "Epoch 9, Batch 50/157, Loss: 2.6683\n",
      "Epoch 9, Batch 51/157, Loss: 2.7180\n",
      "Epoch 9, Batch 52/157, Loss: 2.7376\n",
      "Epoch 9, Batch 53/157, Loss: 2.7923\n",
      "Epoch 9, Batch 54/157, Loss: 2.7196\n",
      "Epoch 9, Batch 55/157, Loss: 2.7586\n",
      "Epoch 9, Batch 56/157, Loss: 2.7325\n",
      "Epoch 9, Batch 57/157, Loss: 2.7672\n",
      "Epoch 9, Batch 58/157, Loss: 2.6921\n",
      "Epoch 9, Batch 59/157, Loss: 2.7343\n",
      "Epoch 9, Batch 60/157, Loss: 2.6614\n",
      "Epoch 9, Batch 61/157, Loss: 2.7636\n",
      "Epoch 9, Batch 62/157, Loss: 2.7558\n",
      "Epoch 9, Batch 63/157, Loss: 2.7175\n",
      "Epoch 9, Batch 64/157, Loss: 2.7067\n",
      "Epoch 9, Batch 65/157, Loss: 2.7353\n",
      "Epoch 9, Batch 66/157, Loss: 2.6848\n",
      "Epoch 9, Batch 67/157, Loss: 2.7825\n",
      "Epoch 9, Batch 68/157, Loss: 2.7916\n",
      "Epoch 9, Batch 69/157, Loss: 2.6918\n",
      "Epoch 9, Batch 70/157, Loss: 2.8532\n",
      "Epoch 9, Batch 71/157, Loss: 2.7626\n",
      "Epoch 9, Batch 72/157, Loss: 2.7269\n",
      "Epoch 9, Batch 73/157, Loss: 2.7932\n",
      "Epoch 9, Batch 74/157, Loss: 2.7589\n",
      "Epoch 9, Batch 75/157, Loss: 2.7433\n",
      "Epoch 9, Batch 76/157, Loss: 2.7474\n",
      "Epoch 9, Batch 77/157, Loss: 2.7315\n",
      "Epoch 9, Batch 78/157, Loss: 2.7750\n",
      "Epoch 9, Batch 79/157, Loss: 2.6844\n",
      "Epoch 9, Batch 80/157, Loss: 2.7277\n",
      "Epoch 9, Batch 81/157, Loss: 2.7552\n",
      "Epoch 9, Batch 82/157, Loss: 2.7604\n",
      "Epoch 9, Batch 83/157, Loss: 2.7408\n",
      "Epoch 9, Batch 84/157, Loss: 2.7071\n",
      "Epoch 9, Batch 85/157, Loss: 2.7724\n",
      "Epoch 9, Batch 86/157, Loss: 2.7504\n",
      "Epoch 9, Batch 87/157, Loss: 2.7557\n",
      "Epoch 9, Batch 88/157, Loss: 2.8673\n",
      "Epoch 9, Batch 89/157, Loss: 2.7243\n",
      "Epoch 9, Batch 90/157, Loss: 2.6658\n",
      "Epoch 9, Batch 91/157, Loss: 2.7377\n",
      "Epoch 9, Batch 92/157, Loss: 2.7031\n",
      "Epoch 9, Batch 93/157, Loss: 2.7196\n",
      "Epoch 9, Batch 94/157, Loss: 2.7302\n",
      "Epoch 9, Batch 95/157, Loss: 2.7790\n",
      "Epoch 9, Batch 96/157, Loss: 2.7807\n",
      "Epoch 9, Batch 97/157, Loss: 2.8281\n",
      "Epoch 9, Batch 98/157, Loss: 2.6570\n",
      "Epoch 9, Batch 99/157, Loss: 2.7136\n",
      "Epoch 9, Batch 100/157, Loss: 2.7597\n",
      "Epoch 9, Batch 101/157, Loss: 2.7356\n",
      "Epoch 9, Batch 102/157, Loss: 2.7093\n",
      "Epoch 9, Batch 103/157, Loss: 2.7508\n",
      "Epoch 9, Batch 104/157, Loss: 2.6828\n",
      "Epoch 9, Batch 105/157, Loss: 2.6759\n",
      "Epoch 9, Batch 106/157, Loss: 2.6245\n",
      "Epoch 9, Batch 107/157, Loss: 2.6977\n",
      "Epoch 9, Batch 108/157, Loss: 2.8059\n",
      "Epoch 9, Batch 109/157, Loss: 2.7492\n",
      "Epoch 9, Batch 110/157, Loss: 2.7231\n",
      "Epoch 9, Batch 111/157, Loss: 2.8013\n",
      "Epoch 9, Batch 112/157, Loss: 2.7020\n",
      "Epoch 9, Batch 113/157, Loss: 2.6950\n",
      "Epoch 9, Batch 114/157, Loss: 2.8073\n",
      "Epoch 9, Batch 115/157, Loss: 2.8031\n",
      "Epoch 9, Batch 116/157, Loss: 2.7074\n",
      "Epoch 9, Batch 117/157, Loss: 2.7573\n",
      "Epoch 9, Batch 118/157, Loss: 2.7140\n",
      "Epoch 9, Batch 119/157, Loss: 2.6897\n",
      "Epoch 9, Batch 120/157, Loss: 2.7046\n",
      "Epoch 9, Batch 121/157, Loss: 2.6707\n",
      "Epoch 9, Batch 122/157, Loss: 2.7367\n",
      "Epoch 9, Batch 123/157, Loss: 2.6685\n",
      "Epoch 9, Batch 124/157, Loss: 2.7615\n",
      "Epoch 9, Batch 125/157, Loss: 2.7519\n",
      "Epoch 9, Batch 126/157, Loss: 2.7410\n",
      "Epoch 9, Batch 127/157, Loss: 2.8124\n",
      "Epoch 9, Batch 128/157, Loss: 2.6309\n",
      "Epoch 9, Batch 129/157, Loss: 2.8144\n",
      "Epoch 9, Batch 130/157, Loss: 2.7362\n",
      "Epoch 9, Batch 131/157, Loss: 2.7995\n",
      "Epoch 9, Batch 132/157, Loss: 2.7090\n",
      "Epoch 9, Batch 133/157, Loss: 2.6808\n",
      "Epoch 9, Batch 134/157, Loss: 2.8119\n",
      "Epoch 9, Batch 135/157, Loss: 2.7933\n",
      "Epoch 9, Batch 136/157, Loss: 2.7728\n",
      "Epoch 9, Batch 137/157, Loss: 2.7409\n",
      "Epoch 9, Batch 138/157, Loss: 2.7518\n",
      "Epoch 9, Batch 139/157, Loss: 2.7965\n",
      "Epoch 9, Batch 140/157, Loss: 2.7044\n",
      "Epoch 9, Batch 141/157, Loss: 2.7362\n",
      "Epoch 9, Batch 142/157, Loss: 2.7581\n",
      "Epoch 9, Batch 143/157, Loss: 2.7109\n",
      "Epoch 9, Batch 144/157, Loss: 2.6888\n",
      "Epoch 9, Batch 145/157, Loss: 2.6880\n",
      "Epoch 9, Batch 146/157, Loss: 2.7382\n",
      "Epoch 9, Batch 147/157, Loss: 2.7364\n",
      "Epoch 9, Batch 148/157, Loss: 2.7845\n",
      "Epoch 9, Batch 149/157, Loss: 2.7717\n",
      "Epoch 9, Batch 150/157, Loss: 2.7141\n",
      "Epoch 9, Batch 151/157, Loss: 2.7546\n",
      "Epoch 9, Batch 152/157, Loss: 2.8027\n",
      "Epoch 9, Batch 153/157, Loss: 2.6934\n",
      "Epoch 9, Batch 154/157, Loss: 2.7546\n",
      "Epoch 9, Batch 155/157, Loss: 2.7545\n",
      "Epoch 9, Batch 156/157, Loss: 2.7606\n",
      "Epoch 9, Batch 157/157, Loss: 2.7380\n",
      "Epoch 9/50, Average Loss: 2.7410\n",
      "Epoch 10, Batch 1/157, Loss: 2.7154\n",
      "Epoch 10, Batch 2/157, Loss: 2.7392\n",
      "Epoch 10, Batch 3/157, Loss: 2.7057\n",
      "Epoch 10, Batch 4/157, Loss: 2.7271\n",
      "Epoch 10, Batch 5/157, Loss: 2.7635\n",
      "Epoch 10, Batch 6/157, Loss: 2.7264\n",
      "Epoch 10, Batch 7/157, Loss: 2.8026\n",
      "Epoch 10, Batch 8/157, Loss: 2.7775\n",
      "Epoch 10, Batch 9/157, Loss: 2.8085\n",
      "Epoch 10, Batch 10/157, Loss: 2.6757\n",
      "Epoch 10, Batch 11/157, Loss: 2.7103\n",
      "Epoch 10, Batch 12/157, Loss: 2.7597\n",
      "Epoch 10, Batch 13/157, Loss: 2.7824\n",
      "Epoch 10, Batch 14/157, Loss: 2.7315\n",
      "Epoch 10, Batch 15/157, Loss: 2.7561\n",
      "Epoch 10, Batch 16/157, Loss: 2.6816\n",
      "Epoch 10, Batch 17/157, Loss: 2.7347\n",
      "Epoch 10, Batch 18/157, Loss: 2.7148\n",
      "Epoch 10, Batch 19/157, Loss: 2.7201\n",
      "Epoch 10, Batch 20/157, Loss: 2.7946\n",
      "Epoch 10, Batch 21/157, Loss: 2.7909\n",
      "Epoch 10, Batch 22/157, Loss: 2.7127\n",
      "Epoch 10, Batch 23/157, Loss: 2.7727\n",
      "Epoch 10, Batch 24/157, Loss: 2.7023\n",
      "Epoch 10, Batch 25/157, Loss: 2.7329\n",
      "Epoch 10, Batch 26/157, Loss: 2.7947\n",
      "Epoch 10, Batch 27/157, Loss: 2.7371\n",
      "Epoch 10, Batch 28/157, Loss: 2.7636\n",
      "Epoch 10, Batch 29/157, Loss: 2.7039\n",
      "Epoch 10, Batch 30/157, Loss: 2.7435\n",
      "Epoch 10, Batch 31/157, Loss: 2.7183\n",
      "Epoch 10, Batch 32/157, Loss: 2.7605\n",
      "Epoch 10, Batch 33/157, Loss: 2.7279\n",
      "Epoch 10, Batch 34/157, Loss: 2.7308\n",
      "Epoch 10, Batch 35/157, Loss: 2.7221\n",
      "Epoch 10, Batch 36/157, Loss: 2.7418\n",
      "Epoch 10, Batch 37/157, Loss: 2.7385\n",
      "Epoch 10, Batch 38/157, Loss: 2.7867\n",
      "Epoch 10, Batch 39/157, Loss: 2.7838\n",
      "Epoch 10, Batch 40/157, Loss: 2.7533\n",
      "Epoch 10, Batch 41/157, Loss: 2.7436\n",
      "Epoch 10, Batch 42/157, Loss: 2.7154\n",
      "Epoch 10, Batch 43/157, Loss: 2.7330\n",
      "Epoch 10, Batch 44/157, Loss: 2.6859\n",
      "Epoch 10, Batch 45/157, Loss: 2.7267\n",
      "Epoch 10, Batch 46/157, Loss: 2.6941\n",
      "Epoch 10, Batch 47/157, Loss: 2.8016\n",
      "Epoch 10, Batch 48/157, Loss: 2.7013\n",
      "Epoch 10, Batch 49/157, Loss: 2.6548\n",
      "Epoch 10, Batch 50/157, Loss: 2.7284\n",
      "Epoch 10, Batch 51/157, Loss: 2.7125\n",
      "Epoch 10, Batch 52/157, Loss: 2.7730\n",
      "Epoch 10, Batch 53/157, Loss: 2.7982\n",
      "Epoch 10, Batch 54/157, Loss: 2.8259\n",
      "Epoch 10, Batch 55/157, Loss: 2.7516\n",
      "Epoch 10, Batch 56/157, Loss: 2.7132\n",
      "Epoch 10, Batch 57/157, Loss: 2.8276\n",
      "Epoch 10, Batch 58/157, Loss: 2.6733\n",
      "Epoch 10, Batch 59/157, Loss: 2.7882\n",
      "Epoch 10, Batch 60/157, Loss: 2.7172\n",
      "Epoch 10, Batch 61/157, Loss: 2.6983\n",
      "Epoch 10, Batch 62/157, Loss: 2.7672\n",
      "Epoch 10, Batch 63/157, Loss: 2.7586\n",
      "Epoch 10, Batch 64/157, Loss: 2.7447\n",
      "Epoch 10, Batch 65/157, Loss: 2.7549\n",
      "Epoch 10, Batch 66/157, Loss: 2.7318\n",
      "Epoch 10, Batch 67/157, Loss: 2.7772\n",
      "Epoch 10, Batch 68/157, Loss: 2.6835\n",
      "Epoch 10, Batch 69/157, Loss: 2.7515\n",
      "Epoch 10, Batch 70/157, Loss: 2.7986\n",
      "Epoch 10, Batch 71/157, Loss: 2.7552\n",
      "Epoch 10, Batch 72/157, Loss: 2.6782\n",
      "Epoch 10, Batch 73/157, Loss: 2.7364\n",
      "Epoch 10, Batch 74/157, Loss: 2.6946\n",
      "Epoch 10, Batch 75/157, Loss: 2.7818\n",
      "Epoch 10, Batch 76/157, Loss: 2.7649\n",
      "Epoch 10, Batch 77/157, Loss: 2.7901\n",
      "Epoch 10, Batch 78/157, Loss: 2.7321\n",
      "Epoch 10, Batch 79/157, Loss: 2.7154\n",
      "Epoch 10, Batch 80/157, Loss: 2.6982\n",
      "Epoch 10, Batch 81/157, Loss: 2.7478\n",
      "Epoch 10, Batch 82/157, Loss: 2.7401\n",
      "Epoch 10, Batch 83/157, Loss: 2.7110\n",
      "Epoch 10, Batch 84/157, Loss: 2.7591\n",
      "Epoch 10, Batch 85/157, Loss: 2.7283\n",
      "Epoch 10, Batch 86/157, Loss: 2.7810\n",
      "Epoch 10, Batch 87/157, Loss: 2.7421\n",
      "Epoch 10, Batch 88/157, Loss: 2.7641\n",
      "Epoch 10, Batch 89/157, Loss: 2.7166\n",
      "Epoch 10, Batch 90/157, Loss: 2.7104\n",
      "Epoch 10, Batch 91/157, Loss: 2.7467\n",
      "Epoch 10, Batch 92/157, Loss: 2.7755\n",
      "Epoch 10, Batch 93/157, Loss: 2.7360\n",
      "Epoch 10, Batch 94/157, Loss: 2.7448\n",
      "Epoch 10, Batch 95/157, Loss: 2.7770\n",
      "Epoch 10, Batch 96/157, Loss: 2.7591\n",
      "Epoch 10, Batch 97/157, Loss: 2.7290\n",
      "Epoch 10, Batch 98/157, Loss: 2.7909\n",
      "Epoch 10, Batch 99/157, Loss: 2.7193\n",
      "Epoch 10, Batch 100/157, Loss: 2.6558\n",
      "Epoch 10, Batch 101/157, Loss: 2.7358\n",
      "Epoch 10, Batch 102/157, Loss: 2.7730\n",
      "Epoch 10, Batch 103/157, Loss: 2.6775\n",
      "Epoch 10, Batch 104/157, Loss: 2.7573\n",
      "Epoch 10, Batch 105/157, Loss: 2.7555\n",
      "Epoch 10, Batch 106/157, Loss: 2.7926\n",
      "Epoch 10, Batch 107/157, Loss: 2.7194\n",
      "Epoch 10, Batch 108/157, Loss: 2.6941\n",
      "Epoch 10, Batch 109/157, Loss: 2.7461\n",
      "Epoch 10, Batch 110/157, Loss: 2.6946\n",
      "Epoch 10, Batch 111/157, Loss: 2.7086\n",
      "Epoch 10, Batch 112/157, Loss: 2.6995\n",
      "Epoch 10, Batch 113/157, Loss: 2.7263\n",
      "Epoch 10, Batch 114/157, Loss: 2.7778\n",
      "Epoch 10, Batch 115/157, Loss: 2.7420\n",
      "Epoch 10, Batch 116/157, Loss: 2.7377\n",
      "Epoch 10, Batch 117/157, Loss: 2.6726\n",
      "Epoch 10, Batch 118/157, Loss: 2.6986\n",
      "Epoch 10, Batch 119/157, Loss: 2.7286\n",
      "Epoch 10, Batch 120/157, Loss: 2.6621\n",
      "Epoch 10, Batch 121/157, Loss: 2.8246\n",
      "Epoch 10, Batch 122/157, Loss: 2.7452\n",
      "Epoch 10, Batch 123/157, Loss: 2.7230\n",
      "Epoch 10, Batch 124/157, Loss: 2.7649\n",
      "Epoch 10, Batch 125/157, Loss: 2.7629\n",
      "Epoch 10, Batch 126/157, Loss: 2.7571\n",
      "Epoch 10, Batch 127/157, Loss: 2.6967\n",
      "Epoch 10, Batch 128/157, Loss: 2.6893\n",
      "Epoch 10, Batch 129/157, Loss: 2.7502\n",
      "Epoch 10, Batch 130/157, Loss: 2.7381\n",
      "Epoch 10, Batch 131/157, Loss: 2.7327\n",
      "Epoch 10, Batch 132/157, Loss: 2.7364\n",
      "Epoch 10, Batch 133/157, Loss: 2.7187\n",
      "Epoch 10, Batch 134/157, Loss: 2.7320\n",
      "Epoch 10, Batch 135/157, Loss: 2.7125\n",
      "Epoch 10, Batch 136/157, Loss: 2.7279\n",
      "Epoch 10, Batch 137/157, Loss: 2.7355\n",
      "Epoch 10, Batch 138/157, Loss: 2.7438\n",
      "Epoch 10, Batch 139/157, Loss: 2.7025\n",
      "Epoch 10, Batch 140/157, Loss: 2.7576\n",
      "Epoch 10, Batch 141/157, Loss: 2.7187\n",
      "Epoch 10, Batch 142/157, Loss: 2.6674\n",
      "Epoch 10, Batch 143/157, Loss: 2.7141\n",
      "Epoch 10, Batch 144/157, Loss: 2.6957\n",
      "Epoch 10, Batch 145/157, Loss: 2.6945\n",
      "Epoch 10, Batch 146/157, Loss: 2.7386\n",
      "Epoch 10, Batch 147/157, Loss: 2.7550\n",
      "Epoch 10, Batch 148/157, Loss: 2.7153\n",
      "Epoch 10, Batch 149/157, Loss: 2.8001\n",
      "Epoch 10, Batch 150/157, Loss: 2.7574\n",
      "Epoch 10, Batch 151/157, Loss: 2.6519\n",
      "Epoch 10, Batch 152/157, Loss: 2.7289\n",
      "Epoch 10, Batch 153/157, Loss: 2.8376\n",
      "Epoch 10, Batch 154/157, Loss: 2.6133\n",
      "Epoch 10, Batch 155/157, Loss: 2.7815\n",
      "Epoch 10, Batch 156/157, Loss: 2.6450\n",
      "Epoch 10, Batch 157/157, Loss: 2.7745\n",
      "Epoch 10/50, Average Loss: 2.7369\n",
      "Epoch 11, Batch 1/157, Loss: 2.8435\n",
      "Epoch 11, Batch 2/157, Loss: 2.7081\n",
      "Epoch 11, Batch 3/157, Loss: 2.7135\n",
      "Epoch 11, Batch 4/157, Loss: 2.7006\n",
      "Epoch 11, Batch 5/157, Loss: 2.7992\n",
      "Epoch 11, Batch 6/157, Loss: 2.7477\n",
      "Epoch 11, Batch 7/157, Loss: 2.8292\n",
      "Epoch 11, Batch 8/157, Loss: 2.6630\n",
      "Epoch 11, Batch 9/157, Loss: 2.7240\n",
      "Epoch 11, Batch 10/157, Loss: 2.7245\n",
      "Epoch 11, Batch 11/157, Loss: 2.7725\n",
      "Epoch 11, Batch 12/157, Loss: 2.7938\n",
      "Epoch 11, Batch 13/157, Loss: 2.7550\n",
      "Epoch 11, Batch 14/157, Loss: 2.7241\n",
      "Epoch 11, Batch 15/157, Loss: 2.7645\n",
      "Epoch 11, Batch 16/157, Loss: 2.7433\n",
      "Epoch 11, Batch 17/157, Loss: 2.7072\n",
      "Epoch 11, Batch 18/157, Loss: 2.6760\n",
      "Epoch 11, Batch 19/157, Loss: 2.7074\n",
      "Epoch 11, Batch 20/157, Loss: 2.7328\n",
      "Epoch 11, Batch 21/157, Loss: 2.7408\n",
      "Epoch 11, Batch 22/157, Loss: 2.7501\n",
      "Epoch 11, Batch 23/157, Loss: 2.7449\n",
      "Epoch 11, Batch 24/157, Loss: 2.7180\n",
      "Epoch 11, Batch 25/157, Loss: 2.7490\n",
      "Epoch 11, Batch 26/157, Loss: 2.7939\n",
      "Epoch 11, Batch 27/157, Loss: 2.7735\n",
      "Epoch 11, Batch 28/157, Loss: 2.7516\n",
      "Epoch 11, Batch 29/157, Loss: 2.7989\n",
      "Epoch 11, Batch 30/157, Loss: 2.7138\n",
      "Epoch 11, Batch 31/157, Loss: 2.7456\n",
      "Epoch 11, Batch 32/157, Loss: 2.7610\n",
      "Epoch 11, Batch 33/157, Loss: 2.6916\n",
      "Epoch 11, Batch 34/157, Loss: 2.6919\n",
      "Epoch 11, Batch 35/157, Loss: 2.6936\n",
      "Epoch 11, Batch 36/157, Loss: 2.7885\n",
      "Epoch 11, Batch 37/157, Loss: 2.7397\n",
      "Epoch 11, Batch 38/157, Loss: 2.6871\n",
      "Epoch 11, Batch 39/157, Loss: 2.6878\n",
      "Epoch 11, Batch 40/157, Loss: 2.6922\n",
      "Epoch 11, Batch 41/157, Loss: 2.7407\n",
      "Epoch 11, Batch 42/157, Loss: 2.7433\n",
      "Epoch 11, Batch 43/157, Loss: 2.7299\n",
      "Epoch 11, Batch 44/157, Loss: 2.6894\n",
      "Epoch 11, Batch 45/157, Loss: 2.6939\n",
      "Epoch 11, Batch 46/157, Loss: 2.7215\n",
      "Epoch 11, Batch 47/157, Loss: 2.7560\n",
      "Epoch 11, Batch 48/157, Loss: 2.7411\n",
      "Epoch 11, Batch 49/157, Loss: 2.7116\n",
      "Epoch 11, Batch 50/157, Loss: 2.7638\n",
      "Epoch 11, Batch 51/157, Loss: 2.6874\n",
      "Epoch 11, Batch 52/157, Loss: 2.7621\n",
      "Epoch 11, Batch 53/157, Loss: 2.7811\n",
      "Epoch 11, Batch 54/157, Loss: 2.7604\n",
      "Epoch 11, Batch 55/157, Loss: 2.7007\n",
      "Epoch 11, Batch 56/157, Loss: 2.8409\n",
      "Epoch 11, Batch 57/157, Loss: 2.7518\n",
      "Epoch 11, Batch 58/157, Loss: 2.7900\n",
      "Epoch 11, Batch 59/157, Loss: 2.7991\n",
      "Epoch 11, Batch 60/157, Loss: 2.7616\n",
      "Epoch 11, Batch 61/157, Loss: 2.6929\n",
      "Epoch 11, Batch 62/157, Loss: 2.7142\n",
      "Epoch 11, Batch 63/157, Loss: 2.7652\n",
      "Epoch 11, Batch 64/157, Loss: 2.7447\n",
      "Epoch 11, Batch 65/157, Loss: 2.7408\n",
      "Epoch 11, Batch 66/157, Loss: 2.7023\n",
      "Epoch 11, Batch 67/157, Loss: 2.7175\n",
      "Epoch 11, Batch 68/157, Loss: 2.7502\n",
      "Epoch 11, Batch 69/157, Loss: 2.6917\n",
      "Epoch 11, Batch 70/157, Loss: 2.7212\n",
      "Epoch 11, Batch 71/157, Loss: 2.7020\n",
      "Epoch 11, Batch 72/157, Loss: 2.6982\n",
      "Epoch 11, Batch 73/157, Loss: 2.7376\n",
      "Epoch 11, Batch 74/157, Loss: 2.7056\n",
      "Epoch 11, Batch 75/157, Loss: 2.6942\n",
      "Epoch 11, Batch 76/157, Loss: 2.6578\n",
      "Epoch 11, Batch 77/157, Loss: 2.7179\n",
      "Epoch 11, Batch 78/157, Loss: 2.7448\n",
      "Epoch 11, Batch 79/157, Loss: 2.7398\n",
      "Epoch 11, Batch 80/157, Loss: 2.7509\n",
      "Epoch 11, Batch 81/157, Loss: 2.8091\n",
      "Epoch 11, Batch 82/157, Loss: 2.7624\n",
      "Epoch 11, Batch 83/157, Loss: 2.7313\n",
      "Epoch 11, Batch 84/157, Loss: 2.7813\n",
      "Epoch 11, Batch 85/157, Loss: 2.6697\n",
      "Epoch 11, Batch 86/157, Loss: 2.7746\n",
      "Epoch 11, Batch 87/157, Loss: 2.7544\n",
      "Epoch 11, Batch 88/157, Loss: 2.7356\n",
      "Epoch 11, Batch 89/157, Loss: 2.6610\n",
      "Epoch 11, Batch 90/157, Loss: 2.7348\n",
      "Epoch 11, Batch 91/157, Loss: 2.7386\n",
      "Epoch 11, Batch 92/157, Loss: 2.7116\n",
      "Epoch 11, Batch 93/157, Loss: 2.7827\n",
      "Epoch 11, Batch 94/157, Loss: 2.6266\n",
      "Epoch 11, Batch 95/157, Loss: 2.8138\n",
      "Epoch 11, Batch 96/157, Loss: 2.7810\n",
      "Epoch 11, Batch 97/157, Loss: 2.6894\n",
      "Epoch 11, Batch 98/157, Loss: 2.6289\n",
      "Epoch 11, Batch 99/157, Loss: 2.7600\n",
      "Epoch 11, Batch 100/157, Loss: 2.7626\n",
      "Epoch 11, Batch 101/157, Loss: 2.7368\n",
      "Epoch 11, Batch 102/157, Loss: 2.7273\n",
      "Epoch 11, Batch 103/157, Loss: 2.6861\n",
      "Epoch 11, Batch 104/157, Loss: 2.6920\n",
      "Epoch 11, Batch 105/157, Loss: 2.7125\n",
      "Epoch 11, Batch 106/157, Loss: 2.6813\n",
      "Epoch 11, Batch 107/157, Loss: 2.7814\n",
      "Epoch 11, Batch 108/157, Loss: 2.6824\n",
      "Epoch 11, Batch 109/157, Loss: 2.7567\n",
      "Epoch 11, Batch 110/157, Loss: 2.7206\n",
      "Epoch 11, Batch 111/157, Loss: 2.7785\n",
      "Epoch 11, Batch 112/157, Loss: 2.7162\n",
      "Epoch 11, Batch 113/157, Loss: 2.6653\n",
      "Epoch 11, Batch 114/157, Loss: 2.7168\n",
      "Epoch 11, Batch 115/157, Loss: 2.7828\n",
      "Epoch 11, Batch 116/157, Loss: 2.7121\n",
      "Epoch 11, Batch 117/157, Loss: 2.7948\n",
      "Epoch 11, Batch 118/157, Loss: 2.7933\n",
      "Epoch 11, Batch 119/157, Loss: 2.7623\n",
      "Epoch 11, Batch 120/157, Loss: 2.7884\n",
      "Epoch 11, Batch 121/157, Loss: 2.7045\n",
      "Epoch 11, Batch 122/157, Loss: 2.6823\n",
      "Epoch 11, Batch 123/157, Loss: 2.7631\n",
      "Epoch 11, Batch 124/157, Loss: 2.6800\n",
      "Epoch 11, Batch 125/157, Loss: 2.7420\n",
      "Epoch 11, Batch 126/157, Loss: 2.7093\n",
      "Epoch 11, Batch 127/157, Loss: 2.6936\n",
      "Epoch 11, Batch 128/157, Loss: 2.8038\n",
      "Epoch 11, Batch 129/157, Loss: 2.6804\n",
      "Epoch 11, Batch 130/157, Loss: 2.7860\n",
      "Epoch 11, Batch 131/157, Loss: 2.7678\n",
      "Epoch 11, Batch 132/157, Loss: 2.7277\n",
      "Epoch 11, Batch 133/157, Loss: 2.7293\n",
      "Epoch 11, Batch 134/157, Loss: 2.7155\n",
      "Epoch 11, Batch 135/157, Loss: 2.7315\n",
      "Epoch 11, Batch 136/157, Loss: 2.7712\n",
      "Epoch 11, Batch 137/157, Loss: 2.7637\n",
      "Epoch 11, Batch 138/157, Loss: 2.7780\n",
      "Epoch 11, Batch 139/157, Loss: 2.6924\n",
      "Epoch 11, Batch 140/157, Loss: 2.7394\n",
      "Epoch 11, Batch 141/157, Loss: 2.6967\n",
      "Epoch 11, Batch 142/157, Loss: 2.7246\n",
      "Epoch 11, Batch 143/157, Loss: 2.6695\n",
      "Epoch 11, Batch 144/157, Loss: 2.6930\n",
      "Epoch 11, Batch 145/157, Loss: 2.7084\n",
      "Epoch 11, Batch 146/157, Loss: 2.7563\n",
      "Epoch 11, Batch 147/157, Loss: 2.7325\n",
      "Epoch 11, Batch 148/157, Loss: 2.7094\n",
      "Epoch 11, Batch 149/157, Loss: 2.7555\n",
      "Epoch 11, Batch 150/157, Loss: 2.7279\n",
      "Epoch 11, Batch 151/157, Loss: 2.7597\n",
      "Epoch 11, Batch 152/157, Loss: 2.7370\n",
      "Epoch 11, Batch 153/157, Loss: 2.7609\n",
      "Epoch 11, Batch 154/157, Loss: 2.7502\n",
      "Epoch 11, Batch 155/157, Loss: 2.7922\n",
      "Epoch 11, Batch 156/157, Loss: 2.7574\n",
      "Epoch 11, Batch 157/157, Loss: 2.7024\n",
      "Epoch 11/50, Average Loss: 2.7347\n",
      "Epoch 12, Batch 1/157, Loss: 2.6848\n",
      "Epoch 12, Batch 2/157, Loss: 2.7132\n",
      "Epoch 12, Batch 3/157, Loss: 2.6967\n",
      "Epoch 12, Batch 4/157, Loss: 2.6693\n",
      "Epoch 12, Batch 5/157, Loss: 2.6999\n",
      "Epoch 12, Batch 6/157, Loss: 2.7874\n",
      "Epoch 12, Batch 7/157, Loss: 2.7224\n",
      "Epoch 12, Batch 8/157, Loss: 2.7269\n",
      "Epoch 12, Batch 9/157, Loss: 2.7922\n",
      "Epoch 12, Batch 10/157, Loss: 2.7266\n",
      "Epoch 12, Batch 11/157, Loss: 2.7595\n",
      "Epoch 12, Batch 12/157, Loss: 2.7007\n",
      "Epoch 12, Batch 13/157, Loss: 2.7756\n",
      "Epoch 12, Batch 14/157, Loss: 2.7545\n",
      "Epoch 12, Batch 15/157, Loss: 2.7836\n",
      "Epoch 12, Batch 16/157, Loss: 2.6838\n",
      "Epoch 12, Batch 17/157, Loss: 2.6714\n",
      "Epoch 12, Batch 18/157, Loss: 2.7709\n",
      "Epoch 12, Batch 19/157, Loss: 2.7465\n",
      "Epoch 12, Batch 20/157, Loss: 2.7511\n",
      "Epoch 12, Batch 21/157, Loss: 2.7207\n",
      "Epoch 12, Batch 22/157, Loss: 2.7053\n",
      "Epoch 12, Batch 23/157, Loss: 2.7462\n",
      "Epoch 12, Batch 24/157, Loss: 2.7752\n",
      "Epoch 12, Batch 25/157, Loss: 2.7694\n",
      "Epoch 12, Batch 26/157, Loss: 2.7260\n",
      "Epoch 12, Batch 27/157, Loss: 2.7104\n",
      "Epoch 12, Batch 28/157, Loss: 2.7780\n",
      "Epoch 12, Batch 29/157, Loss: 2.7772\n",
      "Epoch 12, Batch 30/157, Loss: 2.7950\n",
      "Epoch 12, Batch 31/157, Loss: 2.6640\n",
      "Epoch 12, Batch 32/157, Loss: 2.6912\n",
      "Epoch 12, Batch 33/157, Loss: 2.7364\n",
      "Epoch 12, Batch 34/157, Loss: 2.7672\n",
      "Epoch 12, Batch 35/157, Loss: 2.7233\n",
      "Epoch 12, Batch 36/157, Loss: 2.7418\n",
      "Epoch 12, Batch 37/157, Loss: 2.6964\n",
      "Epoch 12, Batch 38/157, Loss: 2.7509\n",
      "Epoch 12, Batch 39/157, Loss: 2.6864\n",
      "Epoch 12, Batch 40/157, Loss: 2.7294\n",
      "Epoch 12, Batch 41/157, Loss: 2.7360\n",
      "Epoch 12, Batch 42/157, Loss: 2.6697\n",
      "Epoch 12, Batch 43/157, Loss: 2.7651\n",
      "Epoch 12, Batch 44/157, Loss: 2.6750\n",
      "Epoch 12, Batch 45/157, Loss: 2.7867\n",
      "Epoch 12, Batch 46/157, Loss: 2.7575\n",
      "Epoch 12, Batch 47/157, Loss: 2.7580\n",
      "Epoch 12, Batch 48/157, Loss: 2.7169\n",
      "Epoch 12, Batch 49/157, Loss: 2.7845\n",
      "Epoch 12, Batch 50/157, Loss: 2.7633\n",
      "Epoch 12, Batch 51/157, Loss: 2.7225\n",
      "Epoch 12, Batch 52/157, Loss: 2.7040\n",
      "Epoch 12, Batch 53/157, Loss: 2.7016\n",
      "Epoch 12, Batch 54/157, Loss: 2.6981\n",
      "Epoch 12, Batch 55/157, Loss: 2.7852\n",
      "Epoch 12, Batch 56/157, Loss: 2.8080\n",
      "Epoch 12, Batch 57/157, Loss: 2.7290\n",
      "Epoch 12, Batch 58/157, Loss: 2.6451\n",
      "Epoch 12, Batch 59/157, Loss: 2.7106\n",
      "Epoch 12, Batch 60/157, Loss: 2.7377\n",
      "Epoch 12, Batch 61/157, Loss: 2.7270\n",
      "Epoch 12, Batch 62/157, Loss: 2.6833\n",
      "Epoch 12, Batch 63/157, Loss: 2.6561\n",
      "Epoch 12, Batch 64/157, Loss: 2.6895\n",
      "Epoch 12, Batch 65/157, Loss: 2.7668\n",
      "Epoch 12, Batch 66/157, Loss: 2.7267\n",
      "Epoch 12, Batch 67/157, Loss: 2.7471\n",
      "Epoch 12, Batch 68/157, Loss: 2.7754\n",
      "Epoch 12, Batch 69/157, Loss: 2.6935\n",
      "Epoch 12, Batch 70/157, Loss: 2.7289\n",
      "Epoch 12, Batch 71/157, Loss: 2.7591\n",
      "Epoch 12, Batch 72/157, Loss: 2.7422\n",
      "Epoch 12, Batch 73/157, Loss: 2.6886\n",
      "Epoch 12, Batch 74/157, Loss: 2.7517\n",
      "Epoch 12, Batch 75/157, Loss: 2.7475\n",
      "Epoch 12, Batch 76/157, Loss: 2.7348\n",
      "Epoch 12, Batch 77/157, Loss: 2.7437\n",
      "Epoch 12, Batch 78/157, Loss: 2.7649\n",
      "Epoch 12, Batch 79/157, Loss: 2.7363\n",
      "Epoch 12, Batch 80/157, Loss: 2.7276\n",
      "Epoch 12, Batch 81/157, Loss: 2.7432\n",
      "Epoch 12, Batch 82/157, Loss: 2.7204\n",
      "Epoch 12, Batch 83/157, Loss: 2.7251\n",
      "Epoch 12, Batch 84/157, Loss: 2.7752\n",
      "Epoch 12, Batch 85/157, Loss: 2.7300\n",
      "Epoch 12, Batch 86/157, Loss: 2.6920\n",
      "Epoch 12, Batch 87/157, Loss: 2.7005\n",
      "Epoch 12, Batch 88/157, Loss: 2.7519\n",
      "Epoch 12, Batch 89/157, Loss: 2.7504\n",
      "Epoch 12, Batch 90/157, Loss: 2.7578\n",
      "Epoch 12, Batch 91/157, Loss: 2.7717\n",
      "Epoch 12, Batch 92/157, Loss: 2.7411\n",
      "Epoch 12, Batch 93/157, Loss: 2.6949\n",
      "Epoch 12, Batch 94/157, Loss: 2.6701\n",
      "Epoch 12, Batch 95/157, Loss: 2.7032\n",
      "Epoch 12, Batch 96/157, Loss: 2.7621\n",
      "Epoch 12, Batch 97/157, Loss: 2.6512\n",
      "Epoch 12, Batch 98/157, Loss: 2.7269\n",
      "Epoch 12, Batch 99/157, Loss: 2.7324\n",
      "Epoch 12, Batch 100/157, Loss: 2.7387\n",
      "Epoch 12, Batch 101/157, Loss: 2.6504\n",
      "Epoch 12, Batch 102/157, Loss: 2.7737\n",
      "Epoch 12, Batch 103/157, Loss: 2.7130\n",
      "Epoch 12, Batch 104/157, Loss: 2.7761\n",
      "Epoch 12, Batch 105/157, Loss: 2.7863\n",
      "Epoch 12, Batch 106/157, Loss: 2.7025\n",
      "Epoch 12, Batch 107/157, Loss: 2.8431\n",
      "Epoch 12, Batch 108/157, Loss: 2.7065\n",
      "Epoch 12, Batch 109/157, Loss: 2.6558\n",
      "Epoch 12, Batch 110/157, Loss: 2.7089\n",
      "Epoch 12, Batch 111/157, Loss: 2.8133\n",
      "Epoch 12, Batch 112/157, Loss: 2.7694\n",
      "Epoch 12, Batch 113/157, Loss: 2.7056\n",
      "Epoch 12, Batch 114/157, Loss: 2.7245\n",
      "Epoch 12, Batch 115/157, Loss: 2.7919\n",
      "Epoch 12, Batch 116/157, Loss: 2.8126\n",
      "Epoch 12, Batch 117/157, Loss: 2.6825\n",
      "Epoch 12, Batch 118/157, Loss: 2.7010\n",
      "Epoch 12, Batch 119/157, Loss: 2.7369\n",
      "Epoch 12, Batch 120/157, Loss: 2.7379\n",
      "Epoch 12, Batch 121/157, Loss: 2.8116\n",
      "Epoch 12, Batch 122/157, Loss: 2.7048\n",
      "Epoch 12, Batch 123/157, Loss: 2.7780\n",
      "Epoch 12, Batch 124/157, Loss: 2.6980\n",
      "Epoch 12, Batch 125/157, Loss: 2.7830\n",
      "Epoch 12, Batch 126/157, Loss: 2.7163\n",
      "Epoch 12, Batch 127/157, Loss: 2.7193\n",
      "Epoch 12, Batch 128/157, Loss: 2.7349\n",
      "Epoch 12, Batch 129/157, Loss: 2.8128\n",
      "Epoch 12, Batch 130/157, Loss: 2.7575\n",
      "Epoch 12, Batch 131/157, Loss: 2.6598\n",
      "Epoch 12, Batch 132/157, Loss: 2.6630\n",
      "Epoch 12, Batch 133/157, Loss: 2.7604\n",
      "Epoch 12, Batch 134/157, Loss: 2.7020\n",
      "Epoch 12, Batch 135/157, Loss: 2.7695\n",
      "Epoch 12, Batch 136/157, Loss: 2.7813\n",
      "Epoch 12, Batch 137/157, Loss: 2.7685\n",
      "Epoch 12, Batch 138/157, Loss: 2.7848\n",
      "Epoch 12, Batch 139/157, Loss: 2.7521\n",
      "Epoch 12, Batch 140/157, Loss: 2.6866\n",
      "Epoch 12, Batch 141/157, Loss: 2.7089\n",
      "Epoch 12, Batch 142/157, Loss: 2.7244\n",
      "Epoch 12, Batch 143/157, Loss: 2.7724\n",
      "Epoch 12, Batch 144/157, Loss: 2.7241\n",
      "Epoch 12, Batch 145/157, Loss: 2.7659\n",
      "Epoch 12, Batch 146/157, Loss: 2.7427\n",
      "Epoch 12, Batch 147/157, Loss: 2.7123\n",
      "Epoch 12, Batch 148/157, Loss: 2.7125\n",
      "Epoch 12, Batch 149/157, Loss: 2.7224\n",
      "Epoch 12, Batch 150/157, Loss: 2.7344\n",
      "Epoch 12, Batch 151/157, Loss: 2.7319\n",
      "Epoch 12, Batch 152/157, Loss: 2.6671\n",
      "Epoch 12, Batch 153/157, Loss: 2.7324\n",
      "Epoch 12, Batch 154/157, Loss: 2.7551\n",
      "Epoch 12, Batch 155/157, Loss: 2.7440\n",
      "Epoch 12, Batch 156/157, Loss: 2.7794\n",
      "Epoch 12, Batch 157/157, Loss: 2.7405\n",
      "Epoch 12/50, Average Loss: 2.7337\n",
      "Epoch 13, Batch 1/157, Loss: 2.6969\n",
      "Epoch 13, Batch 2/157, Loss: 2.7055\n",
      "Epoch 13, Batch 3/157, Loss: 2.6951\n",
      "Epoch 13, Batch 4/157, Loss: 2.7297\n",
      "Epoch 13, Batch 5/157, Loss: 2.6795\n",
      "Epoch 13, Batch 6/157, Loss: 2.6906\n",
      "Epoch 13, Batch 7/157, Loss: 2.7281\n",
      "Epoch 13, Batch 8/157, Loss: 2.7326\n",
      "Epoch 13, Batch 9/157, Loss: 2.7617\n",
      "Epoch 13, Batch 10/157, Loss: 2.7741\n",
      "Epoch 13, Batch 11/157, Loss: 2.7080\n",
      "Epoch 13, Batch 12/157, Loss: 2.7166\n",
      "Epoch 13, Batch 13/157, Loss: 2.7867\n",
      "Epoch 13, Batch 14/157, Loss: 2.6554\n",
      "Epoch 13, Batch 15/157, Loss: 2.7804\n",
      "Epoch 13, Batch 16/157, Loss: 2.8687\n",
      "Epoch 13, Batch 17/157, Loss: 2.6697\n",
      "Epoch 13, Batch 18/157, Loss: 2.8394\n",
      "Epoch 13, Batch 19/157, Loss: 2.7097\n",
      "Epoch 13, Batch 20/157, Loss: 2.7935\n",
      "Epoch 13, Batch 21/157, Loss: 2.7028\n",
      "Epoch 13, Batch 22/157, Loss: 2.6864\n",
      "Epoch 13, Batch 23/157, Loss: 2.7232\n",
      "Epoch 13, Batch 24/157, Loss: 2.7857\n",
      "Epoch 13, Batch 25/157, Loss: 2.7579\n",
      "Epoch 13, Batch 26/157, Loss: 2.8175\n",
      "Epoch 13, Batch 27/157, Loss: 2.7625\n",
      "Epoch 13, Batch 28/157, Loss: 2.7762\n",
      "Epoch 13, Batch 29/157, Loss: 2.7368\n",
      "Epoch 13, Batch 30/157, Loss: 2.7699\n",
      "Epoch 13, Batch 31/157, Loss: 2.7297\n",
      "Epoch 13, Batch 32/157, Loss: 2.6655\n",
      "Epoch 13, Batch 33/157, Loss: 2.7142\n",
      "Epoch 13, Batch 34/157, Loss: 2.6950\n",
      "Epoch 13, Batch 35/157, Loss: 2.6643\n",
      "Epoch 13, Batch 36/157, Loss: 2.6789\n",
      "Epoch 13, Batch 37/157, Loss: 2.8372\n",
      "Epoch 13, Batch 38/157, Loss: 2.7279\n",
      "Epoch 13, Batch 39/157, Loss: 2.7372\n",
      "Epoch 13, Batch 40/157, Loss: 2.7691\n",
      "Epoch 13, Batch 41/157, Loss: 2.7132\n",
      "Epoch 13, Batch 42/157, Loss: 2.8056\n",
      "Epoch 13, Batch 43/157, Loss: 2.7951\n",
      "Epoch 13, Batch 44/157, Loss: 2.7631\n",
      "Epoch 13, Batch 45/157, Loss: 2.6956\n",
      "Epoch 13, Batch 46/157, Loss: 2.6092\n",
      "Epoch 13, Batch 47/157, Loss: 2.7505\n",
      "Epoch 13, Batch 48/157, Loss: 2.6894\n",
      "Epoch 13, Batch 49/157, Loss: 2.7579\n",
      "Epoch 13, Batch 50/157, Loss: 2.7787\n",
      "Epoch 13, Batch 51/157, Loss: 2.7675\n",
      "Epoch 13, Batch 52/157, Loss: 2.6771\n",
      "Epoch 13, Batch 53/157, Loss: 2.7295\n",
      "Epoch 13, Batch 54/157, Loss: 2.7128\n",
      "Epoch 13, Batch 55/157, Loss: 2.7465\n",
      "Epoch 13, Batch 56/157, Loss: 2.6960\n",
      "Epoch 13, Batch 57/157, Loss: 2.7379\n",
      "Epoch 13, Batch 58/157, Loss: 2.7729\n",
      "Epoch 13, Batch 59/157, Loss: 2.7795\n",
      "Epoch 13, Batch 60/157, Loss: 2.7446\n",
      "Epoch 13, Batch 61/157, Loss: 2.7034\n",
      "Epoch 13, Batch 62/157, Loss: 2.7230\n",
      "Epoch 13, Batch 63/157, Loss: 2.6887\n",
      "Epoch 13, Batch 64/157, Loss: 2.7753\n",
      "Epoch 13, Batch 65/157, Loss: 2.7403\n",
      "Epoch 13, Batch 66/157, Loss: 2.6805\n",
      "Epoch 13, Batch 67/157, Loss: 2.7503\n",
      "Epoch 13, Batch 68/157, Loss: 2.7814\n",
      "Epoch 13, Batch 69/157, Loss: 2.6630\n",
      "Epoch 13, Batch 70/157, Loss: 2.7448\n",
      "Epoch 13, Batch 71/157, Loss: 2.6509\n",
      "Epoch 13, Batch 72/157, Loss: 2.8022\n",
      "Epoch 13, Batch 73/157, Loss: 2.7407\n",
      "Epoch 13, Batch 74/157, Loss: 2.8171\n",
      "Epoch 13, Batch 75/157, Loss: 2.7164\n",
      "Epoch 13, Batch 76/157, Loss: 2.6677\n",
      "Epoch 13, Batch 77/157, Loss: 2.7911\n",
      "Epoch 13, Batch 78/157, Loss: 2.8379\n",
      "Epoch 13, Batch 79/157, Loss: 2.7390\n",
      "Epoch 13, Batch 80/157, Loss: 2.7364\n",
      "Epoch 13, Batch 81/157, Loss: 2.7295\n",
      "Epoch 13, Batch 82/157, Loss: 2.7608\n",
      "Epoch 13, Batch 83/157, Loss: 2.7228\n",
      "Epoch 13, Batch 84/157, Loss: 2.8249\n",
      "Epoch 13, Batch 85/157, Loss: 2.7349\n",
      "Epoch 13, Batch 86/157, Loss: 2.7547\n",
      "Epoch 13, Batch 87/157, Loss: 2.6908\n",
      "Epoch 13, Batch 88/157, Loss: 2.7155\n",
      "Epoch 13, Batch 89/157, Loss: 2.6564\n",
      "Epoch 13, Batch 90/157, Loss: 2.7961\n",
      "Epoch 13, Batch 91/157, Loss: 2.6640\n",
      "Epoch 13, Batch 92/157, Loss: 2.7792\n",
      "Epoch 13, Batch 93/157, Loss: 2.8385\n",
      "Epoch 13, Batch 94/157, Loss: 2.6598\n",
      "Epoch 13, Batch 95/157, Loss: 2.7538\n",
      "Epoch 13, Batch 96/157, Loss: 2.7838\n",
      "Epoch 13, Batch 97/157, Loss: 2.7589\n",
      "Epoch 13, Batch 98/157, Loss: 2.7121\n",
      "Epoch 13, Batch 99/157, Loss: 2.6717\n",
      "Epoch 13, Batch 100/157, Loss: 2.6962\n",
      "Epoch 13, Batch 101/157, Loss: 2.7420\n",
      "Epoch 13, Batch 102/157, Loss: 2.7343\n",
      "Epoch 13, Batch 103/157, Loss: 2.7404\n",
      "Epoch 13, Batch 104/157, Loss: 2.7303\n",
      "Epoch 13, Batch 105/157, Loss: 2.7756\n",
      "Epoch 13, Batch 106/157, Loss: 2.7409\n",
      "Epoch 13, Batch 107/157, Loss: 2.7303\n",
      "Epoch 13, Batch 108/157, Loss: 2.7682\n",
      "Epoch 13, Batch 109/157, Loss: 2.7365\n",
      "Epoch 13, Batch 110/157, Loss: 2.7957\n",
      "Epoch 13, Batch 111/157, Loss: 2.7553\n",
      "Epoch 13, Batch 112/157, Loss: 2.7769\n",
      "Epoch 13, Batch 113/157, Loss: 2.8726\n",
      "Epoch 13, Batch 114/157, Loss: 2.7488\n",
      "Epoch 13, Batch 115/157, Loss: 2.6974\n",
      "Epoch 13, Batch 116/157, Loss: 2.7173\n",
      "Epoch 13, Batch 117/157, Loss: 2.6735\n",
      "Epoch 13, Batch 118/157, Loss: 2.7139\n",
      "Epoch 13, Batch 119/157, Loss: 2.7670\n",
      "Epoch 13, Batch 120/157, Loss: 2.7252\n",
      "Epoch 13, Batch 121/157, Loss: 2.7158\n",
      "Epoch 13, Batch 122/157, Loss: 2.6811\n",
      "Epoch 13, Batch 123/157, Loss: 2.6862\n",
      "Epoch 13, Batch 124/157, Loss: 2.7250\n",
      "Epoch 13, Batch 125/157, Loss: 2.6472\n",
      "Epoch 13, Batch 126/157, Loss: 2.7294\n",
      "Epoch 13, Batch 127/157, Loss: 2.7814\n",
      "Epoch 13, Batch 128/157, Loss: 2.6866\n",
      "Epoch 13, Batch 129/157, Loss: 2.7358\n",
      "Epoch 13, Batch 130/157, Loss: 2.7106\n",
      "Epoch 13, Batch 131/157, Loss: 2.6953\n",
      "Epoch 13, Batch 132/157, Loss: 2.7403\n",
      "Epoch 13, Batch 133/157, Loss: 2.7110\n",
      "Epoch 13, Batch 134/157, Loss: 2.7286\n",
      "Epoch 13, Batch 135/157, Loss: 2.7695\n",
      "Epoch 13, Batch 136/157, Loss: 2.8113\n",
      "Epoch 13, Batch 137/157, Loss: 2.7499\n",
      "Epoch 13, Batch 138/157, Loss: 2.6928\n",
      "Epoch 13, Batch 139/157, Loss: 2.7309\n",
      "Epoch 13, Batch 140/157, Loss: 2.7248\n",
      "Epoch 13, Batch 141/157, Loss: 2.7244\n",
      "Epoch 13, Batch 142/157, Loss: 2.7079\n",
      "Epoch 13, Batch 143/157, Loss: 2.7304\n",
      "Epoch 13, Batch 144/157, Loss: 2.7053\n",
      "Epoch 13, Batch 145/157, Loss: 2.7445\n",
      "Epoch 13, Batch 146/157, Loss: 2.7383\n",
      "Epoch 13, Batch 147/157, Loss: 2.7204\n",
      "Epoch 13, Batch 148/157, Loss: 2.6726\n",
      "Epoch 13, Batch 149/157, Loss: 2.7035\n",
      "Epoch 13, Batch 150/157, Loss: 2.7753\n",
      "Epoch 13, Batch 151/157, Loss: 2.7584\n",
      "Epoch 13, Batch 152/157, Loss: 2.7877\n",
      "Epoch 13, Batch 153/157, Loss: 2.7691\n",
      "Epoch 13, Batch 154/157, Loss: 2.7074\n",
      "Epoch 13, Batch 155/157, Loss: 2.7792\n",
      "Epoch 13, Batch 156/157, Loss: 2.7666\n",
      "Epoch 13, Batch 157/157, Loss: 2.8443\n",
      "Epoch 13/50, Average Loss: 2.7367\n",
      "Epoch 14, Batch 1/157, Loss: 2.7314\n",
      "Epoch 14, Batch 2/157, Loss: 2.7449\n",
      "Epoch 14, Batch 3/157, Loss: 2.7540\n",
      "Epoch 14, Batch 4/157, Loss: 2.7022\n",
      "Epoch 14, Batch 5/157, Loss: 2.8173\n",
      "Epoch 14, Batch 6/157, Loss: 2.8112\n",
      "Epoch 14, Batch 7/157, Loss: 2.7454\n",
      "Epoch 14, Batch 8/157, Loss: 2.7311\n",
      "Epoch 14, Batch 9/157, Loss: 2.7076\n",
      "Epoch 14, Batch 10/157, Loss: 2.7516\n",
      "Epoch 14, Batch 11/157, Loss: 2.7606\n",
      "Epoch 14, Batch 12/157, Loss: 2.7244\n",
      "Epoch 14, Batch 13/157, Loss: 2.6975\n",
      "Epoch 14, Batch 14/157, Loss: 2.7185\n",
      "Epoch 14, Batch 15/157, Loss: 2.7141\n",
      "Epoch 14, Batch 16/157, Loss: 2.6851\n",
      "Epoch 14, Batch 17/157, Loss: 2.7536\n",
      "Epoch 14, Batch 18/157, Loss: 2.7790\n",
      "Epoch 14, Batch 19/157, Loss: 2.7297\n",
      "Epoch 14, Batch 20/157, Loss: 2.6653\n",
      "Epoch 14, Batch 21/157, Loss: 2.7594\n",
      "Epoch 14, Batch 22/157, Loss: 2.7397\n",
      "Epoch 14, Batch 23/157, Loss: 2.6717\n",
      "Epoch 14, Batch 24/157, Loss: 2.7180\n",
      "Epoch 14, Batch 25/157, Loss: 2.7039\n",
      "Epoch 14, Batch 26/157, Loss: 2.7562\n",
      "Epoch 14, Batch 27/157, Loss: 2.7493\n",
      "Epoch 14, Batch 28/157, Loss: 2.6772\n",
      "Epoch 14, Batch 29/157, Loss: 2.8394\n",
      "Epoch 14, Batch 30/157, Loss: 2.6317\n",
      "Epoch 14, Batch 31/157, Loss: 2.6877\n",
      "Epoch 14, Batch 32/157, Loss: 2.7389\n",
      "Epoch 14, Batch 33/157, Loss: 2.8705\n",
      "Epoch 14, Batch 34/157, Loss: 2.8024\n",
      "Epoch 14, Batch 35/157, Loss: 2.7276\n",
      "Epoch 14, Batch 36/157, Loss: 2.7402\n",
      "Epoch 14, Batch 37/157, Loss: 2.7754\n",
      "Epoch 14, Batch 38/157, Loss: 2.7702\n",
      "Epoch 14, Batch 39/157, Loss: 2.6664\n",
      "Epoch 14, Batch 40/157, Loss: 2.7531\n",
      "Epoch 14, Batch 41/157, Loss: 2.7122\n",
      "Epoch 14, Batch 42/157, Loss: 2.7665\n",
      "Epoch 14, Batch 43/157, Loss: 2.7830\n",
      "Epoch 14, Batch 44/157, Loss: 2.6755\n",
      "Epoch 14, Batch 45/157, Loss: 2.7257\n",
      "Epoch 14, Batch 46/157, Loss: 2.7434\n",
      "Epoch 14, Batch 47/157, Loss: 2.7964\n",
      "Epoch 14, Batch 48/157, Loss: 2.7114\n",
      "Epoch 14, Batch 49/157, Loss: 2.7872\n",
      "Epoch 14, Batch 50/157, Loss: 2.7672\n",
      "Epoch 14, Batch 51/157, Loss: 2.7969\n",
      "Epoch 14, Batch 52/157, Loss: 2.7427\n",
      "Epoch 14, Batch 53/157, Loss: 2.6939\n",
      "Epoch 14, Batch 54/157, Loss: 2.7167\n",
      "Epoch 14, Batch 55/157, Loss: 2.7030\n",
      "Epoch 14, Batch 56/157, Loss: 2.7361\n",
      "Epoch 14, Batch 57/157, Loss: 2.7524\n",
      "Epoch 14, Batch 58/157, Loss: 2.7486\n",
      "Epoch 14, Batch 59/157, Loss: 2.7342\n",
      "Epoch 14, Batch 60/157, Loss: 2.7045\n",
      "Epoch 14, Batch 61/157, Loss: 2.7380\n",
      "Epoch 14, Batch 62/157, Loss: 2.6872\n",
      "Epoch 14, Batch 63/157, Loss: 2.7475\n",
      "Epoch 14, Batch 64/157, Loss: 2.6929\n",
      "Epoch 14, Batch 65/157, Loss: 2.7315\n",
      "Epoch 14, Batch 66/157, Loss: 2.6978\n",
      "Epoch 14, Batch 67/157, Loss: 2.7859\n",
      "Epoch 14, Batch 68/157, Loss: 2.6872\n",
      "Epoch 14, Batch 69/157, Loss: 2.7894\n",
      "Epoch 14, Batch 70/157, Loss: 2.7586\n",
      "Epoch 14, Batch 71/157, Loss: 2.7542\n",
      "Epoch 14, Batch 72/157, Loss: 2.7287\n",
      "Epoch 14, Batch 73/157, Loss: 2.7450\n",
      "Epoch 14, Batch 74/157, Loss: 2.7411\n",
      "Epoch 14, Batch 75/157, Loss: 2.7365\n",
      "Epoch 14, Batch 76/157, Loss: 2.7230\n",
      "Epoch 14, Batch 77/157, Loss: 2.7732\n",
      "Epoch 14, Batch 78/157, Loss: 2.7403\n",
      "Epoch 14, Batch 79/157, Loss: 2.7537\n",
      "Epoch 14, Batch 80/157, Loss: 2.7616\n",
      "Epoch 14, Batch 81/157, Loss: 2.7765\n",
      "Epoch 14, Batch 82/157, Loss: 2.7643\n",
      "Epoch 14, Batch 83/157, Loss: 2.7563\n",
      "Epoch 14, Batch 84/157, Loss: 2.7086\n",
      "Epoch 14, Batch 85/157, Loss: 2.6912\n",
      "Epoch 14, Batch 86/157, Loss: 2.7739\n",
      "Epoch 14, Batch 87/157, Loss: 2.7038\n",
      "Epoch 14, Batch 88/157, Loss: 2.6423\n",
      "Epoch 14, Batch 89/157, Loss: 2.7808\n",
      "Epoch 14, Batch 90/157, Loss: 2.7256\n",
      "Epoch 14, Batch 91/157, Loss: 2.7319\n",
      "Epoch 14, Batch 92/157, Loss: 2.7070\n",
      "Epoch 14, Batch 93/157, Loss: 2.7610\n",
      "Epoch 14, Batch 94/157, Loss: 2.7875\n",
      "Epoch 14, Batch 95/157, Loss: 2.7155\n",
      "Epoch 14, Batch 96/157, Loss: 2.7745\n",
      "Epoch 14, Batch 97/157, Loss: 2.7677\n",
      "Epoch 14, Batch 98/157, Loss: 2.7564\n",
      "Epoch 14, Batch 99/157, Loss: 2.6943\n",
      "Epoch 14, Batch 100/157, Loss: 2.7604\n",
      "Epoch 14, Batch 101/157, Loss: 2.7032\n",
      "Epoch 14, Batch 102/157, Loss: 2.7047\n",
      "Epoch 14, Batch 103/157, Loss: 2.7534\n",
      "Epoch 14, Batch 104/157, Loss: 2.7045\n",
      "Epoch 14, Batch 105/157, Loss: 2.7720\n",
      "Epoch 14, Batch 106/157, Loss: 2.6941\n",
      "Epoch 14, Batch 107/157, Loss: 2.7964\n",
      "Epoch 14, Batch 108/157, Loss: 2.7108\n",
      "Epoch 14, Batch 109/157, Loss: 2.7004\n",
      "Epoch 14, Batch 110/157, Loss: 2.7448\n",
      "Epoch 14, Batch 111/157, Loss: 2.7043\n",
      "Epoch 14, Batch 112/157, Loss: 2.7281\n",
      "Epoch 14, Batch 113/157, Loss: 2.7430\n",
      "Epoch 14, Batch 114/157, Loss: 2.7154\n",
      "Epoch 14, Batch 115/157, Loss: 2.6813\n",
      "Epoch 14, Batch 116/157, Loss: 2.6421\n",
      "Epoch 14, Batch 117/157, Loss: 2.8535\n",
      "Epoch 14, Batch 118/157, Loss: 2.7348\n",
      "Epoch 14, Batch 119/157, Loss: 2.6800\n",
      "Epoch 14, Batch 120/157, Loss: 2.7283\n",
      "Epoch 14, Batch 121/157, Loss: 2.7491\n",
      "Epoch 14, Batch 122/157, Loss: 2.7214\n",
      "Epoch 14, Batch 123/157, Loss: 2.7643\n",
      "Epoch 14, Batch 124/157, Loss: 2.6492\n",
      "Epoch 14, Batch 125/157, Loss: 2.8351\n",
      "Epoch 14, Batch 126/157, Loss: 2.7598\n",
      "Epoch 14, Batch 127/157, Loss: 2.7329\n",
      "Epoch 14, Batch 128/157, Loss: 2.7009\n",
      "Epoch 14, Batch 129/157, Loss: 2.7413\n",
      "Epoch 14, Batch 130/157, Loss: 2.6837\n",
      "Epoch 14, Batch 131/157, Loss: 2.6968\n",
      "Epoch 14, Batch 132/157, Loss: 2.6846\n",
      "Epoch 14, Batch 133/157, Loss: 2.7258\n",
      "Epoch 14, Batch 134/157, Loss: 2.7700\n",
      "Epoch 14, Batch 135/157, Loss: 2.7122\n",
      "Epoch 14, Batch 136/157, Loss: 2.6964\n",
      "Epoch 14, Batch 137/157, Loss: 2.7277\n",
      "Epoch 14, Batch 138/157, Loss: 2.7072\n",
      "Epoch 14, Batch 139/157, Loss: 2.7303\n",
      "Epoch 14, Batch 140/157, Loss: 2.7456\n",
      "Epoch 14, Batch 141/157, Loss: 2.7491\n",
      "Epoch 14, Batch 142/157, Loss: 2.7513\n",
      "Epoch 14, Batch 143/157, Loss: 2.6478\n",
      "Epoch 14, Batch 144/157, Loss: 2.7530\n",
      "Epoch 14, Batch 145/157, Loss: 2.7132\n",
      "Epoch 14, Batch 146/157, Loss: 2.7465\n",
      "Epoch 14, Batch 147/157, Loss: 2.7322\n",
      "Epoch 14, Batch 148/157, Loss: 2.7210\n",
      "Epoch 14, Batch 149/157, Loss: 2.7835\n",
      "Epoch 14, Batch 150/157, Loss: 2.7020\n",
      "Epoch 14, Batch 151/157, Loss: 2.7532\n",
      "Epoch 14, Batch 152/157, Loss: 2.7475\n",
      "Epoch 14, Batch 153/157, Loss: 2.7148\n",
      "Epoch 14, Batch 154/157, Loss: 2.7267\n",
      "Epoch 14, Batch 155/157, Loss: 2.6692\n",
      "Epoch 14, Batch 156/157, Loss: 2.6850\n",
      "Epoch 14, Batch 157/157, Loss: 2.6694\n",
      "Epoch 14/50, Average Loss: 2.7334\n",
      "Epoch 15, Batch 1/157, Loss: 2.7240\n",
      "Epoch 15, Batch 2/157, Loss: 2.6754\n",
      "Epoch 15, Batch 3/157, Loss: 2.6273\n",
      "Epoch 15, Batch 4/157, Loss: 2.7164\n",
      "Epoch 15, Batch 5/157, Loss: 2.8453\n",
      "Epoch 15, Batch 6/157, Loss: 2.7782\n",
      "Epoch 15, Batch 7/157, Loss: 2.7205\n",
      "Epoch 15, Batch 8/157, Loss: 2.7141\n",
      "Epoch 15, Batch 9/157, Loss: 2.8128\n",
      "Epoch 15, Batch 10/157, Loss: 2.7449\n",
      "Epoch 15, Batch 11/157, Loss: 2.6897\n",
      "Epoch 15, Batch 12/157, Loss: 2.7800\n",
      "Epoch 15, Batch 13/157, Loss: 2.7595\n",
      "Epoch 15, Batch 14/157, Loss: 2.7454\n",
      "Epoch 15, Batch 15/157, Loss: 2.6628\n",
      "Epoch 15, Batch 16/157, Loss: 2.7725\n",
      "Epoch 15, Batch 17/157, Loss: 2.7814\n",
      "Epoch 15, Batch 18/157, Loss: 2.7108\n",
      "Epoch 15, Batch 19/157, Loss: 2.7446\n",
      "Epoch 15, Batch 20/157, Loss: 2.7678\n",
      "Epoch 15, Batch 21/157, Loss: 2.6916\n",
      "Epoch 15, Batch 22/157, Loss: 2.7362\n",
      "Epoch 15, Batch 23/157, Loss: 2.7219\n",
      "Epoch 15, Batch 24/157, Loss: 2.8033\n",
      "Epoch 15, Batch 25/157, Loss: 2.6805\n",
      "Epoch 15, Batch 26/157, Loss: 2.7478\n",
      "Epoch 15, Batch 27/157, Loss: 2.7124\n",
      "Epoch 15, Batch 28/157, Loss: 2.6690\n",
      "Epoch 15, Batch 29/157, Loss: 2.7342\n",
      "Epoch 15, Batch 30/157, Loss: 2.7027\n",
      "Epoch 15, Batch 31/157, Loss: 2.7088\n",
      "Epoch 15, Batch 32/157, Loss: 2.7866\n",
      "Epoch 15, Batch 33/157, Loss: 2.7288\n",
      "Epoch 15, Batch 34/157, Loss: 2.7149\n",
      "Epoch 15, Batch 35/157, Loss: 2.7741\n",
      "Epoch 15, Batch 36/157, Loss: 2.7212\n",
      "Epoch 15, Batch 37/157, Loss: 2.8297\n",
      "Epoch 15, Batch 38/157, Loss: 2.7739\n",
      "Epoch 15, Batch 39/157, Loss: 2.8063\n",
      "Epoch 15, Batch 40/157, Loss: 2.7502\n",
      "Epoch 15, Batch 41/157, Loss: 2.8296\n",
      "Epoch 15, Batch 42/157, Loss: 2.7380\n",
      "Epoch 15, Batch 43/157, Loss: 2.7711\n",
      "Epoch 15, Batch 44/157, Loss: 2.7413\n",
      "Epoch 15, Batch 45/157, Loss: 2.6831\n",
      "Epoch 15, Batch 46/157, Loss: 2.7420\n",
      "Epoch 15, Batch 47/157, Loss: 2.7225\n",
      "Epoch 15, Batch 48/157, Loss: 2.6941\n",
      "Epoch 15, Batch 49/157, Loss: 2.7314\n",
      "Epoch 15, Batch 50/157, Loss: 2.8032\n",
      "Epoch 15, Batch 51/157, Loss: 2.6627\n",
      "Epoch 15, Batch 52/157, Loss: 2.7548\n",
      "Epoch 15, Batch 53/157, Loss: 2.7801\n",
      "Epoch 15, Batch 54/157, Loss: 2.7772\n",
      "Epoch 15, Batch 55/157, Loss: 2.7567\n",
      "Epoch 15, Batch 56/157, Loss: 2.8245\n",
      "Epoch 15, Batch 57/157, Loss: 2.6857\n",
      "Epoch 15, Batch 58/157, Loss: 2.7810\n",
      "Epoch 15, Batch 59/157, Loss: 2.7140\n",
      "Epoch 15, Batch 60/157, Loss: 2.7704\n",
      "Epoch 15, Batch 61/157, Loss: 2.7367\n",
      "Epoch 15, Batch 62/157, Loss: 2.7407\n",
      "Epoch 15, Batch 63/157, Loss: 2.6655\n",
      "Epoch 15, Batch 64/157, Loss: 2.7097\n",
      "Epoch 15, Batch 65/157, Loss: 2.7077\n",
      "Epoch 15, Batch 66/157, Loss: 2.7081\n",
      "Epoch 15, Batch 67/157, Loss: 2.7102\n",
      "Epoch 15, Batch 68/157, Loss: 2.7453\n",
      "Epoch 15, Batch 69/157, Loss: 2.7195\n",
      "Epoch 15, Batch 70/157, Loss: 2.7622\n",
      "Epoch 15, Batch 71/157, Loss: 2.7237\n",
      "Epoch 15, Batch 72/157, Loss: 2.8169\n",
      "Epoch 15, Batch 73/157, Loss: 2.6774\n",
      "Epoch 15, Batch 74/157, Loss: 2.7783\n",
      "Epoch 15, Batch 75/157, Loss: 2.7367\n",
      "Epoch 15, Batch 76/157, Loss: 2.7407\n",
      "Epoch 15, Batch 77/157, Loss: 2.7218\n",
      "Epoch 15, Batch 78/157, Loss: 2.7509\n",
      "Epoch 15, Batch 79/157, Loss: 2.7130\n",
      "Epoch 15, Batch 80/157, Loss: 2.7420\n",
      "Epoch 15, Batch 81/157, Loss: 2.6919\n",
      "Epoch 15, Batch 82/157, Loss: 2.7275\n",
      "Epoch 15, Batch 83/157, Loss: 2.7604\n",
      "Epoch 15, Batch 84/157, Loss: 2.7517\n",
      "Epoch 15, Batch 85/157, Loss: 2.7379\n",
      "Epoch 15, Batch 86/157, Loss: 2.7466\n",
      "Epoch 15, Batch 87/157, Loss: 2.6916\n",
      "Epoch 15, Batch 88/157, Loss: 2.7079\n",
      "Epoch 15, Batch 89/157, Loss: 2.7418\n",
      "Epoch 15, Batch 90/157, Loss: 2.6846\n",
      "Epoch 15, Batch 91/157, Loss: 2.7088\n",
      "Epoch 15, Batch 92/157, Loss: 2.5923\n",
      "Epoch 15, Batch 93/157, Loss: 2.7561\n",
      "Epoch 15, Batch 94/157, Loss: 2.7576\n",
      "Epoch 15, Batch 95/157, Loss: 2.8344\n",
      "Epoch 15, Batch 96/157, Loss: 2.7234\n",
      "Epoch 15, Batch 97/157, Loss: 2.7833\n",
      "Epoch 15, Batch 98/157, Loss: 2.7627\n",
      "Epoch 15, Batch 99/157, Loss: 2.7563\n",
      "Epoch 15, Batch 100/157, Loss: 2.7786\n",
      "Epoch 15, Batch 101/157, Loss: 2.7226\n",
      "Epoch 15, Batch 102/157, Loss: 2.7757\n",
      "Epoch 15, Batch 103/157, Loss: 2.6818\n",
      "Epoch 15, Batch 104/157, Loss: 2.7462\n",
      "Epoch 15, Batch 105/157, Loss: 2.7034\n",
      "Epoch 15, Batch 106/157, Loss: 2.6585\n",
      "Epoch 15, Batch 107/157, Loss: 2.7033\n",
      "Epoch 15, Batch 108/157, Loss: 2.7510\n",
      "Epoch 15, Batch 109/157, Loss: 2.7465\n",
      "Epoch 15, Batch 110/157, Loss: 2.7239\n",
      "Epoch 15, Batch 111/157, Loss: 2.6610\n",
      "Epoch 15, Batch 112/157, Loss: 2.8212\n",
      "Epoch 15, Batch 113/157, Loss: 2.7640\n",
      "Epoch 15, Batch 114/157, Loss: 2.7474\n",
      "Epoch 15, Batch 115/157, Loss: 2.7582\n",
      "Epoch 15, Batch 116/157, Loss: 2.7608\n",
      "Epoch 15, Batch 117/157, Loss: 2.6848\n",
      "Epoch 15, Batch 118/157, Loss: 2.7395\n",
      "Epoch 15, Batch 119/157, Loss: 2.7359\n",
      "Epoch 15, Batch 120/157, Loss: 2.8003\n",
      "Epoch 15, Batch 121/157, Loss: 2.7404\n",
      "Epoch 15, Batch 122/157, Loss: 2.6959\n",
      "Epoch 15, Batch 123/157, Loss: 2.7612\n",
      "Epoch 15, Batch 124/157, Loss: 2.7670\n",
      "Epoch 15, Batch 125/157, Loss: 2.7014\n",
      "Epoch 15, Batch 126/157, Loss: 2.7156\n",
      "Epoch 15, Batch 127/157, Loss: 2.6686\n",
      "Epoch 15, Batch 128/157, Loss: 2.7443\n",
      "Epoch 15, Batch 129/157, Loss: 2.7123\n",
      "Epoch 15, Batch 130/157, Loss: 2.8012\n",
      "Epoch 15, Batch 131/157, Loss: 2.7456\n",
      "Epoch 15, Batch 132/157, Loss: 2.6610\n",
      "Epoch 15, Batch 133/157, Loss: 2.8117\n",
      "Epoch 15, Batch 134/157, Loss: 2.7786\n",
      "Epoch 15, Batch 135/157, Loss: 2.7749\n",
      "Epoch 15, Batch 136/157, Loss: 2.7081\n",
      "Epoch 15, Batch 137/157, Loss: 2.7288\n",
      "Epoch 15, Batch 138/157, Loss: 2.7899\n",
      "Epoch 15, Batch 139/157, Loss: 2.7054\n",
      "Epoch 15, Batch 140/157, Loss: 2.7690\n",
      "Epoch 15, Batch 141/157, Loss: 2.7844\n",
      "Epoch 15, Batch 142/157, Loss: 2.7223\n",
      "Epoch 15, Batch 143/157, Loss: 2.7243\n",
      "Epoch 15, Batch 144/157, Loss: 2.7243\n",
      "Epoch 15, Batch 145/157, Loss: 2.7232\n",
      "Epoch 15, Batch 146/157, Loss: 2.7631\n",
      "Epoch 15, Batch 147/157, Loss: 2.7382\n",
      "Epoch 15, Batch 148/157, Loss: 2.6982\n",
      "Epoch 15, Batch 149/157, Loss: 2.6770\n",
      "Epoch 15, Batch 150/157, Loss: 2.7190\n",
      "Epoch 15, Batch 151/157, Loss: 2.6889\n",
      "Epoch 15, Batch 152/157, Loss: 2.7184\n",
      "Epoch 15, Batch 153/157, Loss: 2.7050\n",
      "Epoch 15, Batch 154/157, Loss: 2.6942\n",
      "Epoch 15, Batch 155/157, Loss: 2.6872\n",
      "Epoch 15, Batch 156/157, Loss: 2.8166\n",
      "Epoch 15, Batch 157/157, Loss: 2.7883\n",
      "Epoch 15/50, Average Loss: 2.7368\n",
      "Epoch 16, Batch 1/157, Loss: 2.7564\n",
      "Epoch 16, Batch 2/157, Loss: 2.7147\n",
      "Epoch 16, Batch 3/157, Loss: 2.7710\n",
      "Epoch 16, Batch 4/157, Loss: 2.7350\n",
      "Epoch 16, Batch 5/157, Loss: 2.7594\n",
      "Epoch 16, Batch 6/157, Loss: 2.7589\n",
      "Epoch 16, Batch 7/157, Loss: 2.6974\n",
      "Epoch 16, Batch 8/157, Loss: 2.6768\n",
      "Epoch 16, Batch 9/157, Loss: 2.6718\n",
      "Epoch 16, Batch 10/157, Loss: 2.7658\n",
      "Epoch 16, Batch 11/157, Loss: 2.7242\n",
      "Epoch 16, Batch 12/157, Loss: 2.7138\n",
      "Epoch 16, Batch 13/157, Loss: 2.7464\n",
      "Epoch 16, Batch 14/157, Loss: 2.7315\n",
      "Epoch 16, Batch 15/157, Loss: 2.7438\n",
      "Epoch 16, Batch 16/157, Loss: 2.7546\n",
      "Epoch 16, Batch 17/157, Loss: 2.7467\n",
      "Epoch 16, Batch 18/157, Loss: 2.6372\n",
      "Epoch 16, Batch 19/157, Loss: 2.8080\n",
      "Epoch 16, Batch 20/157, Loss: 2.7846\n",
      "Epoch 16, Batch 21/157, Loss: 2.6933\n",
      "Epoch 16, Batch 22/157, Loss: 2.7514\n",
      "Epoch 16, Batch 23/157, Loss: 2.6590\n",
      "Epoch 16, Batch 24/157, Loss: 2.7231\n",
      "Epoch 16, Batch 25/157, Loss: 2.6764\n",
      "Epoch 16, Batch 26/157, Loss: 2.7307\n",
      "Epoch 16, Batch 27/157, Loss: 2.6638\n",
      "Epoch 16, Batch 28/157, Loss: 2.7894\n",
      "Epoch 16, Batch 29/157, Loss: 2.6705\n",
      "Epoch 16, Batch 30/157, Loss: 2.6934\n",
      "Epoch 16, Batch 31/157, Loss: 2.6832\n",
      "Epoch 16, Batch 32/157, Loss: 2.8048\n",
      "Epoch 16, Batch 33/157, Loss: 2.7738\n",
      "Epoch 16, Batch 34/157, Loss: 2.6533\n",
      "Epoch 16, Batch 35/157, Loss: 2.8728\n",
      "Epoch 16, Batch 36/157, Loss: 2.8458\n",
      "Epoch 16, Batch 37/157, Loss: 2.6842\n",
      "Epoch 16, Batch 38/157, Loss: 2.7170\n",
      "Epoch 16, Batch 39/157, Loss: 2.7473\n",
      "Epoch 16, Batch 40/157, Loss: 2.7055\n",
      "Epoch 16, Batch 41/157, Loss: 2.7633\n",
      "Epoch 16, Batch 42/157, Loss: 2.7157\n",
      "Epoch 16, Batch 43/157, Loss: 2.7547\n",
      "Epoch 16, Batch 44/157, Loss: 2.7238\n",
      "Epoch 16, Batch 45/157, Loss: 2.7363\n",
      "Epoch 16, Batch 46/157, Loss: 2.7460\n",
      "Epoch 16, Batch 47/157, Loss: 2.7601\n",
      "Epoch 16, Batch 48/157, Loss: 2.7742\n",
      "Epoch 16, Batch 49/157, Loss: 2.7240\n",
      "Epoch 16, Batch 50/157, Loss: 2.7252\n",
      "Epoch 16, Batch 51/157, Loss: 2.7725\n",
      "Epoch 16, Batch 52/157, Loss: 2.7788\n",
      "Epoch 16, Batch 53/157, Loss: 2.7114\n",
      "Epoch 16, Batch 54/157, Loss: 2.6359\n",
      "Epoch 16, Batch 55/157, Loss: 2.7785\n",
      "Epoch 16, Batch 56/157, Loss: 2.7659\n",
      "Epoch 16, Batch 57/157, Loss: 2.7135\n",
      "Epoch 16, Batch 58/157, Loss: 2.7415\n",
      "Epoch 16, Batch 59/157, Loss: 2.7691\n",
      "Epoch 16, Batch 60/157, Loss: 2.7911\n",
      "Epoch 16, Batch 61/157, Loss: 2.7125\n",
      "Epoch 16, Batch 62/157, Loss: 2.7458\n",
      "Epoch 16, Batch 63/157, Loss: 2.7264\n",
      "Epoch 16, Batch 64/157, Loss: 2.8063\n",
      "Epoch 16, Batch 65/157, Loss: 2.7693\n",
      "Epoch 16, Batch 66/157, Loss: 2.6814\n",
      "Epoch 16, Batch 67/157, Loss: 2.7768\n",
      "Epoch 16, Batch 68/157, Loss: 2.7651\n",
      "Epoch 16, Batch 69/157, Loss: 2.7592\n",
      "Epoch 16, Batch 70/157, Loss: 2.7197\n",
      "Epoch 16, Batch 71/157, Loss: 2.7041\n",
      "Epoch 16, Batch 72/157, Loss: 2.7084\n",
      "Epoch 16, Batch 73/157, Loss: 2.7103\n",
      "Epoch 16, Batch 74/157, Loss: 2.7402\n",
      "Epoch 16, Batch 75/157, Loss: 2.8145\n",
      "Epoch 16, Batch 76/157, Loss: 2.6936\n",
      "Epoch 16, Batch 77/157, Loss: 2.8007\n",
      "Epoch 16, Batch 78/157, Loss: 2.6908\n",
      "Epoch 16, Batch 79/157, Loss: 2.7404\n",
      "Epoch 16, Batch 80/157, Loss: 2.7828\n",
      "Epoch 16, Batch 81/157, Loss: 2.7263\n",
      "Epoch 16, Batch 82/157, Loss: 2.7382\n",
      "Epoch 16, Batch 83/157, Loss: 2.7661\n",
      "Epoch 16, Batch 84/157, Loss: 2.6674\n",
      "Epoch 16, Batch 85/157, Loss: 2.7530\n",
      "Epoch 16, Batch 86/157, Loss: 2.6947\n",
      "Epoch 16, Batch 87/157, Loss: 2.7669\n",
      "Epoch 16, Batch 88/157, Loss: 2.8047\n",
      "Epoch 16, Batch 89/157, Loss: 2.7331\n",
      "Epoch 16, Batch 90/157, Loss: 2.7901\n",
      "Epoch 16, Batch 91/157, Loss: 2.6889\n",
      "Epoch 16, Batch 92/157, Loss: 2.7175\n",
      "Epoch 16, Batch 93/157, Loss: 2.7256\n",
      "Epoch 16, Batch 94/157, Loss: 2.6887\n",
      "Epoch 16, Batch 95/157, Loss: 2.7610\n",
      "Epoch 16, Batch 96/157, Loss: 2.7395\n",
      "Epoch 16, Batch 97/157, Loss: 2.6339\n",
      "Epoch 16, Batch 98/157, Loss: 2.7940\n",
      "Epoch 16, Batch 99/157, Loss: 2.6862\n",
      "Epoch 16, Batch 100/157, Loss: 2.7204\n",
      "Epoch 16, Batch 101/157, Loss: 2.7032\n",
      "Epoch 16, Batch 102/157, Loss: 2.7227\n",
      "Epoch 16, Batch 103/157, Loss: 2.7268\n",
      "Epoch 16, Batch 104/157, Loss: 2.7499\n",
      "Epoch 16, Batch 105/157, Loss: 2.7186\n",
      "Epoch 16, Batch 106/157, Loss: 2.7581\n",
      "Epoch 16, Batch 107/157, Loss: 2.7065\n",
      "Epoch 16, Batch 108/157, Loss: 2.7015\n",
      "Epoch 16, Batch 109/157, Loss: 2.6965\n",
      "Epoch 16, Batch 110/157, Loss: 2.6822\n",
      "Epoch 16, Batch 111/157, Loss: 2.6971\n",
      "Epoch 16, Batch 112/157, Loss: 2.7817\n",
      "Epoch 16, Batch 113/157, Loss: 2.7589\n",
      "Epoch 16, Batch 114/157, Loss: 2.7200\n",
      "Epoch 16, Batch 115/157, Loss: 2.7706\n",
      "Epoch 16, Batch 116/157, Loss: 2.7433\n",
      "Epoch 16, Batch 117/157, Loss: 2.6343\n",
      "Epoch 16, Batch 118/157, Loss: 2.7302\n",
      "Epoch 16, Batch 119/157, Loss: 2.7447\n",
      "Epoch 16, Batch 120/157, Loss: 2.7901\n",
      "Epoch 16, Batch 121/157, Loss: 2.7251\n",
      "Epoch 16, Batch 122/157, Loss: 2.7029\n",
      "Epoch 16, Batch 123/157, Loss: 2.7132\n",
      "Epoch 16, Batch 124/157, Loss: 2.7569\n",
      "Epoch 16, Batch 125/157, Loss: 2.6795\n",
      "Epoch 16, Batch 126/157, Loss: 2.7625\n",
      "Epoch 16, Batch 127/157, Loss: 2.7498\n",
      "Epoch 16, Batch 128/157, Loss: 2.7690\n",
      "Epoch 16, Batch 129/157, Loss: 2.7327\n",
      "Epoch 16, Batch 130/157, Loss: 2.7040\n",
      "Epoch 16, Batch 131/157, Loss: 2.7721\n",
      "Epoch 16, Batch 132/157, Loss: 2.6944\n",
      "Epoch 16, Batch 133/157, Loss: 2.7266\n",
      "Epoch 16, Batch 134/157, Loss: 2.7676\n",
      "Epoch 16, Batch 135/157, Loss: 2.7469\n",
      "Epoch 16, Batch 136/157, Loss: 2.7834\n",
      "Epoch 16, Batch 137/157, Loss: 2.7297\n",
      "Epoch 16, Batch 138/157, Loss: 2.6725\n",
      "Epoch 16, Batch 139/157, Loss: 2.7198\n",
      "Epoch 16, Batch 140/157, Loss: 2.8508\n",
      "Epoch 16, Batch 141/157, Loss: 2.7017\n",
      "Epoch 16, Batch 142/157, Loss: 2.7367\n",
      "Epoch 16, Batch 143/157, Loss: 2.7275\n",
      "Epoch 16, Batch 144/157, Loss: 2.7176\n",
      "Epoch 16, Batch 145/157, Loss: 2.8228\n",
      "Epoch 16, Batch 146/157, Loss: 2.6848\n",
      "Epoch 16, Batch 147/157, Loss: 2.7168\n",
      "Epoch 16, Batch 148/157, Loss: 2.7114\n",
      "Epoch 16, Batch 149/157, Loss: 2.7254\n",
      "Epoch 16, Batch 150/157, Loss: 2.7037\n",
      "Epoch 16, Batch 151/157, Loss: 2.6942\n",
      "Epoch 16, Batch 152/157, Loss: 2.7439\n",
      "Epoch 16, Batch 153/157, Loss: 2.7507\n",
      "Epoch 16, Batch 154/157, Loss: 2.6751\n",
      "Epoch 16, Batch 155/157, Loss: 2.7418\n",
      "Epoch 16, Batch 156/157, Loss: 2.7962\n",
      "Epoch 16, Batch 157/157, Loss: 2.7465\n",
      "Epoch 16/50, Average Loss: 2.7341\n",
      "Epoch 17, Batch 1/157, Loss: 2.7320\n",
      "Epoch 17, Batch 2/157, Loss: 2.7124\n",
      "Epoch 17, Batch 3/157, Loss: 2.7595\n",
      "Epoch 17, Batch 4/157, Loss: 2.6966\n",
      "Epoch 17, Batch 5/157, Loss: 2.6820\n",
      "Epoch 17, Batch 6/157, Loss: 2.7356\n",
      "Epoch 17, Batch 7/157, Loss: 2.6715\n",
      "Epoch 17, Batch 8/157, Loss: 2.7506\n",
      "Epoch 17, Batch 9/157, Loss: 2.7077\n",
      "Epoch 17, Batch 10/157, Loss: 2.7130\n",
      "Epoch 17, Batch 11/157, Loss: 2.7353\n",
      "Epoch 17, Batch 12/157, Loss: 2.6349\n",
      "Epoch 17, Batch 13/157, Loss: 2.8099\n",
      "Epoch 17, Batch 14/157, Loss: 2.6972\n",
      "Epoch 17, Batch 15/157, Loss: 2.7004\n",
      "Epoch 17, Batch 16/157, Loss: 2.7692\n",
      "Epoch 17, Batch 17/157, Loss: 2.7501\n",
      "Epoch 17, Batch 18/157, Loss: 2.7249\n",
      "Epoch 17, Batch 19/157, Loss: 2.7186\n",
      "Epoch 17, Batch 20/157, Loss: 2.7357\n",
      "Epoch 17, Batch 21/157, Loss: 2.6752\n",
      "Epoch 17, Batch 22/157, Loss: 2.7410\n",
      "Epoch 17, Batch 23/157, Loss: 2.7251\n",
      "Epoch 17, Batch 24/157, Loss: 2.7129\n",
      "Epoch 17, Batch 25/157, Loss: 2.7385\n",
      "Epoch 17, Batch 26/157, Loss: 2.7744\n",
      "Epoch 17, Batch 27/157, Loss: 2.7137\n",
      "Epoch 17, Batch 28/157, Loss: 2.7121\n",
      "Epoch 17, Batch 29/157, Loss: 2.7244\n",
      "Epoch 17, Batch 30/157, Loss: 2.7405\n",
      "Epoch 17, Batch 31/157, Loss: 2.7490\n",
      "Epoch 17, Batch 32/157, Loss: 2.7260\n",
      "Epoch 17, Batch 33/157, Loss: 2.7224\n",
      "Epoch 17, Batch 34/157, Loss: 2.7430\n",
      "Epoch 17, Batch 35/157, Loss: 2.7200\n",
      "Epoch 17, Batch 36/157, Loss: 2.7228\n",
      "Epoch 17, Batch 37/157, Loss: 2.7242\n",
      "Epoch 17, Batch 38/157, Loss: 2.7337\n",
      "Epoch 17, Batch 39/157, Loss: 2.6639\n",
      "Epoch 17, Batch 40/157, Loss: 2.7264\n",
      "Epoch 17, Batch 41/157, Loss: 2.7299\n",
      "Epoch 17, Batch 42/157, Loss: 2.7025\n",
      "Epoch 17, Batch 43/157, Loss: 2.7119\n",
      "Epoch 17, Batch 44/157, Loss: 2.7044\n",
      "Epoch 17, Batch 45/157, Loss: 2.7667\n",
      "Epoch 17, Batch 46/157, Loss: 2.7307\n",
      "Epoch 17, Batch 47/157, Loss: 2.7613\n",
      "Epoch 17, Batch 48/157, Loss: 2.7212\n",
      "Epoch 17, Batch 49/157, Loss: 2.6971\n",
      "Epoch 17, Batch 50/157, Loss: 2.7439\n",
      "Epoch 17, Batch 51/157, Loss: 2.7118\n",
      "Epoch 17, Batch 52/157, Loss: 2.6854\n",
      "Epoch 17, Batch 53/157, Loss: 2.6946\n",
      "Epoch 17, Batch 54/157, Loss: 2.6827\n",
      "Epoch 17, Batch 55/157, Loss: 2.7035\n",
      "Epoch 17, Batch 56/157, Loss: 2.7415\n",
      "Epoch 17, Batch 57/157, Loss: 2.7527\n",
      "Epoch 17, Batch 58/157, Loss: 2.7829\n",
      "Epoch 17, Batch 59/157, Loss: 2.6938\n",
      "Epoch 17, Batch 60/157, Loss: 2.7357\n",
      "Epoch 17, Batch 61/157, Loss: 2.7630\n",
      "Epoch 17, Batch 62/157, Loss: 2.7376\n",
      "Epoch 17, Batch 63/157, Loss: 2.6969\n",
      "Epoch 17, Batch 64/157, Loss: 2.7814\n",
      "Epoch 17, Batch 65/157, Loss: 2.7392\n",
      "Epoch 17, Batch 66/157, Loss: 2.7409\n",
      "Epoch 17, Batch 67/157, Loss: 2.6883\n",
      "Epoch 17, Batch 68/157, Loss: 2.7223\n",
      "Epoch 17, Batch 69/157, Loss: 2.7678\n",
      "Epoch 17, Batch 70/157, Loss: 2.7295\n",
      "Epoch 17, Batch 71/157, Loss: 2.7976\n",
      "Epoch 17, Batch 72/157, Loss: 2.7278\n",
      "Epoch 17, Batch 73/157, Loss: 2.7062\n",
      "Epoch 17, Batch 74/157, Loss: 2.7157\n",
      "Epoch 17, Batch 75/157, Loss: 2.7734\n",
      "Epoch 17, Batch 76/157, Loss: 2.7669\n",
      "Epoch 17, Batch 77/157, Loss: 2.7411\n",
      "Epoch 17, Batch 78/157, Loss: 2.7239\n",
      "Epoch 17, Batch 79/157, Loss: 2.7482\n",
      "Epoch 17, Batch 80/157, Loss: 2.6687\n",
      "Epoch 17, Batch 81/157, Loss: 2.7450\n",
      "Epoch 17, Batch 82/157, Loss: 2.7341\n",
      "Epoch 17, Batch 83/157, Loss: 2.8084\n",
      "Epoch 17, Batch 84/157, Loss: 2.7592\n",
      "Epoch 17, Batch 85/157, Loss: 2.7292\n",
      "Epoch 17, Batch 86/157, Loss: 2.7429\n",
      "Epoch 17, Batch 87/157, Loss: 2.7457\n",
      "Epoch 17, Batch 88/157, Loss: 2.7272\n",
      "Epoch 17, Batch 89/157, Loss: 2.7025\n",
      "Epoch 17, Batch 90/157, Loss: 2.6945\n",
      "Epoch 17, Batch 91/157, Loss: 2.6939\n",
      "Epoch 17, Batch 92/157, Loss: 2.7590\n",
      "Epoch 17, Batch 93/157, Loss: 2.7830\n",
      "Epoch 17, Batch 94/157, Loss: 2.7401\n",
      "Epoch 17, Batch 95/157, Loss: 2.7549\n",
      "Epoch 17, Batch 96/157, Loss: 2.7585\n",
      "Epoch 17, Batch 97/157, Loss: 2.7800\n",
      "Epoch 17, Batch 98/157, Loss: 2.7014\n",
      "Epoch 17, Batch 99/157, Loss: 2.6947\n",
      "Epoch 17, Batch 100/157, Loss: 2.7372\n",
      "Epoch 17, Batch 101/157, Loss: 2.7836\n",
      "Epoch 17, Batch 102/157, Loss: 2.7692\n",
      "Epoch 17, Batch 103/157, Loss: 2.7637\n",
      "Epoch 17, Batch 104/157, Loss: 2.7331\n",
      "Epoch 17, Batch 105/157, Loss: 2.7294\n",
      "Epoch 17, Batch 106/157, Loss: 2.7646\n",
      "Epoch 17, Batch 107/157, Loss: 2.6857\n",
      "Epoch 17, Batch 108/157, Loss: 2.7592\n",
      "Epoch 17, Batch 109/157, Loss: 2.7236\n",
      "Epoch 17, Batch 110/157, Loss: 2.7235\n",
      "Epoch 17, Batch 111/157, Loss: 2.6990\n",
      "Epoch 17, Batch 112/157, Loss: 2.7320\n",
      "Epoch 17, Batch 113/157, Loss: 2.7162\n",
      "Epoch 17, Batch 114/157, Loss: 2.7217\n",
      "Epoch 17, Batch 115/157, Loss: 2.7144\n",
      "Epoch 17, Batch 116/157, Loss: 2.7481\n",
      "Epoch 17, Batch 117/157, Loss: 2.7379\n",
      "Epoch 17, Batch 118/157, Loss: 2.6767\n",
      "Epoch 17, Batch 119/157, Loss: 2.6881\n",
      "Epoch 17, Batch 120/157, Loss: 2.6855\n",
      "Epoch 17, Batch 121/157, Loss: 2.7459\n",
      "Epoch 17, Batch 122/157, Loss: 2.6993\n",
      "Epoch 17, Batch 123/157, Loss: 2.7000\n",
      "Epoch 17, Batch 124/157, Loss: 2.7480\n",
      "Epoch 17, Batch 125/157, Loss: 2.7445\n",
      "Epoch 17, Batch 126/157, Loss: 2.7079\n",
      "Epoch 17, Batch 127/157, Loss: 2.7878\n",
      "Epoch 17, Batch 128/157, Loss: 2.7383\n",
      "Epoch 17, Batch 129/157, Loss: 2.7125\n",
      "Epoch 17, Batch 130/157, Loss: 2.7405\n",
      "Epoch 17, Batch 131/157, Loss: 2.7190\n",
      "Epoch 17, Batch 132/157, Loss: 2.6809\n",
      "Epoch 17, Batch 133/157, Loss: 2.7417\n",
      "Epoch 17, Batch 134/157, Loss: 2.7882\n",
      "Epoch 17, Batch 135/157, Loss: 2.7257\n",
      "Epoch 17, Batch 136/157, Loss: 2.6454\n",
      "Epoch 17, Batch 137/157, Loss: 2.7484\n",
      "Epoch 17, Batch 138/157, Loss: 2.6782\n",
      "Epoch 17, Batch 139/157, Loss: 2.7770\n",
      "Epoch 17, Batch 140/157, Loss: 2.7166\n",
      "Epoch 17, Batch 141/157, Loss: 2.7505\n",
      "Epoch 17, Batch 142/157, Loss: 2.8347\n",
      "Epoch 17, Batch 143/157, Loss: 2.7960\n",
      "Epoch 17, Batch 144/157, Loss: 2.7441\n",
      "Epoch 17, Batch 145/157, Loss: 2.7641\n",
      "Epoch 17, Batch 146/157, Loss: 2.6998\n",
      "Epoch 17, Batch 147/157, Loss: 2.7292\n",
      "Epoch 17, Batch 148/157, Loss: 2.7941\n",
      "Epoch 17, Batch 149/157, Loss: 2.7105\n",
      "Epoch 17, Batch 150/157, Loss: 2.7141\n",
      "Epoch 17, Batch 151/157, Loss: 2.7455\n",
      "Epoch 17, Batch 152/157, Loss: 2.6725\n",
      "Epoch 17, Batch 153/157, Loss: 2.7498\n",
      "Epoch 17, Batch 154/157, Loss: 2.7507\n",
      "Epoch 17, Batch 155/157, Loss: 2.6581\n",
      "Epoch 17, Batch 156/157, Loss: 2.7647\n",
      "Epoch 17, Batch 157/157, Loss: 2.7974\n",
      "Epoch 17/50, Average Loss: 2.7305\n",
      "Epoch 18, Batch 1/157, Loss: 2.7624\n",
      "Epoch 18, Batch 2/157, Loss: 2.7764\n",
      "Epoch 18, Batch 3/157, Loss: 2.8058\n",
      "Epoch 18, Batch 4/157, Loss: 2.7041\n",
      "Epoch 18, Batch 5/157, Loss: 2.7766\n",
      "Epoch 18, Batch 6/157, Loss: 2.7288\n",
      "Epoch 18, Batch 7/157, Loss: 2.7446\n",
      "Epoch 18, Batch 8/157, Loss: 2.7076\n",
      "Epoch 18, Batch 9/157, Loss: 2.6743\n",
      "Epoch 18, Batch 10/157, Loss: 2.7095\n",
      "Epoch 18, Batch 11/157, Loss: 2.7133\n",
      "Epoch 18, Batch 12/157, Loss: 2.7330\n",
      "Epoch 18, Batch 13/157, Loss: 2.7708\n",
      "Epoch 18, Batch 14/157, Loss: 2.7784\n",
      "Epoch 18, Batch 15/157, Loss: 2.7578\n",
      "Epoch 18, Batch 16/157, Loss: 2.7688\n",
      "Epoch 18, Batch 17/157, Loss: 2.7173\n",
      "Epoch 18, Batch 18/157, Loss: 2.7304\n",
      "Epoch 18, Batch 19/157, Loss: 2.7294\n",
      "Epoch 18, Batch 20/157, Loss: 2.7547\n",
      "Epoch 18, Batch 21/157, Loss: 2.7508\n",
      "Epoch 18, Batch 22/157, Loss: 2.7405\n",
      "Epoch 18, Batch 23/157, Loss: 2.6786\n",
      "Epoch 18, Batch 24/157, Loss: 2.6567\n",
      "Epoch 18, Batch 25/157, Loss: 2.6878\n",
      "Epoch 18, Batch 26/157, Loss: 2.7595\n",
      "Epoch 18, Batch 27/157, Loss: 2.7306\n",
      "Epoch 18, Batch 28/157, Loss: 2.7189\n",
      "Epoch 18, Batch 29/157, Loss: 2.7104\n",
      "Epoch 18, Batch 30/157, Loss: 2.7472\n",
      "Epoch 18, Batch 31/157, Loss: 2.7449\n",
      "Epoch 18, Batch 32/157, Loss: 2.7418\n",
      "Epoch 18, Batch 33/157, Loss: 2.7573\n",
      "Epoch 18, Batch 34/157, Loss: 2.7437\n",
      "Epoch 18, Batch 35/157, Loss: 2.6868\n",
      "Epoch 18, Batch 36/157, Loss: 2.7286\n",
      "Epoch 18, Batch 37/157, Loss: 2.7438\n",
      "Epoch 18, Batch 38/157, Loss: 2.7071\n",
      "Epoch 18, Batch 39/157, Loss: 2.7032\n",
      "Epoch 18, Batch 40/157, Loss: 2.7496\n",
      "Epoch 18, Batch 41/157, Loss: 2.7544\n",
      "Epoch 18, Batch 42/157, Loss: 2.7378\n",
      "Epoch 18, Batch 43/157, Loss: 2.7434\n",
      "Epoch 18, Batch 44/157, Loss: 2.7157\n",
      "Epoch 18, Batch 45/157, Loss: 2.6608\n",
      "Epoch 18, Batch 46/157, Loss: 2.7598\n",
      "Epoch 18, Batch 47/157, Loss: 2.7122\n",
      "Epoch 18, Batch 48/157, Loss: 2.6915\n",
      "Epoch 18, Batch 49/157, Loss: 2.7317\n",
      "Epoch 18, Batch 50/157, Loss: 2.7544\n",
      "Epoch 18, Batch 51/157, Loss: 2.7284\n",
      "Epoch 18, Batch 52/157, Loss: 2.7137\n",
      "Epoch 18, Batch 53/157, Loss: 2.7041\n",
      "Epoch 18, Batch 54/157, Loss: 2.7324\n",
      "Epoch 18, Batch 55/157, Loss: 2.7692\n",
      "Epoch 18, Batch 56/157, Loss: 2.7220\n",
      "Epoch 18, Batch 57/157, Loss: 2.7227\n",
      "Epoch 18, Batch 58/157, Loss: 2.6605\n",
      "Epoch 18, Batch 59/157, Loss: 2.7127\n",
      "Epoch 18, Batch 60/157, Loss: 2.7334\n",
      "Epoch 18, Batch 61/157, Loss: 2.7115\n",
      "Epoch 18, Batch 62/157, Loss: 2.7191\n",
      "Epoch 18, Batch 63/157, Loss: 2.7246\n",
      "Epoch 18, Batch 64/157, Loss: 2.6816\n",
      "Epoch 18, Batch 65/157, Loss: 2.7453\n",
      "Epoch 18, Batch 66/157, Loss: 2.7678\n",
      "Epoch 18, Batch 67/157, Loss: 2.6214\n",
      "Epoch 18, Batch 68/157, Loss: 2.7091\n",
      "Epoch 18, Batch 69/157, Loss: 2.7069\n",
      "Epoch 18, Batch 70/157, Loss: 2.7434\n",
      "Epoch 18, Batch 71/157, Loss: 2.7807\n",
      "Epoch 18, Batch 72/157, Loss: 2.7065\n",
      "Epoch 18, Batch 73/157, Loss: 2.7233\n",
      "Epoch 18, Batch 74/157, Loss: 2.7240\n",
      "Epoch 18, Batch 75/157, Loss: 2.7885\n",
      "Epoch 18, Batch 76/157, Loss: 2.7621\n",
      "Epoch 18, Batch 77/157, Loss: 2.6841\n",
      "Epoch 18, Batch 78/157, Loss: 2.7303\n",
      "Epoch 18, Batch 79/157, Loss: 2.8262\n",
      "Epoch 18, Batch 80/157, Loss: 2.7581\n",
      "Epoch 18, Batch 81/157, Loss: 2.7854\n",
      "Epoch 18, Batch 82/157, Loss: 2.7076\n",
      "Epoch 18, Batch 83/157, Loss: 2.7347\n",
      "Epoch 18, Batch 84/157, Loss: 2.7400\n",
      "Epoch 18, Batch 85/157, Loss: 2.7391\n",
      "Epoch 18, Batch 86/157, Loss: 2.7475\n",
      "Epoch 18, Batch 87/157, Loss: 2.7191\n",
      "Epoch 18, Batch 88/157, Loss: 2.7079\n",
      "Epoch 18, Batch 89/157, Loss: 2.7091\n",
      "Epoch 18, Batch 90/157, Loss: 2.7587\n",
      "Epoch 18, Batch 91/157, Loss: 2.7849\n",
      "Epoch 18, Batch 92/157, Loss: 2.7532\n",
      "Epoch 18, Batch 93/157, Loss: 2.7885\n",
      "Epoch 18, Batch 94/157, Loss: 2.6795\n",
      "Epoch 18, Batch 95/157, Loss: 2.7765\n",
      "Epoch 18, Batch 96/157, Loss: 2.7281\n",
      "Epoch 18, Batch 97/157, Loss: 2.7270\n",
      "Epoch 18, Batch 98/157, Loss: 2.6701\n",
      "Epoch 18, Batch 99/157, Loss: 2.6863\n",
      "Epoch 18, Batch 100/157, Loss: 2.7709\n",
      "Epoch 18, Batch 101/157, Loss: 2.7327\n",
      "Epoch 18, Batch 102/157, Loss: 2.7518\n",
      "Epoch 18, Batch 103/157, Loss: 2.7404\n",
      "Epoch 18, Batch 104/157, Loss: 2.7468\n",
      "Epoch 18, Batch 105/157, Loss: 2.7434\n",
      "Epoch 18, Batch 106/157, Loss: 2.7090\n",
      "Epoch 18, Batch 107/157, Loss: 2.7431\n",
      "Epoch 18, Batch 108/157, Loss: 2.7890\n",
      "Epoch 18, Batch 109/157, Loss: 2.7536\n",
      "Epoch 18, Batch 110/157, Loss: 2.7395\n",
      "Epoch 18, Batch 111/157, Loss: 2.7477\n",
      "Epoch 18, Batch 112/157, Loss: 2.7213\n",
      "Epoch 18, Batch 113/157, Loss: 2.7776\n",
      "Epoch 18, Batch 114/157, Loss: 2.7203\n",
      "Epoch 18, Batch 115/157, Loss: 2.7199\n",
      "Epoch 18, Batch 116/157, Loss: 2.7699\n",
      "Epoch 18, Batch 117/157, Loss: 2.6550\n",
      "Epoch 18, Batch 118/157, Loss: 2.7403\n",
      "Epoch 18, Batch 119/157, Loss: 2.7129\n",
      "Epoch 18, Batch 120/157, Loss: 2.6835\n",
      "Epoch 18, Batch 121/157, Loss: 2.7052\n",
      "Epoch 18, Batch 122/157, Loss: 2.7224\n",
      "Epoch 18, Batch 123/157, Loss: 2.7224\n",
      "Epoch 18, Batch 124/157, Loss: 2.7460\n",
      "Epoch 18, Batch 125/157, Loss: 2.7043\n",
      "Epoch 18, Batch 126/157, Loss: 2.7968\n",
      "Epoch 18, Batch 127/157, Loss: 2.6901\n",
      "Epoch 18, Batch 128/157, Loss: 2.6198\n",
      "Epoch 18, Batch 129/157, Loss: 2.7754\n",
      "Epoch 18, Batch 130/157, Loss: 2.7345\n",
      "Epoch 18, Batch 131/157, Loss: 2.7084\n",
      "Epoch 18, Batch 132/157, Loss: 2.6726\n",
      "Epoch 18, Batch 133/157, Loss: 2.7612\n",
      "Epoch 18, Batch 134/157, Loss: 2.7146\n",
      "Epoch 18, Batch 135/157, Loss: 2.6714\n",
      "Epoch 18, Batch 136/157, Loss: 2.7262\n",
      "Epoch 18, Batch 137/157, Loss: 2.7198\n",
      "Epoch 18, Batch 138/157, Loss: 2.7097\n",
      "Epoch 18, Batch 139/157, Loss: 2.7750\n",
      "Epoch 18, Batch 140/157, Loss: 2.7919\n",
      "Epoch 18, Batch 141/157, Loss: 2.8106\n",
      "Epoch 18, Batch 142/157, Loss: 2.7145\n",
      "Epoch 18, Batch 143/157, Loss: 2.6779\n",
      "Epoch 18, Batch 144/157, Loss: 2.7338\n",
      "Epoch 18, Batch 145/157, Loss: 2.6876\n",
      "Epoch 18, Batch 146/157, Loss: 2.7265\n",
      "Epoch 18, Batch 147/157, Loss: 2.7725\n",
      "Epoch 18, Batch 148/157, Loss: 2.7833\n",
      "Epoch 18, Batch 149/157, Loss: 2.7098\n",
      "Epoch 18, Batch 150/157, Loss: 2.7149\n",
      "Epoch 18, Batch 151/157, Loss: 2.7225\n",
      "Epoch 18, Batch 152/157, Loss: 2.7087\n",
      "Epoch 18, Batch 153/157, Loss: 2.6722\n",
      "Epoch 18, Batch 154/157, Loss: 2.7409\n",
      "Epoch 18, Batch 155/157, Loss: 2.7696\n",
      "Epoch 18, Batch 156/157, Loss: 2.7630\n",
      "Epoch 18, Batch 157/157, Loss: 2.8807\n",
      "Epoch 18/50, Average Loss: 2.7317\n",
      "Epoch 19, Batch 1/157, Loss: 2.7325\n",
      "Epoch 19, Batch 2/157, Loss: 2.7560\n",
      "Epoch 19, Batch 3/157, Loss: 2.7305\n",
      "Epoch 19, Batch 4/157, Loss: 2.7158\n",
      "Epoch 19, Batch 5/157, Loss: 2.7009\n",
      "Epoch 19, Batch 6/157, Loss: 2.7974\n",
      "Epoch 19, Batch 7/157, Loss: 2.7131\n",
      "Epoch 19, Batch 8/157, Loss: 2.7254\n",
      "Epoch 19, Batch 9/157, Loss: 2.7344\n",
      "Epoch 19, Batch 10/157, Loss: 2.7465\n",
      "Epoch 19, Batch 11/157, Loss: 2.7049\n",
      "Epoch 19, Batch 12/157, Loss: 2.7280\n",
      "Epoch 19, Batch 13/157, Loss: 2.6852\n",
      "Epoch 19, Batch 14/157, Loss: 2.7939\n",
      "Epoch 19, Batch 15/157, Loss: 2.7810\n",
      "Epoch 19, Batch 16/157, Loss: 2.7012\n",
      "Epoch 19, Batch 17/157, Loss: 2.6508\n",
      "Epoch 19, Batch 18/157, Loss: 2.7447\n",
      "Epoch 19, Batch 19/157, Loss: 2.6680\n",
      "Epoch 19, Batch 20/157, Loss: 2.7547\n",
      "Epoch 19, Batch 21/157, Loss: 2.7207\n",
      "Epoch 19, Batch 22/157, Loss: 2.7541\n",
      "Epoch 19, Batch 23/157, Loss: 2.7107\n",
      "Epoch 19, Batch 24/157, Loss: 2.8178\n",
      "Epoch 19, Batch 25/157, Loss: 2.7482\n",
      "Epoch 19, Batch 26/157, Loss: 2.7440\n",
      "Epoch 19, Batch 27/157, Loss: 2.7462\n",
      "Epoch 19, Batch 28/157, Loss: 2.7571\n",
      "Epoch 19, Batch 29/157, Loss: 2.7169\n",
      "Epoch 19, Batch 30/157, Loss: 2.7147\n",
      "Epoch 19, Batch 31/157, Loss: 2.7208\n",
      "Epoch 19, Batch 32/157, Loss: 2.7018\n",
      "Epoch 19, Batch 33/157, Loss: 2.7193\n",
      "Epoch 19, Batch 34/157, Loss: 2.7504\n",
      "Epoch 19, Batch 35/157, Loss: 2.7141\n",
      "Epoch 19, Batch 36/157, Loss: 2.7087\n",
      "Epoch 19, Batch 37/157, Loss: 2.7521\n",
      "Epoch 19, Batch 38/157, Loss: 2.7115\n",
      "Epoch 19, Batch 39/157, Loss: 2.7220\n",
      "Epoch 19, Batch 40/157, Loss: 2.7646\n",
      "Epoch 19, Batch 41/157, Loss: 2.7765\n",
      "Epoch 19, Batch 42/157, Loss: 2.7419\n",
      "Epoch 19, Batch 43/157, Loss: 2.6458\n",
      "Epoch 19, Batch 44/157, Loss: 2.6938\n",
      "Epoch 19, Batch 45/157, Loss: 2.7495\n",
      "Epoch 19, Batch 46/157, Loss: 2.7549\n",
      "Epoch 19, Batch 47/157, Loss: 2.7445\n",
      "Epoch 19, Batch 48/157, Loss: 2.7562\n",
      "Epoch 19, Batch 49/157, Loss: 2.6653\n",
      "Epoch 19, Batch 50/157, Loss: 2.7718\n",
      "Epoch 19, Batch 51/157, Loss: 2.6808\n",
      "Epoch 19, Batch 52/157, Loss: 2.7308\n",
      "Epoch 19, Batch 53/157, Loss: 2.6669\n",
      "Epoch 19, Batch 54/157, Loss: 2.7098\n",
      "Epoch 19, Batch 55/157, Loss: 2.7394\n",
      "Epoch 19, Batch 56/157, Loss: 2.7108\n",
      "Epoch 19, Batch 57/157, Loss: 2.8171\n",
      "Epoch 19, Batch 58/157, Loss: 2.7631\n",
      "Epoch 19, Batch 59/157, Loss: 2.7672\n",
      "Epoch 19, Batch 60/157, Loss: 2.8015\n",
      "Epoch 19, Batch 61/157, Loss: 2.7241\n",
      "Epoch 19, Batch 62/157, Loss: 2.7397\n",
      "Epoch 19, Batch 63/157, Loss: 2.7021\n",
      "Epoch 19, Batch 64/157, Loss: 2.7476\n",
      "Epoch 19, Batch 65/157, Loss: 2.6899\n",
      "Epoch 19, Batch 66/157, Loss: 2.7558\n",
      "Epoch 19, Batch 67/157, Loss: 2.7600\n",
      "Epoch 19, Batch 68/157, Loss: 2.7167\n",
      "Epoch 19, Batch 69/157, Loss: 2.7589\n",
      "Epoch 19, Batch 70/157, Loss: 2.6860\n",
      "Epoch 19, Batch 71/157, Loss: 2.7821\n",
      "Epoch 19, Batch 72/157, Loss: 2.7718\n",
      "Epoch 19, Batch 73/157, Loss: 2.6776\n",
      "Epoch 19, Batch 74/157, Loss: 2.7201\n",
      "Epoch 19, Batch 75/157, Loss: 2.7063\n",
      "Epoch 19, Batch 76/157, Loss: 2.7082\n",
      "Epoch 19, Batch 77/157, Loss: 2.6930\n",
      "Epoch 19, Batch 78/157, Loss: 2.8007\n",
      "Epoch 19, Batch 79/157, Loss: 2.7911\n",
      "Epoch 19, Batch 80/157, Loss: 2.7368\n",
      "Epoch 19, Batch 81/157, Loss: 2.7629\n",
      "Epoch 19, Batch 82/157, Loss: 2.7526\n",
      "Epoch 19, Batch 83/157, Loss: 2.7877\n",
      "Epoch 19, Batch 84/157, Loss: 2.6944\n",
      "Epoch 19, Batch 85/157, Loss: 2.8003\n",
      "Epoch 19, Batch 86/157, Loss: 2.7027\n",
      "Epoch 19, Batch 87/157, Loss: 2.7252\n",
      "Epoch 19, Batch 88/157, Loss: 2.6572\n",
      "Epoch 19, Batch 89/157, Loss: 2.8156\n",
      "Epoch 19, Batch 90/157, Loss: 2.6892\n",
      "Epoch 19, Batch 91/157, Loss: 2.7681\n",
      "Epoch 19, Batch 92/157, Loss: 2.7902\n",
      "Epoch 19, Batch 93/157, Loss: 2.7029\n",
      "Epoch 19, Batch 94/157, Loss: 2.7254\n",
      "Epoch 19, Batch 95/157, Loss: 2.7051\n",
      "Epoch 19, Batch 96/157, Loss: 2.7264\n",
      "Epoch 19, Batch 97/157, Loss: 2.7823\n",
      "Epoch 19, Batch 98/157, Loss: 2.7190\n",
      "Epoch 19, Batch 99/157, Loss: 2.7583\n",
      "Epoch 19, Batch 100/157, Loss: 2.7509\n",
      "Epoch 19, Batch 101/157, Loss: 2.7243\n",
      "Epoch 19, Batch 102/157, Loss: 2.6959\n",
      "Epoch 19, Batch 103/157, Loss: 2.7730\n",
      "Epoch 19, Batch 104/157, Loss: 2.7182\n",
      "Epoch 19, Batch 105/157, Loss: 2.7415\n",
      "Epoch 19, Batch 106/157, Loss: 2.6857\n",
      "Epoch 19, Batch 107/157, Loss: 2.7205\n",
      "Epoch 19, Batch 108/157, Loss: 2.7475\n",
      "Epoch 19, Batch 109/157, Loss: 2.7821\n",
      "Epoch 19, Batch 110/157, Loss: 2.6612\n",
      "Epoch 19, Batch 111/157, Loss: 2.6746\n",
      "Epoch 19, Batch 112/157, Loss: 2.7724\n",
      "Epoch 19, Batch 113/157, Loss: 2.7664\n",
      "Epoch 19, Batch 114/157, Loss: 2.7612\n",
      "Epoch 19, Batch 115/157, Loss: 2.7815\n",
      "Epoch 19, Batch 116/157, Loss: 2.6961\n",
      "Epoch 19, Batch 117/157, Loss: 2.7858\n",
      "Epoch 19, Batch 118/157, Loss: 2.6804\n",
      "Epoch 19, Batch 119/157, Loss: 2.7031\n",
      "Epoch 19, Batch 120/157, Loss: 2.7008\n",
      "Epoch 19, Batch 121/157, Loss: 2.6428\n",
      "Epoch 19, Batch 122/157, Loss: 2.7856\n",
      "Epoch 19, Batch 123/157, Loss: 2.7378\n",
      "Epoch 19, Batch 124/157, Loss: 2.6757\n",
      "Epoch 19, Batch 125/157, Loss: 2.7166\n",
      "Epoch 19, Batch 126/157, Loss: 2.7668\n",
      "Epoch 19, Batch 127/157, Loss: 2.6605\n",
      "Epoch 19, Batch 128/157, Loss: 2.6954\n",
      "Epoch 19, Batch 129/157, Loss: 2.7601\n",
      "Epoch 19, Batch 130/157, Loss: 2.7992\n",
      "Epoch 19, Batch 131/157, Loss: 2.7479\n",
      "Epoch 19, Batch 132/157, Loss: 2.7201\n",
      "Epoch 19, Batch 133/157, Loss: 2.7830\n",
      "Epoch 19, Batch 134/157, Loss: 2.7630\n",
      "Epoch 19, Batch 135/157, Loss: 2.7213\n",
      "Epoch 19, Batch 136/157, Loss: 2.7317\n",
      "Epoch 19, Batch 137/157, Loss: 2.6684\n",
      "Epoch 19, Batch 138/157, Loss: 2.7607\n",
      "Epoch 19, Batch 139/157, Loss: 2.7114\n",
      "Epoch 19, Batch 140/157, Loss: 2.7182\n",
      "Epoch 19, Batch 141/157, Loss: 2.7419\n",
      "Epoch 19, Batch 142/157, Loss: 2.7411\n",
      "Epoch 19, Batch 143/157, Loss: 2.7265\n",
      "Epoch 19, Batch 144/157, Loss: 2.7749\n",
      "Epoch 19, Batch 145/157, Loss: 2.7061\n",
      "Epoch 19, Batch 146/157, Loss: 2.6643\n",
      "Epoch 19, Batch 147/157, Loss: 2.7607\n",
      "Epoch 19, Batch 148/157, Loss: 2.7983\n",
      "Epoch 19, Batch 149/157, Loss: 2.7703\n",
      "Epoch 19, Batch 150/157, Loss: 2.6871\n",
      "Epoch 19, Batch 151/157, Loss: 2.7089\n",
      "Epoch 19, Batch 152/157, Loss: 2.7270\n",
      "Epoch 19, Batch 153/157, Loss: 2.7431\n",
      "Epoch 19, Batch 154/157, Loss: 2.6703\n",
      "Epoch 19, Batch 155/157, Loss: 2.7785\n",
      "Epoch 19, Batch 156/157, Loss: 2.6656\n",
      "Epoch 19, Batch 157/157, Loss: 2.7371\n",
      "Epoch 19/50, Average Loss: 2.7323\n",
      "Epoch 20, Batch 1/157, Loss: 2.7791\n",
      "Epoch 20, Batch 2/157, Loss: 2.7021\n",
      "Epoch 20, Batch 3/157, Loss: 2.8019\n",
      "Epoch 20, Batch 4/157, Loss: 2.7114\n",
      "Epoch 20, Batch 5/157, Loss: 2.8070\n",
      "Epoch 20, Batch 6/157, Loss: 2.8781\n",
      "Epoch 20, Batch 7/157, Loss: 2.6278\n",
      "Epoch 20, Batch 8/157, Loss: 2.8188\n",
      "Epoch 20, Batch 9/157, Loss: 2.7658\n",
      "Epoch 20, Batch 10/157, Loss: 2.6569\n",
      "Epoch 20, Batch 11/157, Loss: 2.6981\n",
      "Epoch 20, Batch 12/157, Loss: 2.6991\n",
      "Epoch 20, Batch 13/157, Loss: 2.7358\n",
      "Epoch 20, Batch 14/157, Loss: 2.7625\n",
      "Epoch 20, Batch 15/157, Loss: 2.7217\n",
      "Epoch 20, Batch 16/157, Loss: 2.7467\n",
      "Epoch 20, Batch 17/157, Loss: 2.7797\n",
      "Epoch 20, Batch 18/157, Loss: 2.7789\n",
      "Epoch 20, Batch 19/157, Loss: 2.7355\n",
      "Epoch 20, Batch 20/157, Loss: 2.7110\n",
      "Epoch 20, Batch 21/157, Loss: 2.7258\n",
      "Epoch 20, Batch 22/157, Loss: 2.7624\n",
      "Epoch 20, Batch 23/157, Loss: 2.7047\n",
      "Epoch 20, Batch 24/157, Loss: 2.7355\n",
      "Epoch 20, Batch 25/157, Loss: 2.7818\n",
      "Epoch 20, Batch 26/157, Loss: 2.7040\n",
      "Epoch 20, Batch 27/157, Loss: 2.7704\n",
      "Epoch 20, Batch 28/157, Loss: 2.7623\n",
      "Epoch 20, Batch 29/157, Loss: 2.7747\n",
      "Epoch 20, Batch 30/157, Loss: 2.7023\n",
      "Epoch 20, Batch 31/157, Loss: 2.7212\n",
      "Epoch 20, Batch 32/157, Loss: 2.7253\n",
      "Epoch 20, Batch 33/157, Loss: 2.6935\n",
      "Epoch 20, Batch 34/157, Loss: 2.6862\n",
      "Epoch 20, Batch 35/157, Loss: 2.7552\n",
      "Epoch 20, Batch 36/157, Loss: 2.7194\n",
      "Epoch 20, Batch 37/157, Loss: 2.7492\n",
      "Epoch 20, Batch 38/157, Loss: 2.7358\n",
      "Epoch 20, Batch 39/157, Loss: 2.7257\n",
      "Epoch 20, Batch 40/157, Loss: 2.6884\n",
      "Epoch 20, Batch 41/157, Loss: 2.7336\n",
      "Epoch 20, Batch 42/157, Loss: 2.7221\n",
      "Epoch 20, Batch 43/157, Loss: 2.6951\n",
      "Epoch 20, Batch 44/157, Loss: 2.7191\n",
      "Epoch 20, Batch 45/157, Loss: 2.7427\n",
      "Epoch 20, Batch 46/157, Loss: 2.7159\n",
      "Epoch 20, Batch 47/157, Loss: 2.7394\n",
      "Epoch 20, Batch 48/157, Loss: 2.7694\n",
      "Epoch 20, Batch 49/157, Loss: 2.7441\n",
      "Epoch 20, Batch 50/157, Loss: 2.7753\n",
      "Epoch 20, Batch 51/157, Loss: 2.7695\n",
      "Epoch 20, Batch 52/157, Loss: 2.7534\n",
      "Epoch 20, Batch 53/157, Loss: 2.6873\n",
      "Epoch 20, Batch 54/157, Loss: 2.6937\n",
      "Epoch 20, Batch 55/157, Loss: 2.6926\n",
      "Epoch 20, Batch 56/157, Loss: 2.7097\n",
      "Epoch 20, Batch 57/157, Loss: 2.6963\n",
      "Epoch 20, Batch 58/157, Loss: 2.7950\n",
      "Epoch 20, Batch 59/157, Loss: 2.7517\n",
      "Epoch 20, Batch 60/157, Loss: 2.7059\n",
      "Epoch 20, Batch 61/157, Loss: 2.6830\n",
      "Epoch 20, Batch 62/157, Loss: 2.7463\n",
      "Epoch 20, Batch 63/157, Loss: 2.7505\n",
      "Epoch 20, Batch 64/157, Loss: 2.7816\n",
      "Epoch 20, Batch 65/157, Loss: 2.7601\n",
      "Epoch 20, Batch 66/157, Loss: 2.6936\n",
      "Epoch 20, Batch 67/157, Loss: 2.7545\n",
      "Epoch 20, Batch 68/157, Loss: 2.7006\n",
      "Epoch 20, Batch 69/157, Loss: 2.7510\n",
      "Epoch 20, Batch 70/157, Loss: 2.6933\n",
      "Epoch 20, Batch 71/157, Loss: 2.7150\n",
      "Epoch 20, Batch 72/157, Loss: 2.7289\n",
      "Epoch 20, Batch 73/157, Loss: 2.7070\n",
      "Epoch 20, Batch 74/157, Loss: 2.7340\n",
      "Epoch 20, Batch 75/157, Loss: 2.7252\n",
      "Epoch 20, Batch 76/157, Loss: 2.7155\n",
      "Epoch 20, Batch 77/157, Loss: 2.8009\n",
      "Epoch 20, Batch 78/157, Loss: 2.7312\n",
      "Epoch 20, Batch 79/157, Loss: 2.7426\n",
      "Epoch 20, Batch 80/157, Loss: 2.7320\n",
      "Epoch 20, Batch 81/157, Loss: 2.6778\n",
      "Epoch 20, Batch 82/157, Loss: 2.7538\n",
      "Epoch 20, Batch 83/157, Loss: 2.7300\n",
      "Epoch 20, Batch 84/157, Loss: 2.7038\n",
      "Epoch 20, Batch 85/157, Loss: 2.6892\n",
      "Epoch 20, Batch 86/157, Loss: 2.6624\n",
      "Epoch 20, Batch 87/157, Loss: 2.7185\n",
      "Epoch 20, Batch 88/157, Loss: 2.6874\n",
      "Epoch 20, Batch 89/157, Loss: 2.8362\n",
      "Epoch 20, Batch 90/157, Loss: 2.7444\n",
      "Epoch 20, Batch 91/157, Loss: 2.7037\n",
      "Epoch 20, Batch 92/157, Loss: 2.7144\n",
      "Epoch 20, Batch 93/157, Loss: 2.7875\n",
      "Epoch 20, Batch 94/157, Loss: 2.7490\n",
      "Epoch 20, Batch 95/157, Loss: 2.7222\n",
      "Epoch 20, Batch 96/157, Loss: 2.6927\n",
      "Epoch 20, Batch 97/157, Loss: 2.7963\n",
      "Epoch 20, Batch 98/157, Loss: 2.7122\n",
      "Epoch 20, Batch 99/157, Loss: 2.6707\n",
      "Epoch 20, Batch 100/157, Loss: 2.7503\n",
      "Epoch 20, Batch 101/157, Loss: 2.7458\n",
      "Epoch 20, Batch 102/157, Loss: 2.8150\n",
      "Epoch 20, Batch 103/157, Loss: 2.8004\n",
      "Epoch 20, Batch 104/157, Loss: 2.7299\n",
      "Epoch 20, Batch 105/157, Loss: 2.7418\n",
      "Epoch 20, Batch 106/157, Loss: 2.7734\n",
      "Epoch 20, Batch 107/157, Loss: 2.7304\n",
      "Epoch 20, Batch 108/157, Loss: 2.6990\n",
      "Epoch 20, Batch 109/157, Loss: 2.6550\n",
      "Epoch 20, Batch 110/157, Loss: 2.7217\n",
      "Epoch 20, Batch 111/157, Loss: 2.6851\n",
      "Epoch 20, Batch 112/157, Loss: 2.6765\n",
      "Epoch 20, Batch 113/157, Loss: 2.6973\n",
      "Epoch 20, Batch 114/157, Loss: 2.7324\n",
      "Epoch 20, Batch 115/157, Loss: 2.7236\n",
      "Epoch 20, Batch 116/157, Loss: 2.7674\n",
      "Epoch 20, Batch 117/157, Loss: 2.8038\n",
      "Epoch 20, Batch 118/157, Loss: 2.6273\n",
      "Epoch 20, Batch 119/157, Loss: 2.7639\n",
      "Epoch 20, Batch 120/157, Loss: 2.6742\n",
      "Epoch 20, Batch 121/157, Loss: 2.7601\n",
      "Epoch 20, Batch 122/157, Loss: 2.8072\n",
      "Epoch 20, Batch 123/157, Loss: 2.7624\n",
      "Epoch 20, Batch 124/157, Loss: 2.7253\n",
      "Epoch 20, Batch 125/157, Loss: 2.7867\n",
      "Epoch 20, Batch 126/157, Loss: 2.7362\n",
      "Epoch 20, Batch 127/157, Loss: 2.7078\n",
      "Epoch 20, Batch 128/157, Loss: 2.7317\n",
      "Epoch 20, Batch 129/157, Loss: 2.7459\n",
      "Epoch 20, Batch 130/157, Loss: 2.7116\n",
      "Epoch 20, Batch 131/157, Loss: 2.7341\n",
      "Epoch 20, Batch 132/157, Loss: 2.7236\n",
      "Epoch 20, Batch 133/157, Loss: 2.7358\n",
      "Epoch 20, Batch 134/157, Loss: 2.7614\n",
      "Epoch 20, Batch 135/157, Loss: 2.7625\n",
      "Epoch 20, Batch 136/157, Loss: 2.7457\n",
      "Epoch 20, Batch 137/157, Loss: 2.7357\n",
      "Epoch 20, Batch 138/157, Loss: 2.7540\n",
      "Epoch 20, Batch 139/157, Loss: 2.6929\n",
      "Epoch 20, Batch 140/157, Loss: 2.7206\n",
      "Epoch 20, Batch 141/157, Loss: 2.7066\n",
      "Epoch 20, Batch 142/157, Loss: 2.7116\n",
      "Epoch 20, Batch 143/157, Loss: 2.7213\n",
      "Epoch 20, Batch 144/157, Loss: 2.7445\n",
      "Epoch 20, Batch 145/157, Loss: 2.7128\n",
      "Epoch 20, Batch 146/157, Loss: 2.7842\n",
      "Epoch 20, Batch 147/157, Loss: 2.7077\n",
      "Epoch 20, Batch 148/157, Loss: 2.7608\n",
      "Epoch 20, Batch 149/157, Loss: 2.7384\n",
      "Epoch 20, Batch 150/157, Loss: 2.7125\n",
      "Epoch 20, Batch 151/157, Loss: 2.7237\n",
      "Epoch 20, Batch 152/157, Loss: 2.6991\n",
      "Epoch 20, Batch 153/157, Loss: 2.7271\n",
      "Epoch 20, Batch 154/157, Loss: 2.7316\n",
      "Epoch 20, Batch 155/157, Loss: 2.6706\n",
      "Epoch 20, Batch 156/157, Loss: 2.7617\n",
      "Epoch 20, Batch 157/157, Loss: 2.6876\n",
      "Epoch 20/50, Average Loss: 2.7325\n",
      "Epoch 21, Batch 1/157, Loss: 2.7161\n",
      "Epoch 21, Batch 2/157, Loss: 2.7057\n",
      "Epoch 21, Batch 3/157, Loss: 2.6995\n",
      "Epoch 21, Batch 4/157, Loss: 2.7625\n",
      "Epoch 21, Batch 5/157, Loss: 2.7350\n",
      "Epoch 21, Batch 6/157, Loss: 2.7458\n",
      "Epoch 21, Batch 7/157, Loss: 2.7682\n",
      "Epoch 21, Batch 8/157, Loss: 2.6679\n",
      "Epoch 21, Batch 9/157, Loss: 2.7310\n",
      "Epoch 21, Batch 10/157, Loss: 2.7559\n",
      "Epoch 21, Batch 11/157, Loss: 2.7451\n",
      "Epoch 21, Batch 12/157, Loss: 2.7754\n",
      "Epoch 21, Batch 13/157, Loss: 2.7452\n",
      "Epoch 21, Batch 14/157, Loss: 2.7534\n",
      "Epoch 21, Batch 15/157, Loss: 2.7436\n",
      "Epoch 21, Batch 16/157, Loss: 2.7227\n",
      "Epoch 21, Batch 17/157, Loss: 2.6719\n",
      "Epoch 21, Batch 18/157, Loss: 2.7548\n",
      "Epoch 21, Batch 19/157, Loss: 2.7289\n",
      "Epoch 21, Batch 20/157, Loss: 2.7319\n",
      "Epoch 21, Batch 21/157, Loss: 2.7432\n",
      "Epoch 21, Batch 22/157, Loss: 2.7481\n",
      "Epoch 21, Batch 23/157, Loss: 2.7472\n",
      "Epoch 21, Batch 24/157, Loss: 2.7356\n",
      "Epoch 21, Batch 25/157, Loss: 2.6864\n",
      "Epoch 21, Batch 26/157, Loss: 2.6856\n",
      "Epoch 21, Batch 27/157, Loss: 2.6927\n",
      "Epoch 21, Batch 28/157, Loss: 2.6517\n",
      "Epoch 21, Batch 29/157, Loss: 2.7288\n",
      "Epoch 21, Batch 30/157, Loss: 2.6683\n",
      "Epoch 21, Batch 31/157, Loss: 2.7890\n",
      "Epoch 21, Batch 32/157, Loss: 2.6984\n",
      "Epoch 21, Batch 33/157, Loss: 2.7043\n",
      "Epoch 21, Batch 34/157, Loss: 2.7717\n",
      "Epoch 21, Batch 35/157, Loss: 2.7164\n",
      "Epoch 21, Batch 36/157, Loss: 2.7941\n",
      "Epoch 21, Batch 37/157, Loss: 2.7078\n",
      "Epoch 21, Batch 38/157, Loss: 2.7132\n",
      "Epoch 21, Batch 39/157, Loss: 2.8457\n",
      "Epoch 21, Batch 40/157, Loss: 2.7908\n",
      "Epoch 21, Batch 41/157, Loss: 2.8774\n",
      "Epoch 21, Batch 42/157, Loss: 2.7499\n",
      "Epoch 21, Batch 43/157, Loss: 2.7109\n",
      "Epoch 21, Batch 44/157, Loss: 2.6847\n",
      "Epoch 21, Batch 45/157, Loss: 2.7224\n",
      "Epoch 21, Batch 46/157, Loss: 2.7434\n",
      "Epoch 21, Batch 47/157, Loss: 2.7323\n",
      "Epoch 21, Batch 48/157, Loss: 2.7001\n",
      "Epoch 21, Batch 49/157, Loss: 2.7251\n",
      "Epoch 21, Batch 50/157, Loss: 2.7502\n",
      "Epoch 21, Batch 51/157, Loss: 2.6965\n",
      "Epoch 21, Batch 52/157, Loss: 2.7195\n",
      "Epoch 21, Batch 53/157, Loss: 2.7944\n",
      "Epoch 21, Batch 54/157, Loss: 2.6686\n",
      "Epoch 21, Batch 55/157, Loss: 2.6570\n",
      "Epoch 21, Batch 56/157, Loss: 2.7329\n",
      "Epoch 21, Batch 57/157, Loss: 2.6855\n",
      "Epoch 21, Batch 58/157, Loss: 2.7931\n",
      "Epoch 21, Batch 59/157, Loss: 2.7121\n",
      "Epoch 21, Batch 60/157, Loss: 2.6417\n",
      "Epoch 21, Batch 61/157, Loss: 2.8615\n",
      "Epoch 21, Batch 62/157, Loss: 2.7666\n",
      "Epoch 21, Batch 63/157, Loss: 2.7398\n",
      "Epoch 21, Batch 64/157, Loss: 2.7333\n",
      "Epoch 21, Batch 65/157, Loss: 2.8291\n",
      "Epoch 21, Batch 66/157, Loss: 2.7030\n",
      "Epoch 21, Batch 67/157, Loss: 2.7373\n",
      "Epoch 21, Batch 68/157, Loss: 2.7174\n",
      "Epoch 21, Batch 69/157, Loss: 2.6952\n",
      "Epoch 21, Batch 70/157, Loss: 2.6721\n",
      "Epoch 21, Batch 71/157, Loss: 2.6902\n",
      "Epoch 21, Batch 72/157, Loss: 2.7288\n",
      "Epoch 21, Batch 73/157, Loss: 2.7235\n",
      "Epoch 21, Batch 74/157, Loss: 2.7528\n",
      "Epoch 21, Batch 75/157, Loss: 2.7266\n",
      "Epoch 21, Batch 76/157, Loss: 2.7725\n",
      "Epoch 21, Batch 77/157, Loss: 2.7432\n",
      "Epoch 21, Batch 78/157, Loss: 2.7132\n",
      "Epoch 21, Batch 79/157, Loss: 2.7025\n",
      "Epoch 21, Batch 80/157, Loss: 2.7347\n",
      "Epoch 21, Batch 81/157, Loss: 2.8100\n",
      "Epoch 21, Batch 82/157, Loss: 2.7592\n",
      "Epoch 21, Batch 83/157, Loss: 2.7524\n",
      "Epoch 21, Batch 84/157, Loss: 2.7416\n",
      "Epoch 21, Batch 85/157, Loss: 2.7314\n",
      "Epoch 21, Batch 86/157, Loss: 2.7327\n",
      "Epoch 21, Batch 87/157, Loss: 2.7193\n",
      "Epoch 21, Batch 88/157, Loss: 2.7001\n",
      "Epoch 21, Batch 89/157, Loss: 2.7352\n",
      "Epoch 21, Batch 90/157, Loss: 2.7137\n",
      "Epoch 21, Batch 91/157, Loss: 2.7154\n",
      "Epoch 21, Batch 92/157, Loss: 2.7446\n",
      "Epoch 21, Batch 93/157, Loss: 2.7538\n",
      "Epoch 21, Batch 94/157, Loss: 2.6575\n",
      "Epoch 21, Batch 95/157, Loss: 2.7447\n",
      "Epoch 21, Batch 96/157, Loss: 2.7395\n",
      "Epoch 21, Batch 97/157, Loss: 2.7205\n",
      "Epoch 21, Batch 98/157, Loss: 2.7123\n",
      "Epoch 21, Batch 99/157, Loss: 2.6983\n",
      "Epoch 21, Batch 100/157, Loss: 2.6823\n",
      "Epoch 21, Batch 101/157, Loss: 2.7348\n",
      "Epoch 21, Batch 102/157, Loss: 2.7521\n",
      "Epoch 21, Batch 103/157, Loss: 2.7142\n",
      "Epoch 21, Batch 104/157, Loss: 2.6837\n",
      "Epoch 21, Batch 105/157, Loss: 2.7704\n",
      "Epoch 21, Batch 106/157, Loss: 2.6647\n",
      "Epoch 21, Batch 107/157, Loss: 2.7192\n",
      "Epoch 21, Batch 108/157, Loss: 2.7209\n",
      "Epoch 21, Batch 109/157, Loss: 2.7452\n",
      "Epoch 21, Batch 110/157, Loss: 2.6917\n",
      "Epoch 21, Batch 111/157, Loss: 2.7747\n",
      "Epoch 21, Batch 112/157, Loss: 2.7405\n",
      "Epoch 21, Batch 113/157, Loss: 2.7570\n",
      "Epoch 21, Batch 114/157, Loss: 2.7575\n",
      "Epoch 21, Batch 115/157, Loss: 2.7125\n",
      "Epoch 21, Batch 116/157, Loss: 2.7639\n",
      "Epoch 21, Batch 117/157, Loss: 2.7904\n",
      "Epoch 21, Batch 118/157, Loss: 2.7542\n",
      "Epoch 21, Batch 119/157, Loss: 2.7049\n",
      "Epoch 21, Batch 120/157, Loss: 2.7468\n",
      "Epoch 21, Batch 121/157, Loss: 2.7750\n",
      "Epoch 21, Batch 122/157, Loss: 2.7337\n",
      "Epoch 21, Batch 123/157, Loss: 2.7372\n",
      "Epoch 21, Batch 124/157, Loss: 2.6785\n",
      "Epoch 21, Batch 125/157, Loss: 2.6817\n",
      "Epoch 21, Batch 126/157, Loss: 2.7102\n",
      "Epoch 21, Batch 127/157, Loss: 2.7155\n",
      "Epoch 21, Batch 128/157, Loss: 2.7079\n",
      "Epoch 21, Batch 129/157, Loss: 2.7539\n",
      "Epoch 21, Batch 130/157, Loss: 2.7118\n",
      "Epoch 21, Batch 131/157, Loss: 2.7285\n",
      "Epoch 21, Batch 132/157, Loss: 2.7810\n",
      "Epoch 21, Batch 133/157, Loss: 2.7306\n",
      "Epoch 21, Batch 134/157, Loss: 2.7039\n",
      "Epoch 21, Batch 135/157, Loss: 2.7071\n",
      "Epoch 21, Batch 136/157, Loss: 2.7369\n",
      "Epoch 21, Batch 137/157, Loss: 2.6936\n",
      "Epoch 21, Batch 138/157, Loss: 2.7042\n",
      "Epoch 21, Batch 139/157, Loss: 2.7278\n",
      "Epoch 21, Batch 140/157, Loss: 2.7118\n",
      "Epoch 21, Batch 141/157, Loss: 2.7090\n",
      "Epoch 21, Batch 142/157, Loss: 2.7244\n",
      "Epoch 21, Batch 143/157, Loss: 2.6897\n",
      "Epoch 21, Batch 144/157, Loss: 2.6804\n",
      "Epoch 21, Batch 145/157, Loss: 2.7767\n",
      "Epoch 21, Batch 146/157, Loss: 2.6989\n",
      "Epoch 21, Batch 147/157, Loss: 2.7107\n",
      "Epoch 21, Batch 148/157, Loss: 2.7371\n",
      "Epoch 21, Batch 149/157, Loss: 2.8313\n",
      "Epoch 21, Batch 150/157, Loss: 2.7821\n",
      "Epoch 21, Batch 151/157, Loss: 2.7139\n",
      "Epoch 21, Batch 152/157, Loss: 2.7293\n",
      "Epoch 21, Batch 153/157, Loss: 2.7396\n",
      "Epoch 21, Batch 154/157, Loss: 2.6701\n",
      "Epoch 21, Batch 155/157, Loss: 2.7599\n",
      "Epoch 21, Batch 156/157, Loss: 2.7278\n",
      "Epoch 21, Batch 157/157, Loss: 2.6822\n",
      "Epoch 21/50, Average Loss: 2.7298\n",
      "Epoch 22, Batch 1/157, Loss: 2.7651\n",
      "Epoch 22, Batch 2/157, Loss: 2.6781\n",
      "Epoch 22, Batch 3/157, Loss: 2.7626\n",
      "Epoch 22, Batch 4/157, Loss: 2.6870\n",
      "Epoch 22, Batch 5/157, Loss: 2.7932\n",
      "Epoch 22, Batch 6/157, Loss: 2.7211\n",
      "Epoch 22, Batch 7/157, Loss: 2.7390\n",
      "Epoch 22, Batch 8/157, Loss: 2.6509\n",
      "Epoch 22, Batch 9/157, Loss: 2.7088\n",
      "Epoch 22, Batch 10/157, Loss: 2.7437\n",
      "Epoch 22, Batch 11/157, Loss: 2.7708\n",
      "Epoch 22, Batch 12/157, Loss: 2.7133\n",
      "Epoch 22, Batch 13/157, Loss: 2.7306\n",
      "Epoch 22, Batch 14/157, Loss: 2.7356\n",
      "Epoch 22, Batch 15/157, Loss: 2.7350\n",
      "Epoch 22, Batch 16/157, Loss: 2.7387\n",
      "Epoch 22, Batch 17/157, Loss: 2.6985\n",
      "Epoch 22, Batch 18/157, Loss: 2.7273\n",
      "Epoch 22, Batch 19/157, Loss: 2.7410\n",
      "Epoch 22, Batch 20/157, Loss: 2.7261\n",
      "Epoch 22, Batch 21/157, Loss: 2.7400\n",
      "Epoch 22, Batch 22/157, Loss: 2.7163\n",
      "Epoch 22, Batch 23/157, Loss: 2.6971\n",
      "Epoch 22, Batch 24/157, Loss: 2.7589\n",
      "Epoch 22, Batch 25/157, Loss: 2.7744\n",
      "Epoch 22, Batch 26/157, Loss: 2.7757\n",
      "Epoch 22, Batch 27/157, Loss: 2.6675\n",
      "Epoch 22, Batch 28/157, Loss: 2.7030\n",
      "Epoch 22, Batch 29/157, Loss: 2.7429\n",
      "Epoch 22, Batch 30/157, Loss: 2.7660\n",
      "Epoch 22, Batch 31/157, Loss: 2.7681\n",
      "Epoch 22, Batch 32/157, Loss: 2.7511\n",
      "Epoch 22, Batch 33/157, Loss: 2.6980\n",
      "Epoch 22, Batch 34/157, Loss: 2.7333\n",
      "Epoch 22, Batch 35/157, Loss: 2.7522\n",
      "Epoch 22, Batch 36/157, Loss: 2.7153\n",
      "Epoch 22, Batch 37/157, Loss: 2.7259\n",
      "Epoch 22, Batch 38/157, Loss: 2.7258\n",
      "Epoch 22, Batch 39/157, Loss: 2.7313\n",
      "Epoch 22, Batch 40/157, Loss: 2.7216\n",
      "Epoch 22, Batch 41/157, Loss: 2.6806\n",
      "Epoch 22, Batch 42/157, Loss: 2.7314\n",
      "Epoch 22, Batch 43/157, Loss: 2.7183\n",
      "Epoch 22, Batch 44/157, Loss: 2.7254\n",
      "Epoch 22, Batch 45/157, Loss: 2.7240\n",
      "Epoch 22, Batch 46/157, Loss: 2.7026\n",
      "Epoch 22, Batch 47/157, Loss: 2.7036\n",
      "Epoch 22, Batch 48/157, Loss: 2.7295\n",
      "Epoch 22, Batch 49/157, Loss: 2.6904\n",
      "Epoch 22, Batch 50/157, Loss: 2.7415\n",
      "Epoch 22, Batch 51/157, Loss: 2.7192\n",
      "Epoch 22, Batch 52/157, Loss: 2.7200\n",
      "Epoch 22, Batch 53/157, Loss: 2.7347\n",
      "Epoch 22, Batch 54/157, Loss: 2.7166\n",
      "Epoch 22, Batch 55/157, Loss: 2.7045\n",
      "Epoch 22, Batch 56/157, Loss: 2.7457\n",
      "Epoch 22, Batch 57/157, Loss: 2.6714\n",
      "Epoch 22, Batch 58/157, Loss: 2.7348\n",
      "Epoch 22, Batch 59/157, Loss: 2.7190\n",
      "Epoch 22, Batch 60/157, Loss: 2.7678\n",
      "Epoch 22, Batch 61/157, Loss: 2.6917\n",
      "Epoch 22, Batch 62/157, Loss: 2.7677\n",
      "Epoch 22, Batch 63/157, Loss: 2.6694\n",
      "Epoch 22, Batch 64/157, Loss: 2.7758\n",
      "Epoch 22, Batch 65/157, Loss: 2.7315\n",
      "Epoch 22, Batch 66/157, Loss: 2.7088\n",
      "Epoch 22, Batch 67/157, Loss: 2.7492\n",
      "Epoch 22, Batch 68/157, Loss: 2.7558\n",
      "Epoch 22, Batch 69/157, Loss: 2.7055\n",
      "Epoch 22, Batch 70/157, Loss: 2.7533\n",
      "Epoch 22, Batch 71/157, Loss: 2.7503\n",
      "Epoch 22, Batch 72/157, Loss: 2.7065\n",
      "Epoch 22, Batch 73/157, Loss: 2.6440\n",
      "Epoch 22, Batch 74/157, Loss: 2.7461\n",
      "Epoch 22, Batch 75/157, Loss: 2.7272\n",
      "Epoch 22, Batch 76/157, Loss: 2.7231\n",
      "Epoch 22, Batch 77/157, Loss: 2.7269\n",
      "Epoch 22, Batch 78/157, Loss: 2.7803\n",
      "Epoch 22, Batch 79/157, Loss: 2.7374\n",
      "Epoch 22, Batch 80/157, Loss: 2.7462\n",
      "Epoch 22, Batch 81/157, Loss: 2.7081\n",
      "Epoch 22, Batch 82/157, Loss: 2.7199\n",
      "Epoch 22, Batch 83/157, Loss: 2.6813\n",
      "Epoch 22, Batch 84/157, Loss: 2.6886\n",
      "Epoch 22, Batch 85/157, Loss: 2.7303\n",
      "Epoch 22, Batch 86/157, Loss: 2.6547\n",
      "Epoch 22, Batch 87/157, Loss: 2.6864\n",
      "Epoch 22, Batch 88/157, Loss: 2.7946\n",
      "Epoch 22, Batch 89/157, Loss: 2.7201\n",
      "Epoch 22, Batch 90/157, Loss: 2.7318\n",
      "Epoch 22, Batch 91/157, Loss: 2.7292\n",
      "Epoch 22, Batch 92/157, Loss: 2.7447\n",
      "Epoch 22, Batch 93/157, Loss: 2.7143\n",
      "Epoch 22, Batch 94/157, Loss: 2.7467\n",
      "Epoch 22, Batch 95/157, Loss: 2.7049\n",
      "Epoch 22, Batch 96/157, Loss: 2.7736\n",
      "Epoch 22, Batch 97/157, Loss: 2.7632\n",
      "Epoch 22, Batch 98/157, Loss: 2.7472\n",
      "Epoch 22, Batch 99/157, Loss: 2.7798\n",
      "Epoch 22, Batch 100/157, Loss: 2.8276\n",
      "Epoch 22, Batch 101/157, Loss: 2.7511\n",
      "Epoch 22, Batch 102/157, Loss: 2.7400\n",
      "Epoch 22, Batch 103/157, Loss: 2.7953\n",
      "Epoch 22, Batch 104/157, Loss: 2.7012\n",
      "Epoch 22, Batch 105/157, Loss: 2.7147\n",
      "Epoch 22, Batch 106/157, Loss: 2.7458\n",
      "Epoch 22, Batch 107/157, Loss: 2.7082\n",
      "Epoch 22, Batch 108/157, Loss: 2.7417\n",
      "Epoch 22, Batch 109/157, Loss: 2.7029\n",
      "Epoch 22, Batch 110/157, Loss: 2.7598\n",
      "Epoch 22, Batch 111/157, Loss: 2.7251\n",
      "Epoch 22, Batch 112/157, Loss: 2.7360\n",
      "Epoch 22, Batch 113/157, Loss: 2.7438\n",
      "Epoch 22, Batch 114/157, Loss: 2.6795\n",
      "Epoch 22, Batch 115/157, Loss: 2.6927\n",
      "Epoch 22, Batch 116/157, Loss: 2.7617\n",
      "Epoch 22, Batch 117/157, Loss: 2.7397\n",
      "Epoch 22, Batch 118/157, Loss: 2.7311\n",
      "Epoch 22, Batch 119/157, Loss: 2.7305\n",
      "Epoch 22, Batch 120/157, Loss: 2.7191\n",
      "Epoch 22, Batch 121/157, Loss: 2.7390\n",
      "Epoch 22, Batch 122/157, Loss: 2.6916\n",
      "Epoch 22, Batch 123/157, Loss: 2.7857\n",
      "Epoch 22, Batch 124/157, Loss: 2.7363\n",
      "Epoch 22, Batch 125/157, Loss: 2.7157\n",
      "Epoch 22, Batch 126/157, Loss: 2.6753\n",
      "Epoch 22, Batch 127/157, Loss: 2.7913\n",
      "Epoch 22, Batch 128/157, Loss: 2.7322\n",
      "Epoch 22, Batch 129/157, Loss: 2.7184\n",
      "Epoch 22, Batch 130/157, Loss: 2.7445\n",
      "Epoch 22, Batch 131/157, Loss: 2.7473\n",
      "Epoch 22, Batch 132/157, Loss: 2.7068\n",
      "Epoch 22, Batch 133/157, Loss: 2.7080\n",
      "Epoch 22, Batch 134/157, Loss: 2.6900\n",
      "Epoch 22, Batch 135/157, Loss: 2.6917\n",
      "Epoch 22, Batch 136/157, Loss: 2.7380\n",
      "Epoch 22, Batch 137/157, Loss: 2.7704\n",
      "Epoch 22, Batch 138/157, Loss: 2.7484\n",
      "Epoch 22, Batch 139/157, Loss: 2.6955\n",
      "Epoch 22, Batch 140/157, Loss: 2.7154\n",
      "Epoch 22, Batch 141/157, Loss: 2.6894\n",
      "Epoch 22, Batch 142/157, Loss: 2.7173\n",
      "Epoch 22, Batch 143/157, Loss: 2.7038\n",
      "Epoch 22, Batch 144/157, Loss: 2.7643\n",
      "Epoch 22, Batch 145/157, Loss: 2.7458\n",
      "Epoch 22, Batch 146/157, Loss: 2.6920\n",
      "Epoch 22, Batch 147/157, Loss: 2.6932\n",
      "Epoch 22, Batch 148/157, Loss: 2.7040\n",
      "Epoch 22, Batch 149/157, Loss: 2.7339\n",
      "Epoch 22, Batch 150/157, Loss: 2.7677\n",
      "Epoch 22, Batch 151/157, Loss: 2.7636\n",
      "Epoch 22, Batch 152/157, Loss: 2.7526\n",
      "Epoch 22, Batch 153/157, Loss: 2.7185\n",
      "Epoch 22, Batch 154/157, Loss: 2.7801\n",
      "Epoch 22, Batch 155/157, Loss: 2.7239\n",
      "Epoch 22, Batch 156/157, Loss: 2.7581\n",
      "Epoch 22, Batch 157/157, Loss: 2.7419\n",
      "Epoch 22/50, Average Loss: 2.7291\n",
      "Epoch 23, Batch 1/157, Loss: 2.7530\n",
      "Epoch 23, Batch 2/157, Loss: 2.7112\n",
      "Epoch 23, Batch 3/157, Loss: 2.7186\n",
      "Epoch 23, Batch 4/157, Loss: 2.7592\n",
      "Epoch 23, Batch 5/157, Loss: 2.7338\n",
      "Epoch 23, Batch 6/157, Loss: 2.6975\n",
      "Epoch 23, Batch 7/157, Loss: 2.7179\n",
      "Epoch 23, Batch 8/157, Loss: 2.8080\n",
      "Epoch 23, Batch 9/157, Loss: 2.7172\n",
      "Epoch 23, Batch 10/157, Loss: 2.7391\n",
      "Epoch 23, Batch 11/157, Loss: 2.7254\n",
      "Epoch 23, Batch 12/157, Loss: 2.7201\n",
      "Epoch 23, Batch 13/157, Loss: 2.7323\n",
      "Epoch 23, Batch 14/157, Loss: 2.7246\n",
      "Epoch 23, Batch 15/157, Loss: 2.8148\n",
      "Epoch 23, Batch 16/157, Loss: 2.8111\n",
      "Epoch 23, Batch 17/157, Loss: 2.7774\n",
      "Epoch 23, Batch 18/157, Loss: 2.8008\n",
      "Epoch 23, Batch 19/157, Loss: 2.7647\n",
      "Epoch 23, Batch 20/157, Loss: 2.7634\n",
      "Epoch 23, Batch 21/157, Loss: 2.7044\n",
      "Epoch 23, Batch 22/157, Loss: 2.7364\n",
      "Epoch 23, Batch 23/157, Loss: 2.7307\n",
      "Epoch 23, Batch 24/157, Loss: 2.7327\n",
      "Epoch 23, Batch 25/157, Loss: 2.6804\n",
      "Epoch 23, Batch 26/157, Loss: 2.7250\n",
      "Epoch 23, Batch 27/157, Loss: 2.7294\n",
      "Epoch 23, Batch 28/157, Loss: 2.6883\n",
      "Epoch 23, Batch 29/157, Loss: 2.6951\n",
      "Epoch 23, Batch 30/157, Loss: 2.7246\n",
      "Epoch 23, Batch 31/157, Loss: 2.6791\n",
      "Epoch 23, Batch 32/157, Loss: 2.6931\n",
      "Epoch 23, Batch 33/157, Loss: 2.7586\n",
      "Epoch 23, Batch 34/157, Loss: 2.7249\n",
      "Epoch 23, Batch 35/157, Loss: 2.6925\n",
      "Epoch 23, Batch 36/157, Loss: 2.7044\n",
      "Epoch 23, Batch 37/157, Loss: 2.7868\n",
      "Epoch 23, Batch 38/157, Loss: 2.7141\n",
      "Epoch 23, Batch 39/157, Loss: 2.6913\n",
      "Epoch 23, Batch 40/157, Loss: 2.7107\n",
      "Epoch 23, Batch 41/157, Loss: 2.6949\n",
      "Epoch 23, Batch 42/157, Loss: 2.7148\n",
      "Epoch 23, Batch 43/157, Loss: 2.8767\n",
      "Epoch 23, Batch 44/157, Loss: 2.7071\n",
      "Epoch 23, Batch 45/157, Loss: 2.7876\n",
      "Epoch 23, Batch 46/157, Loss: 2.7155\n",
      "Epoch 23, Batch 47/157, Loss: 2.7465\n",
      "Epoch 23, Batch 48/157, Loss: 2.8202\n",
      "Epoch 23, Batch 49/157, Loss: 2.7562\n",
      "Epoch 23, Batch 50/157, Loss: 2.7536\n",
      "Epoch 23, Batch 51/157, Loss: 2.7833\n",
      "Epoch 23, Batch 52/157, Loss: 2.7286\n",
      "Epoch 23, Batch 53/157, Loss: 2.7670\n",
      "Epoch 23, Batch 54/157, Loss: 2.7567\n",
      "Epoch 23, Batch 55/157, Loss: 2.7385\n",
      "Epoch 23, Batch 56/157, Loss: 2.7295\n",
      "Epoch 23, Batch 57/157, Loss: 2.7144\n",
      "Epoch 23, Batch 58/157, Loss: 2.7926\n",
      "Epoch 23, Batch 59/157, Loss: 2.7581\n",
      "Epoch 23, Batch 60/157, Loss: 2.7405\n",
      "Epoch 23, Batch 61/157, Loss: 2.7607\n",
      "Epoch 23, Batch 62/157, Loss: 2.7232\n",
      "Epoch 23, Batch 63/157, Loss: 2.7113\n",
      "Epoch 23, Batch 64/157, Loss: 2.7151\n",
      "Epoch 23, Batch 65/157, Loss: 2.7398\n",
      "Epoch 23, Batch 66/157, Loss: 2.7684\n",
      "Epoch 23, Batch 67/157, Loss: 2.7472\n",
      "Epoch 23, Batch 68/157, Loss: 2.6764\n",
      "Epoch 23, Batch 69/157, Loss: 2.7395\n",
      "Epoch 23, Batch 70/157, Loss: 2.6657\n",
      "Epoch 23, Batch 71/157, Loss: 2.6612\n",
      "Epoch 23, Batch 72/157, Loss: 2.7747\n",
      "Epoch 23, Batch 73/157, Loss: 2.7394\n",
      "Epoch 23, Batch 74/157, Loss: 2.8146\n",
      "Epoch 23, Batch 75/157, Loss: 2.6786\n",
      "Epoch 23, Batch 76/157, Loss: 2.8285\n",
      "Epoch 23, Batch 77/157, Loss: 2.7348\n",
      "Epoch 23, Batch 78/157, Loss: 2.7744\n",
      "Epoch 23, Batch 79/157, Loss: 2.7923\n",
      "Epoch 23, Batch 80/157, Loss: 2.7107\n",
      "Epoch 23, Batch 81/157, Loss: 2.7415\n",
      "Epoch 23, Batch 82/157, Loss: 2.6716\n",
      "Epoch 23, Batch 83/157, Loss: 2.7395\n",
      "Epoch 23, Batch 84/157, Loss: 2.7581\n",
      "Epoch 23, Batch 85/157, Loss: 2.7229\n",
      "Epoch 23, Batch 86/157, Loss: 2.7355\n",
      "Epoch 23, Batch 87/157, Loss: 2.7371\n",
      "Epoch 23, Batch 88/157, Loss: 2.7331\n",
      "Epoch 23, Batch 89/157, Loss: 2.7694\n",
      "Epoch 23, Batch 90/157, Loss: 2.7870\n",
      "Epoch 23, Batch 91/157, Loss: 2.7264\n",
      "Epoch 23, Batch 92/157, Loss: 2.7708\n",
      "Epoch 23, Batch 93/157, Loss: 2.7644\n",
      "Epoch 23, Batch 94/157, Loss: 2.7469\n",
      "Epoch 23, Batch 95/157, Loss: 2.7268\n",
      "Epoch 23, Batch 96/157, Loss: 2.7168\n",
      "Epoch 23, Batch 97/157, Loss: 2.7494\n",
      "Epoch 23, Batch 98/157, Loss: 2.7637\n",
      "Epoch 23, Batch 99/157, Loss: 2.7332\n",
      "Epoch 23, Batch 100/157, Loss: 2.7379\n",
      "Epoch 23, Batch 101/157, Loss: 2.7462\n",
      "Epoch 23, Batch 102/157, Loss: 2.7485\n",
      "Epoch 23, Batch 103/157, Loss: 2.7766\n",
      "Epoch 23, Batch 104/157, Loss: 2.6902\n",
      "Epoch 23, Batch 105/157, Loss: 2.7392\n",
      "Epoch 23, Batch 106/157, Loss: 2.7523\n",
      "Epoch 23, Batch 107/157, Loss: 2.8051\n",
      "Epoch 23, Batch 108/157, Loss: 2.7357\n",
      "Epoch 23, Batch 109/157, Loss: 2.7602\n",
      "Epoch 23, Batch 110/157, Loss: 2.7407\n",
      "Epoch 23, Batch 111/157, Loss: 2.7131\n",
      "Epoch 23, Batch 112/157, Loss: 2.7334\n",
      "Epoch 23, Batch 113/157, Loss: 2.7356\n",
      "Epoch 23, Batch 114/157, Loss: 2.7398\n",
      "Epoch 23, Batch 115/157, Loss: 2.6804\n",
      "Epoch 23, Batch 116/157, Loss: 2.6821\n",
      "Epoch 23, Batch 117/157, Loss: 2.7172\n",
      "Epoch 23, Batch 118/157, Loss: 2.6638\n",
      "Epoch 23, Batch 119/157, Loss: 2.7555\n",
      "Epoch 23, Batch 120/157, Loss: 2.7479\n",
      "Epoch 23, Batch 121/157, Loss: 2.7355\n",
      "Epoch 23, Batch 122/157, Loss: 2.7035\n",
      "Epoch 23, Batch 123/157, Loss: 2.6650\n",
      "Epoch 23, Batch 124/157, Loss: 2.7256\n",
      "Epoch 23, Batch 125/157, Loss: 2.7225\n",
      "Epoch 23, Batch 126/157, Loss: 2.7252\n",
      "Epoch 23, Batch 127/157, Loss: 2.7144\n",
      "Epoch 23, Batch 128/157, Loss: 2.7863\n",
      "Epoch 23, Batch 129/157, Loss: 2.8117\n",
      "Epoch 23, Batch 130/157, Loss: 2.7138\n",
      "Epoch 23, Batch 131/157, Loss: 2.7412\n",
      "Epoch 23, Batch 132/157, Loss: 2.6459\n",
      "Epoch 23, Batch 133/157, Loss: 2.6586\n",
      "Epoch 23, Batch 134/157, Loss: 2.7375\n",
      "Epoch 23, Batch 135/157, Loss: 2.7486\n",
      "Epoch 23, Batch 136/157, Loss: 2.6361\n",
      "Epoch 23, Batch 137/157, Loss: 2.7104\n",
      "Epoch 23, Batch 138/157, Loss: 2.7837\n",
      "Epoch 23, Batch 139/157, Loss: 2.7521\n",
      "Epoch 23, Batch 140/157, Loss: 2.7637\n",
      "Epoch 23, Batch 141/157, Loss: 2.6725\n",
      "Epoch 23, Batch 142/157, Loss: 2.7492\n",
      "Epoch 23, Batch 143/157, Loss: 2.6913\n",
      "Epoch 23, Batch 144/157, Loss: 2.8106\n",
      "Epoch 23, Batch 145/157, Loss: 2.7584\n",
      "Epoch 23, Batch 146/157, Loss: 2.7377\n",
      "Epoch 23, Batch 147/157, Loss: 2.7707\n",
      "Epoch 23, Batch 148/157, Loss: 2.7170\n",
      "Epoch 23, Batch 149/157, Loss: 2.7120\n",
      "Epoch 23, Batch 150/157, Loss: 2.6993\n",
      "Epoch 23, Batch 151/157, Loss: 2.7512\n",
      "Epoch 23, Batch 152/157, Loss: 2.7475\n",
      "Epoch 23, Batch 153/157, Loss: 2.7342\n",
      "Epoch 23, Batch 154/157, Loss: 2.7976\n",
      "Epoch 23, Batch 155/157, Loss: 2.8164\n",
      "Epoch 23, Batch 156/157, Loss: 2.7983\n",
      "Epoch 23, Batch 157/157, Loss: 2.9182\n",
      "Epoch 23/50, Average Loss: 2.7388\n",
      "Epoch 24, Batch 1/157, Loss: 2.8153\n",
      "Epoch 24, Batch 2/157, Loss: 2.7139\n",
      "Epoch 24, Batch 3/157, Loss: 2.7282\n",
      "Epoch 24, Batch 4/157, Loss: 2.8012\n",
      "Epoch 24, Batch 5/157, Loss: 2.7478\n",
      "Epoch 24, Batch 6/157, Loss: 2.7188\n",
      "Epoch 24, Batch 7/157, Loss: 2.7313\n",
      "Epoch 24, Batch 8/157, Loss: 2.6686\n",
      "Epoch 24, Batch 9/157, Loss: 2.7625\n",
      "Epoch 24, Batch 10/157, Loss: 2.7901\n",
      "Epoch 24, Batch 11/157, Loss: 2.7849\n",
      "Epoch 24, Batch 12/157, Loss: 2.8011\n",
      "Epoch 24, Batch 13/157, Loss: 2.6596\n",
      "Epoch 24, Batch 14/157, Loss: 2.6869\n",
      "Epoch 24, Batch 15/157, Loss: 2.7438\n",
      "Epoch 24, Batch 16/157, Loss: 2.7150\n",
      "Epoch 24, Batch 17/157, Loss: 2.7058\n",
      "Epoch 24, Batch 18/157, Loss: 2.6534\n",
      "Epoch 24, Batch 19/157, Loss: 2.7717\n",
      "Epoch 24, Batch 20/157, Loss: 2.7383\n",
      "Epoch 24, Batch 21/157, Loss: 2.7364\n",
      "Epoch 24, Batch 22/157, Loss: 2.7411\n",
      "Epoch 24, Batch 23/157, Loss: 2.7505\n",
      "Epoch 24, Batch 24/157, Loss: 2.6770\n",
      "Epoch 24, Batch 25/157, Loss: 2.7184\n",
      "Epoch 24, Batch 26/157, Loss: 2.7695\n",
      "Epoch 24, Batch 27/157, Loss: 2.7729\n",
      "Epoch 24, Batch 28/157, Loss: 2.7433\n",
      "Epoch 24, Batch 29/157, Loss: 2.7467\n",
      "Epoch 24, Batch 30/157, Loss: 2.6840\n",
      "Epoch 24, Batch 31/157, Loss: 2.7723\n",
      "Epoch 24, Batch 32/157, Loss: 2.7621\n",
      "Epoch 24, Batch 33/157, Loss: 2.7735\n",
      "Epoch 24, Batch 34/157, Loss: 2.6712\n",
      "Epoch 24, Batch 35/157, Loss: 2.8052\n",
      "Epoch 24, Batch 36/157, Loss: 2.6993\n",
      "Epoch 24, Batch 37/157, Loss: 2.6371\n",
      "Epoch 24, Batch 38/157, Loss: 2.7484\n",
      "Epoch 24, Batch 39/157, Loss: 2.7088\n",
      "Epoch 24, Batch 40/157, Loss: 2.8047\n",
      "Epoch 24, Batch 41/157, Loss: 2.7357\n",
      "Epoch 24, Batch 42/157, Loss: 2.7891\n",
      "Epoch 24, Batch 43/157, Loss: 2.7709\n",
      "Epoch 24, Batch 44/157, Loss: 2.7458\n",
      "Epoch 24, Batch 45/157, Loss: 2.7715\n",
      "Epoch 24, Batch 46/157, Loss: 2.7300\n",
      "Epoch 24, Batch 47/157, Loss: 2.6937\n",
      "Epoch 24, Batch 48/157, Loss: 2.7350\n",
      "Epoch 24, Batch 49/157, Loss: 2.7227\n",
      "Epoch 24, Batch 50/157, Loss: 2.7388\n",
      "Epoch 24, Batch 51/157, Loss: 2.7449\n",
      "Epoch 24, Batch 52/157, Loss: 2.7690\n",
      "Epoch 24, Batch 53/157, Loss: 2.6994\n",
      "Epoch 24, Batch 54/157, Loss: 2.7265\n",
      "Epoch 24, Batch 55/157, Loss: 2.7735\n",
      "Epoch 24, Batch 56/157, Loss: 2.7099\n",
      "Epoch 24, Batch 57/157, Loss: 2.7303\n",
      "Epoch 24, Batch 58/157, Loss: 2.6924\n",
      "Epoch 24, Batch 59/157, Loss: 2.7505\n",
      "Epoch 24, Batch 60/157, Loss: 2.6867\n",
      "Epoch 24, Batch 61/157, Loss: 2.7786\n",
      "Epoch 24, Batch 62/157, Loss: 2.7022\n",
      "Epoch 24, Batch 63/157, Loss: 2.7348\n",
      "Epoch 24, Batch 64/157, Loss: 2.7779\n",
      "Epoch 24, Batch 65/157, Loss: 2.7057\n",
      "Epoch 24, Batch 66/157, Loss: 2.6912\n",
      "Epoch 24, Batch 67/157, Loss: 2.6921\n",
      "Epoch 24, Batch 68/157, Loss: 2.7225\n",
      "Epoch 24, Batch 69/157, Loss: 2.7198\n",
      "Epoch 24, Batch 70/157, Loss: 2.7083\n",
      "Epoch 24, Batch 71/157, Loss: 2.7807\n",
      "Epoch 24, Batch 72/157, Loss: 2.7051\n",
      "Epoch 24, Batch 73/157, Loss: 2.7165\n",
      "Epoch 24, Batch 74/157, Loss: 2.6623\n",
      "Epoch 24, Batch 75/157, Loss: 2.7283\n",
      "Epoch 24, Batch 76/157, Loss: 2.7108\n",
      "Epoch 24, Batch 77/157, Loss: 2.7176\n",
      "Epoch 24, Batch 78/157, Loss: 2.7190\n",
      "Epoch 24, Batch 79/157, Loss: 2.6772\n",
      "Epoch 24, Batch 80/157, Loss: 2.6891\n",
      "Epoch 24, Batch 81/157, Loss: 2.8494\n",
      "Epoch 24, Batch 82/157, Loss: 2.8113\n",
      "Epoch 24, Batch 83/157, Loss: 2.6143\n",
      "Epoch 24, Batch 84/157, Loss: 2.7419\n",
      "Epoch 24, Batch 85/157, Loss: 2.7677\n",
      "Epoch 24, Batch 86/157, Loss: 2.7455\n",
      "Epoch 24, Batch 87/157, Loss: 2.7430\n",
      "Epoch 24, Batch 88/157, Loss: 2.8472\n",
      "Epoch 24, Batch 89/157, Loss: 2.7606\n",
      "Epoch 24, Batch 90/157, Loss: 2.7320\n",
      "Epoch 24, Batch 91/157, Loss: 2.6825\n",
      "Epoch 24, Batch 92/157, Loss: 2.7262\n",
      "Epoch 24, Batch 93/157, Loss: 2.6934\n",
      "Epoch 24, Batch 94/157, Loss: 2.7308\n",
      "Epoch 24, Batch 95/157, Loss: 2.7591\n",
      "Epoch 24, Batch 96/157, Loss: 2.7341\n",
      "Epoch 24, Batch 97/157, Loss: 2.6941\n",
      "Epoch 24, Batch 98/157, Loss: 2.7044\n",
      "Epoch 24, Batch 99/157, Loss: 2.7091\n",
      "Epoch 24, Batch 100/157, Loss: 2.7502\n",
      "Epoch 24, Batch 101/157, Loss: 2.8218\n",
      "Epoch 24, Batch 102/157, Loss: 2.7517\n",
      "Epoch 24, Batch 103/157, Loss: 2.7769\n",
      "Epoch 24, Batch 104/157, Loss: 2.7940\n",
      "Epoch 24, Batch 105/157, Loss: 2.7590\n",
      "Epoch 24, Batch 106/157, Loss: 2.7280\n",
      "Epoch 24, Batch 107/157, Loss: 2.7592\n",
      "Epoch 24, Batch 108/157, Loss: 2.7971\n",
      "Epoch 24, Batch 109/157, Loss: 2.7627\n",
      "Epoch 24, Batch 110/157, Loss: 2.7133\n",
      "Epoch 24, Batch 111/157, Loss: 2.7534\n",
      "Epoch 24, Batch 112/157, Loss: 2.6471\n",
      "Epoch 24, Batch 113/157, Loss: 2.7947\n",
      "Epoch 24, Batch 114/157, Loss: 2.7778\n",
      "Epoch 24, Batch 115/157, Loss: 2.7016\n",
      "Epoch 24, Batch 116/157, Loss: 2.7579\n",
      "Epoch 24, Batch 117/157, Loss: 2.7314\n",
      "Epoch 24, Batch 118/157, Loss: 2.7016\n",
      "Epoch 24, Batch 119/157, Loss: 2.7230\n",
      "Epoch 24, Batch 120/157, Loss: 2.7633\n",
      "Epoch 24, Batch 121/157, Loss: 2.7177\n",
      "Epoch 24, Batch 122/157, Loss: 2.7402\n",
      "Epoch 24, Batch 123/157, Loss: 2.7363\n",
      "Epoch 24, Batch 124/157, Loss: 2.7382\n",
      "Epoch 24, Batch 125/157, Loss: 2.7456\n",
      "Epoch 24, Batch 126/157, Loss: 2.7865\n",
      "Epoch 24, Batch 127/157, Loss: 2.7879\n",
      "Epoch 24, Batch 128/157, Loss: 2.7004\n",
      "Epoch 24, Batch 129/157, Loss: 2.7353\n",
      "Epoch 24, Batch 130/157, Loss: 2.7153\n",
      "Epoch 24, Batch 131/157, Loss: 2.7465\n",
      "Epoch 24, Batch 132/157, Loss: 2.7163\n",
      "Epoch 24, Batch 133/157, Loss: 2.7034\n",
      "Epoch 24, Batch 134/157, Loss: 2.7165\n",
      "Epoch 24, Batch 135/157, Loss: 2.7183\n",
      "Epoch 24, Batch 136/157, Loss: 2.7344\n",
      "Epoch 24, Batch 137/157, Loss: 2.7730\n",
      "Epoch 24, Batch 138/157, Loss: 2.7817\n",
      "Epoch 24, Batch 139/157, Loss: 2.7616\n",
      "Epoch 24, Batch 140/157, Loss: 2.7120\n",
      "Epoch 24, Batch 141/157, Loss: 2.7142\n",
      "Epoch 24, Batch 142/157, Loss: 2.7286\n",
      "Epoch 24, Batch 143/157, Loss: 2.7748\n",
      "Epoch 24, Batch 144/157, Loss: 2.7132\n",
      "Epoch 24, Batch 145/157, Loss: 2.7469\n",
      "Epoch 24, Batch 146/157, Loss: 2.6641\n",
      "Epoch 24, Batch 147/157, Loss: 2.7108\n",
      "Epoch 24, Batch 148/157, Loss: 2.7637\n",
      "Epoch 24, Batch 149/157, Loss: 2.6959\n",
      "Epoch 24, Batch 150/157, Loss: 2.7777\n",
      "Epoch 24, Batch 151/157, Loss: 2.7366\n",
      "Epoch 24, Batch 152/157, Loss: 2.6928\n",
      "Epoch 24, Batch 153/157, Loss: 2.6991\n",
      "Epoch 24, Batch 154/157, Loss: 2.7639\n",
      "Epoch 24, Batch 155/157, Loss: 2.7483\n",
      "Epoch 24, Batch 156/157, Loss: 2.7613\n",
      "Epoch 24, Batch 157/157, Loss: 2.8235\n",
      "Epoch 24/50, Average Loss: 2.7364\n",
      "Epoch 25, Batch 1/157, Loss: 2.7294\n",
      "Epoch 25, Batch 2/157, Loss: 2.8095\n",
      "Epoch 25, Batch 3/157, Loss: 2.7300\n",
      "Epoch 25, Batch 4/157, Loss: 2.7978\n",
      "Epoch 25, Batch 5/157, Loss: 2.6688\n",
      "Epoch 25, Batch 6/157, Loss: 2.7546\n",
      "Epoch 25, Batch 7/157, Loss: 2.7617\n",
      "Epoch 25, Batch 8/157, Loss: 2.6748\n",
      "Epoch 25, Batch 9/157, Loss: 2.7351\n",
      "Epoch 25, Batch 10/157, Loss: 2.7240\n",
      "Epoch 25, Batch 11/157, Loss: 2.7234\n",
      "Epoch 25, Batch 12/157, Loss: 2.7485\n",
      "Epoch 25, Batch 13/157, Loss: 2.7978\n",
      "Epoch 25, Batch 14/157, Loss: 2.7056\n",
      "Epoch 25, Batch 15/157, Loss: 2.7244\n",
      "Epoch 25, Batch 16/157, Loss: 2.7891\n",
      "Epoch 25, Batch 17/157, Loss: 2.6895\n",
      "Epoch 25, Batch 18/157, Loss: 2.7266\n",
      "Epoch 25, Batch 19/157, Loss: 2.7549\n",
      "Epoch 25, Batch 20/157, Loss: 2.7166\n",
      "Epoch 25, Batch 21/157, Loss: 2.6973\n",
      "Epoch 25, Batch 22/157, Loss: 2.7186\n",
      "Epoch 25, Batch 23/157, Loss: 2.7410\n",
      "Epoch 25, Batch 24/157, Loss: 2.7222\n",
      "Epoch 25, Batch 25/157, Loss: 2.7165\n",
      "Epoch 25, Batch 26/157, Loss: 2.6986\n",
      "Epoch 25, Batch 27/157, Loss: 2.7493\n",
      "Epoch 25, Batch 28/157, Loss: 2.7095\n",
      "Epoch 25, Batch 29/157, Loss: 2.6900\n",
      "Epoch 25, Batch 30/157, Loss: 2.6981\n",
      "Epoch 25, Batch 31/157, Loss: 2.6846\n",
      "Epoch 25, Batch 32/157, Loss: 2.7648\n",
      "Epoch 25, Batch 33/157, Loss: 2.7336\n",
      "Epoch 25, Batch 34/157, Loss: 2.6344\n",
      "Epoch 25, Batch 35/157, Loss: 2.7453\n",
      "Epoch 25, Batch 36/157, Loss: 2.6759\n",
      "Epoch 25, Batch 37/157, Loss: 2.7237\n",
      "Epoch 25, Batch 38/157, Loss: 2.7014\n",
      "Epoch 25, Batch 39/157, Loss: 2.7893\n",
      "Epoch 25, Batch 40/157, Loss: 2.7737\n",
      "Epoch 25, Batch 41/157, Loss: 2.6464\n",
      "Epoch 25, Batch 42/157, Loss: 2.7888\n",
      "Epoch 25, Batch 43/157, Loss: 2.7696\n",
      "Epoch 25, Batch 44/157, Loss: 2.7607\n",
      "Epoch 25, Batch 45/157, Loss: 2.7205\n",
      "Epoch 25, Batch 46/157, Loss: 2.7265\n",
      "Epoch 25, Batch 47/157, Loss: 2.7094\n",
      "Epoch 25, Batch 48/157, Loss: 2.7050\n",
      "Epoch 25, Batch 49/157, Loss: 2.7059\n",
      "Epoch 25, Batch 50/157, Loss: 2.7092\n",
      "Epoch 25, Batch 51/157, Loss: 2.6761\n",
      "Epoch 25, Batch 52/157, Loss: 2.7308\n",
      "Epoch 25, Batch 53/157, Loss: 2.7687\n",
      "Epoch 25, Batch 54/157, Loss: 2.6860\n",
      "Epoch 25, Batch 55/157, Loss: 2.7265\n",
      "Epoch 25, Batch 56/157, Loss: 2.7123\n",
      "Epoch 25, Batch 57/157, Loss: 2.7193\n",
      "Epoch 25, Batch 58/157, Loss: 2.7089\n",
      "Epoch 25, Batch 59/157, Loss: 2.7417\n",
      "Epoch 25, Batch 60/157, Loss: 2.7301\n",
      "Epoch 25, Batch 61/157, Loss: 2.6989\n",
      "Epoch 25, Batch 62/157, Loss: 2.6795\n",
      "Epoch 25, Batch 63/157, Loss: 2.7413\n",
      "Epoch 25, Batch 64/157, Loss: 2.7396\n",
      "Epoch 25, Batch 65/157, Loss: 2.7492\n",
      "Epoch 25, Batch 66/157, Loss: 2.7205\n",
      "Epoch 25, Batch 67/157, Loss: 2.6578\n",
      "Epoch 25, Batch 68/157, Loss: 2.6800\n",
      "Epoch 25, Batch 69/157, Loss: 2.7128\n",
      "Epoch 25, Batch 70/157, Loss: 2.7713\n",
      "Epoch 25, Batch 71/157, Loss: 2.6965\n",
      "Epoch 25, Batch 72/157, Loss: 2.7218\n",
      "Epoch 25, Batch 73/157, Loss: 2.7870\n",
      "Epoch 25, Batch 74/157, Loss: 2.8277\n",
      "Epoch 25, Batch 75/157, Loss: 2.7828\n",
      "Epoch 25, Batch 76/157, Loss: 2.7532\n",
      "Epoch 25, Batch 77/157, Loss: 2.7582\n",
      "Epoch 25, Batch 78/157, Loss: 2.7582\n",
      "Epoch 25, Batch 79/157, Loss: 2.7241\n",
      "Epoch 25, Batch 80/157, Loss: 2.7547\n",
      "Epoch 25, Batch 81/157, Loss: 2.7599\n",
      "Epoch 25, Batch 82/157, Loss: 2.7570\n",
      "Epoch 25, Batch 83/157, Loss: 2.7212\n",
      "Epoch 25, Batch 84/157, Loss: 2.7273\n",
      "Epoch 25, Batch 85/157, Loss: 2.7149\n",
      "Epoch 25, Batch 86/157, Loss: 2.7208\n",
      "Epoch 25, Batch 87/157, Loss: 2.7122\n",
      "Epoch 25, Batch 88/157, Loss: 2.7143\n",
      "Epoch 25, Batch 89/157, Loss: 2.6841\n",
      "Epoch 25, Batch 90/157, Loss: 2.6850\n",
      "Epoch 25, Batch 91/157, Loss: 2.7240\n",
      "Epoch 25, Batch 92/157, Loss: 2.7143\n",
      "Epoch 25, Batch 93/157, Loss: 2.7027\n",
      "Epoch 25, Batch 94/157, Loss: 2.7383\n",
      "Epoch 25, Batch 95/157, Loss: 2.7208\n",
      "Epoch 25, Batch 96/157, Loss: 2.7147\n",
      "Epoch 25, Batch 97/157, Loss: 2.6952\n",
      "Epoch 25, Batch 98/157, Loss: 2.7952\n",
      "Epoch 25, Batch 99/157, Loss: 2.6766\n",
      "Epoch 25, Batch 100/157, Loss: 2.7044\n",
      "Epoch 25, Batch 101/157, Loss: 2.8235\n",
      "Epoch 25, Batch 102/157, Loss: 2.6931\n",
      "Epoch 25, Batch 103/157, Loss: 2.7426\n",
      "Epoch 25, Batch 104/157, Loss: 2.7522\n",
      "Epoch 25, Batch 105/157, Loss: 2.7091\n",
      "Epoch 25, Batch 106/157, Loss: 2.6869\n",
      "Epoch 25, Batch 107/157, Loss: 2.7627\n",
      "Epoch 25, Batch 108/157, Loss: 2.7964\n",
      "Epoch 25, Batch 109/157, Loss: 2.7205\n",
      "Epoch 25, Batch 110/157, Loss: 2.7548\n",
      "Epoch 25, Batch 111/157, Loss: 2.6372\n",
      "Epoch 25, Batch 112/157, Loss: 2.7013\n",
      "Epoch 25, Batch 113/157, Loss: 2.7552\n",
      "Epoch 25, Batch 114/157, Loss: 2.7726\n",
      "Epoch 25, Batch 115/157, Loss: 2.6806\n",
      "Epoch 25, Batch 116/157, Loss: 2.7401\n",
      "Epoch 25, Batch 117/157, Loss: 2.7672\n",
      "Epoch 25, Batch 118/157, Loss: 2.7580\n",
      "Epoch 25, Batch 119/157, Loss: 2.7220\n",
      "Epoch 25, Batch 120/157, Loss: 2.7045\n",
      "Epoch 25, Batch 121/157, Loss: 2.7442\n",
      "Epoch 25, Batch 122/157, Loss: 2.6986\n",
      "Epoch 25, Batch 123/157, Loss: 2.6841\n",
      "Epoch 25, Batch 124/157, Loss: 2.7228\n",
      "Epoch 25, Batch 125/157, Loss: 2.7472\n",
      "Epoch 25, Batch 126/157, Loss: 2.6683\n",
      "Epoch 25, Batch 127/157, Loss: 2.7343\n",
      "Epoch 25, Batch 128/157, Loss: 2.7186\n",
      "Epoch 25, Batch 129/157, Loss: 2.7336\n",
      "Epoch 25, Batch 130/157, Loss: 2.7390\n",
      "Epoch 25, Batch 131/157, Loss: 2.8015\n",
      "Epoch 25, Batch 132/157, Loss: 2.7781\n",
      "Epoch 25, Batch 133/157, Loss: 2.6900\n",
      "Epoch 25, Batch 134/157, Loss: 2.7664\n",
      "Epoch 25, Batch 135/157, Loss: 2.7486\n",
      "Epoch 25, Batch 136/157, Loss: 2.7196\n",
      "Epoch 25, Batch 137/157, Loss: 2.7561\n",
      "Epoch 25, Batch 138/157, Loss: 2.7263\n",
      "Epoch 25, Batch 139/157, Loss: 2.6849\n",
      "Epoch 25, Batch 140/157, Loss: 2.6900\n",
      "Epoch 25, Batch 141/157, Loss: 2.7262\n",
      "Epoch 25, Batch 142/157, Loss: 2.6802\n",
      "Epoch 25, Batch 143/157, Loss: 2.7403\n",
      "Epoch 25, Batch 144/157, Loss: 2.6616\n",
      "Epoch 25, Batch 145/157, Loss: 2.7808\n",
      "Epoch 25, Batch 146/157, Loss: 2.7444\n",
      "Epoch 25, Batch 147/157, Loss: 2.7542\n",
      "Epoch 25, Batch 148/157, Loss: 2.7541\n",
      "Epoch 25, Batch 149/157, Loss: 2.7488\n",
      "Epoch 25, Batch 150/157, Loss: 2.6865\n",
      "Epoch 25, Batch 151/157, Loss: 2.7286\n",
      "Epoch 25, Batch 152/157, Loss: 2.7128\n",
      "Epoch 25, Batch 153/157, Loss: 2.6965\n",
      "Epoch 25, Batch 154/157, Loss: 2.6352\n",
      "Epoch 25, Batch 155/157, Loss: 2.7467\n",
      "Epoch 25, Batch 156/157, Loss: 2.7246\n",
      "Epoch 25, Batch 157/157, Loss: 2.7304\n",
      "Epoch 25/50, Average Loss: 2.7271\n",
      "Epoch 26, Batch 1/157, Loss: 2.7883\n",
      "Epoch 26, Batch 2/157, Loss: 2.7370\n",
      "Epoch 26, Batch 3/157, Loss: 2.7284\n",
      "Epoch 26, Batch 4/157, Loss: 2.6816\n",
      "Epoch 26, Batch 5/157, Loss: 2.7794\n",
      "Epoch 26, Batch 6/157, Loss: 2.7365\n",
      "Epoch 26, Batch 7/157, Loss: 2.7177\n",
      "Epoch 26, Batch 8/157, Loss: 2.8858\n",
      "Epoch 26, Batch 9/157, Loss: 2.6781\n",
      "Epoch 26, Batch 10/157, Loss: 2.8105\n",
      "Epoch 26, Batch 11/157, Loss: 2.8140\n",
      "Epoch 26, Batch 12/157, Loss: 2.7109\n",
      "Epoch 26, Batch 13/157, Loss: 2.7600\n",
      "Epoch 26, Batch 14/157, Loss: 2.7037\n",
      "Epoch 26, Batch 15/157, Loss: 2.6178\n",
      "Epoch 26, Batch 16/157, Loss: 2.7897\n",
      "Epoch 26, Batch 17/157, Loss: 2.6992\n",
      "Epoch 26, Batch 18/157, Loss: 2.7535\n",
      "Epoch 26, Batch 19/157, Loss: 2.7022\n",
      "Epoch 26, Batch 20/157, Loss: 2.7630\n",
      "Epoch 26, Batch 21/157, Loss: 2.7280\n",
      "Epoch 26, Batch 22/157, Loss: 2.7262\n",
      "Epoch 26, Batch 23/157, Loss: 2.6997\n",
      "Epoch 26, Batch 24/157, Loss: 2.7020\n",
      "Epoch 26, Batch 25/157, Loss: 2.7294\n",
      "Epoch 26, Batch 26/157, Loss: 2.6888\n",
      "Epoch 26, Batch 27/157, Loss: 2.6869\n",
      "Epoch 26, Batch 28/157, Loss: 2.7052\n",
      "Epoch 26, Batch 29/157, Loss: 2.7081\n",
      "Epoch 26, Batch 30/157, Loss: 2.8016\n",
      "Epoch 26, Batch 31/157, Loss: 2.7285\n",
      "Epoch 26, Batch 32/157, Loss: 2.6899\n",
      "Epoch 26, Batch 33/157, Loss: 2.7131\n",
      "Epoch 26, Batch 34/157, Loss: 2.6876\n",
      "Epoch 26, Batch 35/157, Loss: 2.7573\n",
      "Epoch 26, Batch 36/157, Loss: 2.7782\n",
      "Epoch 26, Batch 37/157, Loss: 2.7700\n",
      "Epoch 26, Batch 38/157, Loss: 2.7595\n",
      "Epoch 26, Batch 39/157, Loss: 2.7994\n",
      "Epoch 26, Batch 40/157, Loss: 2.7257\n",
      "Epoch 26, Batch 41/157, Loss: 2.7935\n",
      "Epoch 26, Batch 42/157, Loss: 2.6874\n",
      "Epoch 26, Batch 43/157, Loss: 2.7603\n",
      "Epoch 26, Batch 44/157, Loss: 2.7499\n",
      "Epoch 26, Batch 45/157, Loss: 2.7578\n",
      "Epoch 26, Batch 46/157, Loss: 2.6923\n",
      "Epoch 26, Batch 47/157, Loss: 2.7505\n",
      "Epoch 26, Batch 48/157, Loss: 2.7346\n",
      "Epoch 26, Batch 49/157, Loss: 2.6521\n",
      "Epoch 26, Batch 50/157, Loss: 2.6789\n",
      "Epoch 26, Batch 51/157, Loss: 2.7254\n",
      "Epoch 26, Batch 52/157, Loss: 2.7493\n",
      "Epoch 26, Batch 53/157, Loss: 2.7166\n",
      "Epoch 26, Batch 54/157, Loss: 2.7451\n",
      "Epoch 26, Batch 55/157, Loss: 2.7083\n",
      "Epoch 26, Batch 56/157, Loss: 2.6822\n",
      "Epoch 26, Batch 57/157, Loss: 2.6989\n",
      "Epoch 26, Batch 58/157, Loss: 2.6752\n",
      "Epoch 26, Batch 59/157, Loss: 2.7158\n",
      "Epoch 26, Batch 60/157, Loss: 2.7294\n",
      "Epoch 26, Batch 61/157, Loss: 2.7252\n",
      "Epoch 26, Batch 62/157, Loss: 2.7227\n",
      "Epoch 26, Batch 63/157, Loss: 2.6860\n",
      "Epoch 26, Batch 64/157, Loss: 2.7596\n",
      "Epoch 26, Batch 65/157, Loss: 2.7270\n",
      "Epoch 26, Batch 66/157, Loss: 2.8065\n",
      "Epoch 26, Batch 67/157, Loss: 2.6669\n",
      "Epoch 26, Batch 68/157, Loss: 2.7265\n",
      "Epoch 26, Batch 69/157, Loss: 2.7135\n",
      "Epoch 26, Batch 70/157, Loss: 2.7674\n",
      "Epoch 26, Batch 71/157, Loss: 2.7548\n",
      "Epoch 26, Batch 72/157, Loss: 2.6996\n",
      "Epoch 26, Batch 73/157, Loss: 2.7658\n",
      "Epoch 26, Batch 74/157, Loss: 2.6882\n",
      "Epoch 26, Batch 75/157, Loss: 2.6963\n",
      "Epoch 26, Batch 76/157, Loss: 2.7387\n",
      "Epoch 26, Batch 77/157, Loss: 2.7194\n",
      "Epoch 26, Batch 78/157, Loss: 2.6941\n",
      "Epoch 26, Batch 79/157, Loss: 2.7004\n",
      "Epoch 26, Batch 80/157, Loss: 2.6550\n",
      "Epoch 26, Batch 81/157, Loss: 2.7264\n",
      "Epoch 26, Batch 82/157, Loss: 2.7377\n",
      "Epoch 26, Batch 83/157, Loss: 2.6799\n",
      "Epoch 26, Batch 84/157, Loss: 2.7002\n",
      "Epoch 26, Batch 85/157, Loss: 2.8085\n",
      "Epoch 26, Batch 86/157, Loss: 2.7903\n",
      "Epoch 26, Batch 87/157, Loss: 2.7378\n",
      "Epoch 26, Batch 88/157, Loss: 2.7091\n",
      "Epoch 26, Batch 89/157, Loss: 2.6812\n",
      "Epoch 26, Batch 90/157, Loss: 2.6635\n",
      "Epoch 26, Batch 91/157, Loss: 2.7830\n",
      "Epoch 26, Batch 92/157, Loss: 2.7422\n",
      "Epoch 26, Batch 93/157, Loss: 2.7196\n",
      "Epoch 26, Batch 94/157, Loss: 2.7835\n",
      "Epoch 26, Batch 95/157, Loss: 2.6908\n",
      "Epoch 26, Batch 96/157, Loss: 2.7303\n",
      "Epoch 26, Batch 97/157, Loss: 2.7647\n",
      "Epoch 26, Batch 98/157, Loss: 2.7540\n",
      "Epoch 26, Batch 99/157, Loss: 2.7591\n",
      "Epoch 26, Batch 100/157, Loss: 2.6736\n",
      "Epoch 26, Batch 101/157, Loss: 2.6968\n",
      "Epoch 26, Batch 102/157, Loss: 2.7537\n",
      "Epoch 26, Batch 103/157, Loss: 2.7753\n",
      "Epoch 26, Batch 104/157, Loss: 2.7134\n",
      "Epoch 26, Batch 105/157, Loss: 2.6688\n",
      "Epoch 26, Batch 106/157, Loss: 2.7070\n",
      "Epoch 26, Batch 107/157, Loss: 2.7673\n",
      "Epoch 26, Batch 108/157, Loss: 2.7288\n",
      "Epoch 26, Batch 109/157, Loss: 2.7042\n",
      "Epoch 26, Batch 110/157, Loss: 2.7218\n",
      "Epoch 26, Batch 111/157, Loss: 2.6717\n",
      "Epoch 26, Batch 112/157, Loss: 2.7339\n",
      "Epoch 26, Batch 113/157, Loss: 2.7447\n",
      "Epoch 26, Batch 114/157, Loss: 2.6976\n",
      "Epoch 26, Batch 115/157, Loss: 2.7045\n",
      "Epoch 26, Batch 116/157, Loss: 2.7281\n",
      "Epoch 26, Batch 117/157, Loss: 2.7181\n",
      "Epoch 26, Batch 118/157, Loss: 2.7695\n",
      "Epoch 26, Batch 119/157, Loss: 2.7892\n",
      "Epoch 26, Batch 120/157, Loss: 2.7174\n",
      "Epoch 26, Batch 121/157, Loss: 2.7337\n",
      "Epoch 26, Batch 122/157, Loss: 2.7129\n",
      "Epoch 26, Batch 123/157, Loss: 2.6951\n",
      "Epoch 26, Batch 124/157, Loss: 2.7161\n",
      "Epoch 26, Batch 125/157, Loss: 2.7338\n",
      "Epoch 26, Batch 126/157, Loss: 2.7370\n",
      "Epoch 26, Batch 127/157, Loss: 2.7340\n",
      "Epoch 26, Batch 128/157, Loss: 2.7344\n",
      "Epoch 26, Batch 129/157, Loss: 2.7372\n",
      "Epoch 26, Batch 130/157, Loss: 2.7346\n",
      "Epoch 26, Batch 131/157, Loss: 2.6802\n",
      "Epoch 26, Batch 132/157, Loss: 2.7034\n",
      "Epoch 26, Batch 133/157, Loss: 2.7602\n",
      "Epoch 26, Batch 134/157, Loss: 2.7152\n",
      "Epoch 26, Batch 135/157, Loss: 2.6845\n",
      "Epoch 26, Batch 136/157, Loss: 2.7186\n",
      "Epoch 26, Batch 137/157, Loss: 2.6451\n",
      "Epoch 26, Batch 138/157, Loss: 2.7508\n",
      "Epoch 26, Batch 139/157, Loss: 2.7311\n",
      "Epoch 26, Batch 140/157, Loss: 2.6880\n",
      "Epoch 26, Batch 141/157, Loss: 2.7447\n",
      "Epoch 26, Batch 142/157, Loss: 2.7549\n",
      "Epoch 26, Batch 143/157, Loss: 2.7969\n",
      "Epoch 26, Batch 144/157, Loss: 2.7028\n",
      "Epoch 26, Batch 145/157, Loss: 2.7984\n",
      "Epoch 26, Batch 146/157, Loss: 2.7069\n",
      "Epoch 26, Batch 147/157, Loss: 2.7042\n",
      "Epoch 26, Batch 148/157, Loss: 2.7099\n",
      "Epoch 26, Batch 149/157, Loss: 2.7363\n",
      "Epoch 26, Batch 150/157, Loss: 2.7458\n",
      "Epoch 26, Batch 151/157, Loss: 2.7347\n",
      "Epoch 26, Batch 152/157, Loss: 2.7103\n",
      "Epoch 26, Batch 153/157, Loss: 2.7499\n",
      "Epoch 26, Batch 154/157, Loss: 2.6918\n",
      "Epoch 26, Batch 155/157, Loss: 2.7511\n",
      "Epoch 26, Batch 156/157, Loss: 2.7420\n",
      "Epoch 26, Batch 157/157, Loss: 2.7878\n",
      "Epoch 26/50, Average Loss: 2.7282\n",
      "Epoch 27, Batch 1/157, Loss: 2.7315\n",
      "Epoch 27, Batch 2/157, Loss: 2.7818\n",
      "Epoch 27, Batch 3/157, Loss: 2.6721\n",
      "Epoch 27, Batch 4/157, Loss: 2.7746\n",
      "Epoch 27, Batch 5/157, Loss: 2.7806\n",
      "Epoch 27, Batch 6/157, Loss: 2.7033\n",
      "Epoch 27, Batch 7/157, Loss: 2.7419\n",
      "Epoch 27, Batch 8/157, Loss: 2.7420\n",
      "Epoch 27, Batch 9/157, Loss: 2.7437\n",
      "Epoch 27, Batch 10/157, Loss: 2.7148\n",
      "Epoch 27, Batch 11/157, Loss: 2.6859\n",
      "Epoch 27, Batch 12/157, Loss: 2.7574\n",
      "Epoch 27, Batch 13/157, Loss: 2.7625\n",
      "Epoch 27, Batch 14/157, Loss: 2.7001\n",
      "Epoch 27, Batch 15/157, Loss: 2.6912\n",
      "Epoch 27, Batch 16/157, Loss: 2.7485\n",
      "Epoch 27, Batch 17/157, Loss: 2.7145\n",
      "Epoch 27, Batch 18/157, Loss: 2.7237\n",
      "Epoch 27, Batch 19/157, Loss: 2.7535\n",
      "Epoch 27, Batch 20/157, Loss: 2.7596\n",
      "Epoch 27, Batch 21/157, Loss: 2.7078\n",
      "Epoch 27, Batch 22/157, Loss: 2.7254\n",
      "Epoch 27, Batch 23/157, Loss: 2.7098\n",
      "Epoch 27, Batch 24/157, Loss: 2.6914\n",
      "Epoch 27, Batch 25/157, Loss: 2.7942\n",
      "Epoch 27, Batch 26/157, Loss: 2.7497\n",
      "Epoch 27, Batch 27/157, Loss: 2.7362\n",
      "Epoch 27, Batch 28/157, Loss: 2.7561\n",
      "Epoch 27, Batch 29/157, Loss: 2.7345\n",
      "Epoch 27, Batch 30/157, Loss: 2.7360\n",
      "Epoch 27, Batch 31/157, Loss: 2.7171\n",
      "Epoch 27, Batch 32/157, Loss: 2.7872\n",
      "Epoch 27, Batch 33/157, Loss: 2.7511\n",
      "Epoch 27, Batch 34/157, Loss: 2.6479\n",
      "Epoch 27, Batch 35/157, Loss: 2.7554\n",
      "Epoch 27, Batch 36/157, Loss: 2.7445\n",
      "Epoch 27, Batch 37/157, Loss: 2.7197\n",
      "Epoch 27, Batch 38/157, Loss: 2.7375\n",
      "Epoch 27, Batch 39/157, Loss: 2.7374\n",
      "Epoch 27, Batch 40/157, Loss: 2.7202\n",
      "Epoch 27, Batch 41/157, Loss: 2.7382\n",
      "Epoch 27, Batch 42/157, Loss: 2.7587\n",
      "Epoch 27, Batch 43/157, Loss: 2.7286\n",
      "Epoch 27, Batch 44/157, Loss: 2.7187\n",
      "Epoch 27, Batch 45/157, Loss: 2.7563\n",
      "Epoch 27, Batch 46/157, Loss: 2.7215\n",
      "Epoch 27, Batch 47/157, Loss: 2.7141\n",
      "Epoch 27, Batch 48/157, Loss: 2.7242\n",
      "Epoch 27, Batch 49/157, Loss: 2.7068\n",
      "Epoch 27, Batch 50/157, Loss: 2.7052\n",
      "Epoch 27, Batch 51/157, Loss: 2.7181\n",
      "Epoch 27, Batch 52/157, Loss: 2.7415\n",
      "Epoch 27, Batch 53/157, Loss: 2.6894\n",
      "Epoch 27, Batch 54/157, Loss: 2.7059\n",
      "Epoch 27, Batch 55/157, Loss: 2.7651\n",
      "Epoch 27, Batch 56/157, Loss: 2.7143\n",
      "Epoch 27, Batch 57/157, Loss: 2.7420\n",
      "Epoch 27, Batch 58/157, Loss: 2.7516\n",
      "Epoch 27, Batch 59/157, Loss: 2.7241\n",
      "Epoch 27, Batch 60/157, Loss: 2.7311\n",
      "Epoch 27, Batch 61/157, Loss: 2.7365\n",
      "Epoch 27, Batch 62/157, Loss: 2.7770\n",
      "Epoch 27, Batch 63/157, Loss: 2.7576\n",
      "Epoch 27, Batch 64/157, Loss: 2.6790\n",
      "Epoch 27, Batch 65/157, Loss: 2.6906\n",
      "Epoch 27, Batch 66/157, Loss: 2.6791\n",
      "Epoch 27, Batch 67/157, Loss: 2.7290\n",
      "Epoch 27, Batch 68/157, Loss: 2.7426\n",
      "Epoch 27, Batch 69/157, Loss: 2.7631\n",
      "Epoch 27, Batch 70/157, Loss: 2.7779\n",
      "Epoch 27, Batch 71/157, Loss: 2.7359\n",
      "Epoch 27, Batch 72/157, Loss: 2.7440\n",
      "Epoch 27, Batch 73/157, Loss: 2.7395\n",
      "Epoch 27, Batch 74/157, Loss: 2.7637\n",
      "Epoch 27, Batch 75/157, Loss: 2.7294\n",
      "Epoch 27, Batch 76/157, Loss: 2.6920\n",
      "Epoch 27, Batch 77/157, Loss: 2.7237\n",
      "Epoch 27, Batch 78/157, Loss: 2.7060\n",
      "Epoch 27, Batch 79/157, Loss: 2.7481\n",
      "Epoch 27, Batch 80/157, Loss: 2.6755\n",
      "Epoch 27, Batch 81/157, Loss: 2.7076\n",
      "Epoch 27, Batch 82/157, Loss: 2.7844\n",
      "Epoch 27, Batch 83/157, Loss: 2.6867\n",
      "Epoch 27, Batch 84/157, Loss: 2.7456\n",
      "Epoch 27, Batch 85/157, Loss: 2.7617\n",
      "Epoch 27, Batch 86/157, Loss: 2.7228\n",
      "Epoch 27, Batch 87/157, Loss: 2.7460\n",
      "Epoch 27, Batch 88/157, Loss: 2.7483\n",
      "Epoch 27, Batch 89/157, Loss: 2.7320\n",
      "Epoch 27, Batch 90/157, Loss: 2.7196\n",
      "Epoch 27, Batch 91/157, Loss: 2.6282\n",
      "Epoch 27, Batch 92/157, Loss: 2.7912\n",
      "Epoch 27, Batch 93/157, Loss: 2.7549\n",
      "Epoch 27, Batch 94/157, Loss: 2.7096\n",
      "Epoch 27, Batch 95/157, Loss: 2.8004\n",
      "Epoch 27, Batch 96/157, Loss: 2.6905\n",
      "Epoch 27, Batch 97/157, Loss: 2.7172\n",
      "Epoch 27, Batch 98/157, Loss: 2.7182\n",
      "Epoch 27, Batch 99/157, Loss: 2.7490\n",
      "Epoch 27, Batch 100/157, Loss: 2.8110\n",
      "Epoch 27, Batch 101/157, Loss: 2.7173\n",
      "Epoch 27, Batch 102/157, Loss: 2.7184\n",
      "Epoch 27, Batch 103/157, Loss: 2.7085\n",
      "Epoch 27, Batch 104/157, Loss: 2.7448\n",
      "Epoch 27, Batch 105/157, Loss: 2.6839\n",
      "Epoch 27, Batch 106/157, Loss: 2.7077\n",
      "Epoch 27, Batch 107/157, Loss: 2.7130\n",
      "Epoch 27, Batch 108/157, Loss: 2.7333\n",
      "Epoch 27, Batch 109/157, Loss: 2.6799\n",
      "Epoch 27, Batch 110/157, Loss: 2.7170\n",
      "Epoch 27, Batch 111/157, Loss: 2.7207\n",
      "Epoch 27, Batch 112/157, Loss: 2.6844\n",
      "Epoch 27, Batch 113/157, Loss: 2.7439\n",
      "Epoch 27, Batch 114/157, Loss: 2.7373\n",
      "Epoch 27, Batch 115/157, Loss: 2.7614\n",
      "Epoch 27, Batch 116/157, Loss: 2.7150\n",
      "Epoch 27, Batch 117/157, Loss: 2.7398\n",
      "Epoch 27, Batch 118/157, Loss: 2.7268\n",
      "Epoch 27, Batch 119/157, Loss: 2.7218\n",
      "Epoch 27, Batch 120/157, Loss: 2.6739\n",
      "Epoch 27, Batch 121/157, Loss: 2.7199\n",
      "Epoch 27, Batch 122/157, Loss: 2.7514\n",
      "Epoch 27, Batch 123/157, Loss: 2.7131\n",
      "Epoch 27, Batch 124/157, Loss: 2.7124\n",
      "Epoch 27, Batch 125/157, Loss: 2.7782\n",
      "Epoch 27, Batch 126/157, Loss: 2.6235\n",
      "Epoch 27, Batch 127/157, Loss: 2.7042\n",
      "Epoch 27, Batch 128/157, Loss: 2.7448\n",
      "Epoch 27, Batch 129/157, Loss: 2.7281\n",
      "Epoch 27, Batch 130/157, Loss: 2.7910\n",
      "Epoch 27, Batch 131/157, Loss: 2.6854\n",
      "Epoch 27, Batch 132/157, Loss: 2.7999\n",
      "Epoch 27, Batch 133/157, Loss: 2.7342\n",
      "Epoch 27, Batch 134/157, Loss: 2.7479\n",
      "Epoch 27, Batch 135/157, Loss: 2.7194\n",
      "Epoch 27, Batch 136/157, Loss: 2.6605\n",
      "Epoch 27, Batch 137/157, Loss: 2.7082\n",
      "Epoch 27, Batch 138/157, Loss: 2.7195\n",
      "Epoch 27, Batch 139/157, Loss: 2.7634\n",
      "Epoch 27, Batch 140/157, Loss: 2.6957\n",
      "Epoch 27, Batch 141/157, Loss: 2.7251\n",
      "Epoch 27, Batch 142/157, Loss: 2.7480\n",
      "Epoch 27, Batch 143/157, Loss: 2.7812\n",
      "Epoch 27, Batch 144/157, Loss: 2.7189\n",
      "Epoch 27, Batch 145/157, Loss: 2.7686\n",
      "Epoch 27, Batch 146/157, Loss: 2.7793\n",
      "Epoch 27, Batch 147/157, Loss: 2.7658\n",
      "Epoch 27, Batch 148/157, Loss: 2.6806\n",
      "Epoch 27, Batch 149/157, Loss: 2.7182\n",
      "Epoch 27, Batch 150/157, Loss: 2.6648\n",
      "Epoch 27, Batch 151/157, Loss: 2.7223\n",
      "Epoch 27, Batch 152/157, Loss: 2.6978\n",
      "Epoch 27, Batch 153/157, Loss: 2.7124\n",
      "Epoch 27, Batch 154/157, Loss: 2.7759\n",
      "Epoch 27, Batch 155/157, Loss: 2.6592\n",
      "Epoch 27, Batch 156/157, Loss: 2.7279\n",
      "Epoch 27, Batch 157/157, Loss: 2.7378\n",
      "Epoch 27/50, Average Loss: 2.7290\n",
      "Epoch 28, Batch 1/157, Loss: 2.7586\n",
      "Epoch 28, Batch 2/157, Loss: 2.7087\n",
      "Epoch 28, Batch 3/157, Loss: 2.7369\n",
      "Epoch 28, Batch 4/157, Loss: 2.7368\n",
      "Epoch 28, Batch 5/157, Loss: 2.7469\n",
      "Epoch 28, Batch 6/157, Loss: 2.7582\n",
      "Epoch 28, Batch 7/157, Loss: 2.7266\n",
      "Epoch 28, Batch 8/157, Loss: 2.7176\n",
      "Epoch 28, Batch 9/157, Loss: 2.6676\n",
      "Epoch 28, Batch 10/157, Loss: 2.7217\n",
      "Epoch 28, Batch 11/157, Loss: 2.6928\n",
      "Epoch 28, Batch 12/157, Loss: 2.7697\n",
      "Epoch 28, Batch 13/157, Loss: 2.7150\n",
      "Epoch 28, Batch 14/157, Loss: 2.8068\n",
      "Epoch 28, Batch 15/157, Loss: 2.7134\n",
      "Epoch 28, Batch 16/157, Loss: 2.7340\n",
      "Epoch 28, Batch 17/157, Loss: 2.7705\n",
      "Epoch 28, Batch 18/157, Loss: 2.7184\n",
      "Epoch 28, Batch 19/157, Loss: 2.7220\n",
      "Epoch 28, Batch 20/157, Loss: 2.6897\n",
      "Epoch 28, Batch 21/157, Loss: 2.7524\n",
      "Epoch 28, Batch 22/157, Loss: 2.7114\n",
      "Epoch 28, Batch 23/157, Loss: 2.7411\n",
      "Epoch 28, Batch 24/157, Loss: 2.7242\n",
      "Epoch 28, Batch 25/157, Loss: 2.7498\n",
      "Epoch 28, Batch 26/157, Loss: 2.7208\n",
      "Epoch 28, Batch 27/157, Loss: 2.7032\n",
      "Epoch 28, Batch 28/157, Loss: 2.7075\n",
      "Epoch 28, Batch 29/157, Loss: 2.6889\n",
      "Epoch 28, Batch 30/157, Loss: 2.7296\n",
      "Epoch 28, Batch 31/157, Loss: 2.6804\n",
      "Epoch 28, Batch 32/157, Loss: 2.6942\n",
      "Epoch 28, Batch 33/157, Loss: 2.7477\n",
      "Epoch 28, Batch 34/157, Loss: 2.7235\n",
      "Epoch 28, Batch 35/157, Loss: 2.7887\n",
      "Epoch 28, Batch 36/157, Loss: 2.7866\n",
      "Epoch 28, Batch 37/157, Loss: 2.7072\n",
      "Epoch 28, Batch 38/157, Loss: 2.7887\n",
      "Epoch 28, Batch 39/157, Loss: 2.7984\n",
      "Epoch 28, Batch 40/157, Loss: 2.7266\n",
      "Epoch 28, Batch 41/157, Loss: 2.7464\n",
      "Epoch 28, Batch 42/157, Loss: 2.6914\n",
      "Epoch 28, Batch 43/157, Loss: 2.6894\n",
      "Epoch 28, Batch 44/157, Loss: 2.7685\n",
      "Epoch 28, Batch 45/157, Loss: 2.7075\n",
      "Epoch 28, Batch 46/157, Loss: 2.7111\n",
      "Epoch 28, Batch 47/157, Loss: 2.6921\n",
      "Epoch 28, Batch 48/157, Loss: 2.7741\n",
      "Epoch 28, Batch 49/157, Loss: 2.7141\n",
      "Epoch 28, Batch 50/157, Loss: 2.7252\n",
      "Epoch 28, Batch 51/157, Loss: 2.7345\n",
      "Epoch 28, Batch 52/157, Loss: 2.7219\n",
      "Epoch 28, Batch 53/157, Loss: 2.7115\n",
      "Epoch 28, Batch 54/157, Loss: 2.6987\n",
      "Epoch 28, Batch 55/157, Loss: 2.7508\n",
      "Epoch 28, Batch 56/157, Loss: 2.6719\n",
      "Epoch 28, Batch 57/157, Loss: 2.6861\n",
      "Epoch 28, Batch 58/157, Loss: 2.7335\n",
      "Epoch 28, Batch 59/157, Loss: 2.7191\n",
      "Epoch 28, Batch 60/157, Loss: 2.7380\n",
      "Epoch 28, Batch 61/157, Loss: 2.7715\n",
      "Epoch 28, Batch 62/157, Loss: 2.6597\n",
      "Epoch 28, Batch 63/157, Loss: 2.7177\n",
      "Epoch 28, Batch 64/157, Loss: 2.7397\n",
      "Epoch 28, Batch 65/157, Loss: 2.7466\n",
      "Epoch 28, Batch 66/157, Loss: 2.6857\n",
      "Epoch 28, Batch 67/157, Loss: 2.6454\n",
      "Epoch 28, Batch 68/157, Loss: 2.6459\n",
      "Epoch 28, Batch 69/157, Loss: 2.7047\n",
      "Epoch 28, Batch 70/157, Loss: 2.6941\n",
      "Epoch 28, Batch 71/157, Loss: 2.6982\n",
      "Epoch 28, Batch 72/157, Loss: 2.7506\n",
      "Epoch 28, Batch 73/157, Loss: 2.7293\n",
      "Epoch 28, Batch 74/157, Loss: 2.6832\n",
      "Epoch 28, Batch 75/157, Loss: 2.7477\n",
      "Epoch 28, Batch 76/157, Loss: 2.7491\n",
      "Epoch 28, Batch 77/157, Loss: 2.7114\n",
      "Epoch 28, Batch 78/157, Loss: 2.7290\n",
      "Epoch 28, Batch 79/157, Loss: 2.6884\n",
      "Epoch 28, Batch 80/157, Loss: 2.7779\n",
      "Epoch 28, Batch 81/157, Loss: 2.6803\n",
      "Epoch 28, Batch 82/157, Loss: 2.8120\n",
      "Epoch 28, Batch 83/157, Loss: 2.7064\n",
      "Epoch 28, Batch 84/157, Loss: 2.7675\n",
      "Epoch 28, Batch 85/157, Loss: 2.6594\n",
      "Epoch 28, Batch 86/157, Loss: 2.7333\n",
      "Epoch 28, Batch 87/157, Loss: 2.6893\n",
      "Epoch 28, Batch 88/157, Loss: 2.7300\n",
      "Epoch 28, Batch 89/157, Loss: 2.7050\n",
      "Epoch 28, Batch 90/157, Loss: 2.6933\n",
      "Epoch 28, Batch 91/157, Loss: 2.7108\n",
      "Epoch 28, Batch 92/157, Loss: 2.7715\n",
      "Epoch 28, Batch 93/157, Loss: 2.7223\n",
      "Epoch 28, Batch 94/157, Loss: 2.7007\n",
      "Epoch 28, Batch 95/157, Loss: 2.7443\n",
      "Epoch 28, Batch 96/157, Loss: 2.6880\n",
      "Epoch 28, Batch 97/157, Loss: 2.7339\n",
      "Epoch 28, Batch 98/157, Loss: 2.7090\n",
      "Epoch 28, Batch 99/157, Loss: 2.7188\n",
      "Epoch 28, Batch 100/157, Loss: 2.7237\n",
      "Epoch 28, Batch 101/157, Loss: 2.7091\n",
      "Epoch 28, Batch 102/157, Loss: 2.6973\n",
      "Epoch 28, Batch 103/157, Loss: 2.7464\n",
      "Epoch 28, Batch 104/157, Loss: 2.7675\n",
      "Epoch 28, Batch 105/157, Loss: 2.7225\n",
      "Epoch 28, Batch 106/157, Loss: 2.7097\n",
      "Epoch 28, Batch 107/157, Loss: 2.6717\n",
      "Epoch 28, Batch 108/157, Loss: 2.7324\n",
      "Epoch 28, Batch 109/157, Loss: 2.6768\n",
      "Epoch 28, Batch 110/157, Loss: 2.6888\n",
      "Epoch 28, Batch 111/157, Loss: 2.7517\n",
      "Epoch 28, Batch 112/157, Loss: 2.7534\n",
      "Epoch 28, Batch 113/157, Loss: 2.7490\n",
      "Epoch 28, Batch 114/157, Loss: 2.6950\n",
      "Epoch 28, Batch 115/157, Loss: 2.7205\n",
      "Epoch 28, Batch 116/157, Loss: 2.7130\n",
      "Epoch 28, Batch 117/157, Loss: 2.6952\n",
      "Epoch 28, Batch 118/157, Loss: 2.7165\n",
      "Epoch 28, Batch 119/157, Loss: 2.7452\n",
      "Epoch 28, Batch 120/157, Loss: 2.7646\n",
      "Epoch 28, Batch 121/157, Loss: 2.7354\n",
      "Epoch 28, Batch 122/157, Loss: 2.7238\n",
      "Epoch 28, Batch 123/157, Loss: 2.7286\n",
      "Epoch 28, Batch 124/157, Loss: 2.7084\n",
      "Epoch 28, Batch 125/157, Loss: 2.7006\n",
      "Epoch 28, Batch 126/157, Loss: 2.7225\n",
      "Epoch 28, Batch 127/157, Loss: 2.7034\n",
      "Epoch 28, Batch 128/157, Loss: 2.7017\n",
      "Epoch 28, Batch 129/157, Loss: 2.7234\n",
      "Epoch 28, Batch 130/157, Loss: 2.7108\n",
      "Epoch 28, Batch 131/157, Loss: 2.7688\n",
      "Epoch 28, Batch 132/157, Loss: 2.7854\n",
      "Epoch 28, Batch 133/157, Loss: 2.7173\n",
      "Epoch 28, Batch 134/157, Loss: 2.6898\n",
      "Epoch 28, Batch 135/157, Loss: 2.6661\n",
      "Epoch 28, Batch 136/157, Loss: 2.7276\n",
      "Epoch 28, Batch 137/157, Loss: 2.7081\n",
      "Epoch 28, Batch 138/157, Loss: 2.6764\n",
      "Epoch 28, Batch 139/157, Loss: 2.7994\n",
      "Epoch 28, Batch 140/157, Loss: 2.7506\n",
      "Epoch 28, Batch 141/157, Loss: 2.7330\n",
      "Epoch 28, Batch 142/157, Loss: 2.7255\n",
      "Epoch 28, Batch 143/157, Loss: 2.7743\n",
      "Epoch 28, Batch 144/157, Loss: 2.7711\n",
      "Epoch 28, Batch 145/157, Loss: 2.7301\n",
      "Epoch 28, Batch 146/157, Loss: 2.7231\n",
      "Epoch 28, Batch 147/157, Loss: 2.7416\n",
      "Epoch 28, Batch 148/157, Loss: 2.7039\n",
      "Epoch 28, Batch 149/157, Loss: 2.7420\n",
      "Epoch 28, Batch 150/157, Loss: 2.7173\n",
      "Epoch 28, Batch 151/157, Loss: 2.6908\n",
      "Epoch 28, Batch 152/157, Loss: 2.7827\n",
      "Epoch 28, Batch 153/157, Loss: 2.7263\n",
      "Epoch 28, Batch 154/157, Loss: 2.7564\n",
      "Epoch 28, Batch 155/157, Loss: 2.7488\n",
      "Epoch 28, Batch 156/157, Loss: 2.7037\n",
      "Epoch 28, Batch 157/157, Loss: 2.6620\n",
      "Epoch 28/50, Average Loss: 2.7239\n",
      "Epoch 29, Batch 1/157, Loss: 2.7482\n",
      "Epoch 29, Batch 2/157, Loss: 2.7683\n",
      "Epoch 29, Batch 3/157, Loss: 2.7233\n",
      "Epoch 29, Batch 4/157, Loss: 2.7270\n",
      "Epoch 29, Batch 5/157, Loss: 2.7392\n",
      "Epoch 29, Batch 6/157, Loss: 2.7663\n",
      "Epoch 29, Batch 7/157, Loss: 2.7118\n",
      "Epoch 29, Batch 8/157, Loss: 2.6804\n",
      "Epoch 29, Batch 9/157, Loss: 2.7090\n",
      "Epoch 29, Batch 10/157, Loss: 2.7409\n",
      "Epoch 29, Batch 11/157, Loss: 2.6916\n",
      "Epoch 29, Batch 12/157, Loss: 2.6778\n",
      "Epoch 29, Batch 13/157, Loss: 2.7110\n",
      "Epoch 29, Batch 14/157, Loss: 2.7299\n",
      "Epoch 29, Batch 15/157, Loss: 2.7219\n",
      "Epoch 29, Batch 16/157, Loss: 2.7074\n",
      "Epoch 29, Batch 17/157, Loss: 2.7488\n",
      "Epoch 29, Batch 18/157, Loss: 2.7217\n",
      "Epoch 29, Batch 19/157, Loss: 2.6983\n",
      "Epoch 29, Batch 20/157, Loss: 2.7635\n",
      "Epoch 29, Batch 21/157, Loss: 2.7599\n",
      "Epoch 29, Batch 22/157, Loss: 2.7124\n",
      "Epoch 29, Batch 23/157, Loss: 2.6909\n",
      "Epoch 29, Batch 24/157, Loss: 2.7009\n",
      "Epoch 29, Batch 25/157, Loss: 2.7239\n",
      "Epoch 29, Batch 26/157, Loss: 2.7048\n",
      "Epoch 29, Batch 27/157, Loss: 2.7542\n",
      "Epoch 29, Batch 28/157, Loss: 2.7429\n",
      "Epoch 29, Batch 29/157, Loss: 2.7189\n",
      "Epoch 29, Batch 30/157, Loss: 2.7009\n",
      "Epoch 29, Batch 31/157, Loss: 2.7388\n",
      "Epoch 29, Batch 32/157, Loss: 2.7138\n",
      "Epoch 29, Batch 33/157, Loss: 2.7271\n",
      "Epoch 29, Batch 34/157, Loss: 2.7026\n",
      "Epoch 29, Batch 35/157, Loss: 2.8202\n",
      "Epoch 29, Batch 36/157, Loss: 2.6753\n",
      "Epoch 29, Batch 37/157, Loss: 2.7420\n",
      "Epoch 29, Batch 38/157, Loss: 2.7914\n",
      "Epoch 29, Batch 39/157, Loss: 2.6634\n",
      "Epoch 29, Batch 40/157, Loss: 2.7244\n",
      "Epoch 29, Batch 41/157, Loss: 2.8060\n",
      "Epoch 29, Batch 42/157, Loss: 2.7315\n",
      "Epoch 29, Batch 43/157, Loss: 2.6973\n",
      "Epoch 29, Batch 44/157, Loss: 2.6611\n",
      "Epoch 29, Batch 45/157, Loss: 2.7254\n",
      "Epoch 29, Batch 46/157, Loss: 2.6942\n",
      "Epoch 29, Batch 47/157, Loss: 2.7416\n",
      "Epoch 29, Batch 48/157, Loss: 2.7365\n",
      "Epoch 29, Batch 49/157, Loss: 2.7217\n",
      "Epoch 29, Batch 50/157, Loss: 2.7309\n",
      "Epoch 29, Batch 51/157, Loss: 2.7256\n",
      "Epoch 29, Batch 52/157, Loss: 2.7351\n",
      "Epoch 29, Batch 53/157, Loss: 2.6667\n",
      "Epoch 29, Batch 54/157, Loss: 2.7102\n",
      "Epoch 29, Batch 55/157, Loss: 2.7090\n",
      "Epoch 29, Batch 56/157, Loss: 2.7264\n",
      "Epoch 29, Batch 57/157, Loss: 2.6523\n",
      "Epoch 29, Batch 58/157, Loss: 2.7232\n",
      "Epoch 29, Batch 59/157, Loss: 2.6908\n",
      "Epoch 29, Batch 60/157, Loss: 2.6971\n",
      "Epoch 29, Batch 61/157, Loss: 2.6662\n",
      "Epoch 29, Batch 62/157, Loss: 2.7442\n",
      "Epoch 29, Batch 63/157, Loss: 2.7663\n",
      "Epoch 29, Batch 64/157, Loss: 2.7239\n",
      "Epoch 29, Batch 65/157, Loss: 2.7541\n",
      "Epoch 29, Batch 66/157, Loss: 2.6982\n",
      "Epoch 29, Batch 67/157, Loss: 2.7399\n",
      "Epoch 29, Batch 68/157, Loss: 2.8224\n",
      "Epoch 29, Batch 69/157, Loss: 2.6868\n",
      "Epoch 29, Batch 70/157, Loss: 2.6815\n",
      "Epoch 29, Batch 71/157, Loss: 2.7362\n",
      "Epoch 29, Batch 72/157, Loss: 2.7815\n",
      "Epoch 29, Batch 73/157, Loss: 2.7517\n",
      "Epoch 29, Batch 74/157, Loss: 2.6952\n",
      "Epoch 29, Batch 75/157, Loss: 2.6903\n",
      "Epoch 29, Batch 76/157, Loss: 2.6900\n",
      "Epoch 29, Batch 77/157, Loss: 2.7488\n",
      "Epoch 29, Batch 78/157, Loss: 2.6914\n",
      "Epoch 29, Batch 79/157, Loss: 2.7184\n",
      "Epoch 29, Batch 80/157, Loss: 2.6986\n",
      "Epoch 29, Batch 81/157, Loss: 2.6966\n",
      "Epoch 29, Batch 82/157, Loss: 2.7243\n",
      "Epoch 29, Batch 83/157, Loss: 2.6789\n",
      "Epoch 29, Batch 84/157, Loss: 2.7317\n",
      "Epoch 29, Batch 85/157, Loss: 2.6952\n",
      "Epoch 29, Batch 86/157, Loss: 2.7862\n",
      "Epoch 29, Batch 87/157, Loss: 2.7560\n",
      "Epoch 29, Batch 88/157, Loss: 2.6838\n",
      "Epoch 29, Batch 89/157, Loss: 2.7070\n",
      "Epoch 29, Batch 90/157, Loss: 2.7027\n",
      "Epoch 29, Batch 91/157, Loss: 2.7387\n",
      "Epoch 29, Batch 92/157, Loss: 2.6531\n",
      "Epoch 29, Batch 93/157, Loss: 2.7142\n",
      "Epoch 29, Batch 94/157, Loss: 2.6823\n",
      "Epoch 29, Batch 95/157, Loss: 2.7038\n",
      "Epoch 29, Batch 96/157, Loss: 2.7411\n",
      "Epoch 29, Batch 97/157, Loss: 2.7417\n",
      "Epoch 29, Batch 98/157, Loss: 2.7769\n",
      "Epoch 29, Batch 99/157, Loss: 2.7537\n",
      "Epoch 29, Batch 100/157, Loss: 2.7268\n",
      "Epoch 29, Batch 101/157, Loss: 2.6665\n",
      "Epoch 29, Batch 102/157, Loss: 2.7622\n",
      "Epoch 29, Batch 103/157, Loss: 2.7113\n",
      "Epoch 29, Batch 104/157, Loss: 2.7117\n",
      "Epoch 29, Batch 105/157, Loss: 2.7634\n",
      "Epoch 29, Batch 106/157, Loss: 2.7335\n",
      "Epoch 29, Batch 107/157, Loss: 2.6705\n",
      "Epoch 29, Batch 108/157, Loss: 2.6851\n",
      "Epoch 29, Batch 109/157, Loss: 2.6910\n",
      "Epoch 29, Batch 110/157, Loss: 2.7057\n",
      "Epoch 29, Batch 111/157, Loss: 2.7221\n",
      "Epoch 29, Batch 112/157, Loss: 2.7197\n",
      "Epoch 29, Batch 113/157, Loss: 2.7393\n",
      "Epoch 29, Batch 114/157, Loss: 2.7120\n",
      "Epoch 29, Batch 115/157, Loss: 2.7451\n",
      "Epoch 29, Batch 116/157, Loss: 2.7678\n",
      "Epoch 29, Batch 117/157, Loss: 2.7403\n",
      "Epoch 29, Batch 118/157, Loss: 2.7223\n",
      "Epoch 29, Batch 119/157, Loss: 2.7162\n",
      "Epoch 29, Batch 120/157, Loss: 2.7472\n",
      "Epoch 29, Batch 121/157, Loss: 2.6936\n",
      "Epoch 29, Batch 122/157, Loss: 2.7245\n",
      "Epoch 29, Batch 123/157, Loss: 2.7128\n",
      "Epoch 29, Batch 124/157, Loss: 2.7471\n",
      "Epoch 29, Batch 125/157, Loss: 2.6889\n",
      "Epoch 29, Batch 126/157, Loss: 2.7124\n",
      "Epoch 29, Batch 127/157, Loss: 2.7772\n",
      "Epoch 29, Batch 128/157, Loss: 2.6916\n",
      "Epoch 29, Batch 129/157, Loss: 2.7337\n",
      "Epoch 29, Batch 130/157, Loss: 2.6861\n",
      "Epoch 29, Batch 131/157, Loss: 2.6822\n",
      "Epoch 29, Batch 132/157, Loss: 2.7379\n",
      "Epoch 29, Batch 133/157, Loss: 2.7628\n",
      "Epoch 29, Batch 134/157, Loss: 2.7167\n",
      "Epoch 29, Batch 135/157, Loss: 2.7092\n",
      "Epoch 29, Batch 136/157, Loss: 2.6978\n",
      "Epoch 29, Batch 137/157, Loss: 2.6669\n",
      "Epoch 29, Batch 138/157, Loss: 2.7408\n",
      "Epoch 29, Batch 139/157, Loss: 2.6751\n",
      "Epoch 29, Batch 140/157, Loss: 2.7967\n",
      "Epoch 29, Batch 141/157, Loss: 2.7226\n",
      "Epoch 29, Batch 142/157, Loss: 2.6912\n",
      "Epoch 29, Batch 143/157, Loss: 2.7358\n",
      "Epoch 29, Batch 144/157, Loss: 2.7097\n",
      "Epoch 29, Batch 145/157, Loss: 2.6799\n",
      "Epoch 29, Batch 146/157, Loss: 2.7403\n",
      "Epoch 29, Batch 147/157, Loss: 2.7308\n",
      "Epoch 29, Batch 148/157, Loss: 2.7426\n",
      "Epoch 29, Batch 149/157, Loss: 2.7043\n",
      "Epoch 29, Batch 150/157, Loss: 2.7513\n",
      "Epoch 29, Batch 151/157, Loss: 2.7755\n",
      "Epoch 29, Batch 152/157, Loss: 2.7065\n",
      "Epoch 29, Batch 153/157, Loss: 2.7359\n",
      "Epoch 29, Batch 154/157, Loss: 2.7223\n",
      "Epoch 29, Batch 155/157, Loss: 2.7501\n",
      "Epoch 29, Batch 156/157, Loss: 2.7272\n",
      "Epoch 29, Batch 157/157, Loss: 2.6906\n",
      "Epoch 29/50, Average Loss: 2.7214\n",
      "Epoch 30, Batch 1/157, Loss: 2.7376\n",
      "Epoch 30, Batch 2/157, Loss: 2.7336\n",
      "Epoch 30, Batch 3/157, Loss: 2.7159\n",
      "Epoch 30, Batch 4/157, Loss: 2.7633\n",
      "Epoch 30, Batch 5/157, Loss: 2.7609\n",
      "Epoch 30, Batch 6/157, Loss: 2.7549\n",
      "Epoch 30, Batch 7/157, Loss: 2.8235\n",
      "Epoch 30, Batch 8/157, Loss: 2.6961\n",
      "Epoch 30, Batch 9/157, Loss: 2.7403\n",
      "Epoch 30, Batch 10/157, Loss: 2.7654\n",
      "Epoch 30, Batch 11/157, Loss: 2.7380\n",
      "Epoch 30, Batch 12/157, Loss: 2.6901\n",
      "Epoch 30, Batch 13/157, Loss: 2.7962\n",
      "Epoch 30, Batch 14/157, Loss: 2.7190\n",
      "Epoch 30, Batch 15/157, Loss: 2.7428\n",
      "Epoch 30, Batch 16/157, Loss: 2.7266\n",
      "Epoch 30, Batch 17/157, Loss: 2.7065\n",
      "Epoch 30, Batch 18/157, Loss: 2.7224\n",
      "Epoch 30, Batch 19/157, Loss: 2.7481\n",
      "Epoch 30, Batch 20/157, Loss: 2.7046\n",
      "Epoch 30, Batch 21/157, Loss: 2.7337\n",
      "Epoch 30, Batch 22/157, Loss: 2.7661\n",
      "Epoch 30, Batch 23/157, Loss: 2.7205\n",
      "Epoch 30, Batch 24/157, Loss: 2.7504\n",
      "Epoch 30, Batch 25/157, Loss: 2.7131\n",
      "Epoch 30, Batch 26/157, Loss: 2.6962\n",
      "Epoch 30, Batch 27/157, Loss: 2.7023\n",
      "Epoch 30, Batch 28/157, Loss: 2.7076\n",
      "Epoch 30, Batch 29/157, Loss: 2.7400\n",
      "Epoch 30, Batch 30/157, Loss: 2.7191\n",
      "Epoch 30, Batch 31/157, Loss: 2.6949\n",
      "Epoch 30, Batch 32/157, Loss: 2.7488\n",
      "Epoch 30, Batch 33/157, Loss: 2.7149\n",
      "Epoch 30, Batch 34/157, Loss: 2.6917\n",
      "Epoch 30, Batch 35/157, Loss: 2.7612\n",
      "Epoch 30, Batch 36/157, Loss: 2.7372\n",
      "Epoch 30, Batch 37/157, Loss: 2.7700\n",
      "Epoch 30, Batch 38/157, Loss: 2.7186\n",
      "Epoch 30, Batch 39/157, Loss: 2.7428\n",
      "Epoch 30, Batch 40/157, Loss: 2.7512\n",
      "Epoch 30, Batch 41/157, Loss: 2.7492\n",
      "Epoch 30, Batch 42/157, Loss: 2.7097\n",
      "Epoch 30, Batch 43/157, Loss: 2.7005\n",
      "Epoch 30, Batch 44/157, Loss: 2.7685\n",
      "Epoch 30, Batch 45/157, Loss: 2.7101\n",
      "Epoch 30, Batch 46/157, Loss: 2.7350\n",
      "Epoch 30, Batch 47/157, Loss: 2.7166\n",
      "Epoch 30, Batch 48/157, Loss: 2.7601\n",
      "Epoch 30, Batch 49/157, Loss: 2.7371\n",
      "Epoch 30, Batch 50/157, Loss: 2.6926\n",
      "Epoch 30, Batch 51/157, Loss: 2.7114\n",
      "Epoch 30, Batch 52/157, Loss: 2.7167\n",
      "Epoch 30, Batch 53/157, Loss: 2.7552\n",
      "Epoch 30, Batch 54/157, Loss: 2.6874\n",
      "Epoch 30, Batch 55/157, Loss: 2.7097\n",
      "Epoch 30, Batch 56/157, Loss: 2.7503\n",
      "Epoch 30, Batch 57/157, Loss: 2.7131\n",
      "Epoch 30, Batch 58/157, Loss: 2.7423\n",
      "Epoch 30, Batch 59/157, Loss: 2.7180\n",
      "Epoch 30, Batch 60/157, Loss: 2.7184\n",
      "Epoch 30, Batch 61/157, Loss: 2.6750\n",
      "Epoch 30, Batch 62/157, Loss: 2.7047\n",
      "Epoch 30, Batch 63/157, Loss: 2.7253\n",
      "Epoch 30, Batch 64/157, Loss: 2.6492\n",
      "Epoch 30, Batch 65/157, Loss: 2.7392\n",
      "Epoch 30, Batch 66/157, Loss: 2.6947\n",
      "Epoch 30, Batch 67/157, Loss: 2.7145\n",
      "Epoch 30, Batch 68/157, Loss: 2.7488\n",
      "Epoch 30, Batch 69/157, Loss: 2.6880\n",
      "Epoch 30, Batch 70/157, Loss: 2.6734\n",
      "Epoch 30, Batch 71/157, Loss: 2.7676\n",
      "Epoch 30, Batch 72/157, Loss: 2.6733\n",
      "Epoch 30, Batch 73/157, Loss: 2.6824\n",
      "Epoch 30, Batch 74/157, Loss: 2.7437\n",
      "Epoch 30, Batch 75/157, Loss: 2.7681\n",
      "Epoch 30, Batch 76/157, Loss: 2.6716\n",
      "Epoch 30, Batch 77/157, Loss: 2.7828\n",
      "Epoch 30, Batch 78/157, Loss: 2.7389\n",
      "Epoch 30, Batch 79/157, Loss: 2.7371\n",
      "Epoch 30, Batch 80/157, Loss: 2.7425\n",
      "Epoch 30, Batch 81/157, Loss: 2.7076\n",
      "Epoch 30, Batch 82/157, Loss: 2.7786\n",
      "Epoch 30, Batch 83/157, Loss: 2.7611\n",
      "Epoch 30, Batch 84/157, Loss: 2.6963\n",
      "Epoch 30, Batch 85/157, Loss: 2.7290\n",
      "Epoch 30, Batch 86/157, Loss: 2.7487\n",
      "Epoch 30, Batch 87/157, Loss: 2.7502\n",
      "Epoch 30, Batch 88/157, Loss: 2.7384\n",
      "Epoch 30, Batch 89/157, Loss: 2.7377\n",
      "Epoch 30, Batch 90/157, Loss: 2.7242\n",
      "Epoch 30, Batch 91/157, Loss: 2.7223\n",
      "Epoch 30, Batch 92/157, Loss: 2.7268\n",
      "Epoch 30, Batch 93/157, Loss: 2.6988\n",
      "Epoch 30, Batch 94/157, Loss: 2.7082\n",
      "Epoch 30, Batch 95/157, Loss: 2.7274\n",
      "Epoch 30, Batch 96/157, Loss: 2.7110\n",
      "Epoch 30, Batch 97/157, Loss: 2.7028\n",
      "Epoch 30, Batch 98/157, Loss: 2.7383\n",
      "Epoch 30, Batch 99/157, Loss: 2.7323\n",
      "Epoch 30, Batch 100/157, Loss: 2.7236\n",
      "Epoch 30, Batch 101/157, Loss: 2.7090\n",
      "Epoch 30, Batch 102/157, Loss: 2.6967\n",
      "Epoch 30, Batch 103/157, Loss: 2.7132\n",
      "Epoch 30, Batch 104/157, Loss: 2.7005\n",
      "Epoch 30, Batch 105/157, Loss: 2.7381\n",
      "Epoch 30, Batch 106/157, Loss: 2.7226\n",
      "Epoch 30, Batch 107/157, Loss: 2.7288\n",
      "Epoch 30, Batch 108/157, Loss: 2.7175\n",
      "Epoch 30, Batch 109/157, Loss: 2.7111\n",
      "Epoch 30, Batch 110/157, Loss: 2.7039\n",
      "Epoch 30, Batch 111/157, Loss: 2.7650\n",
      "Epoch 30, Batch 112/157, Loss: 2.7833\n",
      "Epoch 30, Batch 113/157, Loss: 2.6783\n",
      "Epoch 30, Batch 114/157, Loss: 2.7051\n",
      "Epoch 30, Batch 115/157, Loss: 2.6771\n",
      "Epoch 30, Batch 116/157, Loss: 2.7344\n",
      "Epoch 30, Batch 117/157, Loss: 2.7200\n",
      "Epoch 30, Batch 118/157, Loss: 2.7140\n",
      "Epoch 30, Batch 119/157, Loss: 2.7532\n",
      "Epoch 30, Batch 120/157, Loss: 2.7309\n",
      "Epoch 30, Batch 121/157, Loss: 2.7064\n",
      "Epoch 30, Batch 122/157, Loss: 2.7446\n",
      "Epoch 30, Batch 123/157, Loss: 2.7199\n",
      "Epoch 30, Batch 124/157, Loss: 2.7076\n",
      "Epoch 30, Batch 125/157, Loss: 2.7174\n",
      "Epoch 30, Batch 126/157, Loss: 2.6988\n",
      "Epoch 30, Batch 127/157, Loss: 2.7155\n",
      "Epoch 30, Batch 128/157, Loss: 2.7472\n",
      "Epoch 30, Batch 129/157, Loss: 2.7161\n",
      "Epoch 30, Batch 130/157, Loss: 2.7012\n",
      "Epoch 30, Batch 131/157, Loss: 2.7056\n",
      "Epoch 30, Batch 132/157, Loss: 2.7415\n",
      "Epoch 30, Batch 133/157, Loss: 2.7263\n",
      "Epoch 30, Batch 134/157, Loss: 2.7240\n",
      "Epoch 30, Batch 135/157, Loss: 2.7082\n",
      "Epoch 30, Batch 136/157, Loss: 2.7236\n",
      "Epoch 30, Batch 137/157, Loss: 2.6980\n",
      "Epoch 30, Batch 138/157, Loss: 2.7271\n",
      "Epoch 30, Batch 139/157, Loss: 2.7265\n",
      "Epoch 30, Batch 140/157, Loss: 2.7346\n",
      "Epoch 30, Batch 141/157, Loss: 2.7356\n",
      "Epoch 30, Batch 142/157, Loss: 2.7261\n",
      "Epoch 30, Batch 143/157, Loss: 2.6938\n",
      "Epoch 30, Batch 144/157, Loss: 2.7040\n",
      "Epoch 30, Batch 145/157, Loss: 2.7326\n",
      "Epoch 30, Batch 146/157, Loss: 2.6935\n",
      "Epoch 30, Batch 147/157, Loss: 2.7191\n",
      "Epoch 30, Batch 148/157, Loss: 2.7124\n",
      "Epoch 30, Batch 149/157, Loss: 2.6507\n",
      "Epoch 30, Batch 150/157, Loss: 2.7267\n",
      "Epoch 30, Batch 151/157, Loss: 2.7506\n",
      "Epoch 30, Batch 152/157, Loss: 2.7711\n",
      "Epoch 30, Batch 153/157, Loss: 2.7488\n",
      "Epoch 30, Batch 154/157, Loss: 2.7000\n",
      "Epoch 30, Batch 155/157, Loss: 2.7507\n",
      "Epoch 30, Batch 156/157, Loss: 2.7239\n",
      "Epoch 30, Batch 157/157, Loss: 2.7439\n",
      "Epoch 30/50, Average Loss: 2.7250\n",
      "Epoch 31, Batch 1/157, Loss: 2.7053\n",
      "Epoch 31, Batch 2/157, Loss: 2.7045\n",
      "Epoch 31, Batch 3/157, Loss: 2.6890\n",
      "Epoch 31, Batch 4/157, Loss: 2.7038\n",
      "Epoch 31, Batch 5/157, Loss: 2.7486\n",
      "Epoch 31, Batch 6/157, Loss: 2.6938\n",
      "Epoch 31, Batch 7/157, Loss: 2.7284\n",
      "Epoch 31, Batch 8/157, Loss: 2.7620\n",
      "Epoch 31, Batch 9/157, Loss: 2.7901\n",
      "Epoch 31, Batch 10/157, Loss: 2.7723\n",
      "Epoch 31, Batch 11/157, Loss: 2.6667\n",
      "Epoch 31, Batch 12/157, Loss: 2.7795\n",
      "Epoch 31, Batch 13/157, Loss: 2.7543\n",
      "Epoch 31, Batch 14/157, Loss: 2.6439\n",
      "Epoch 31, Batch 15/157, Loss: 2.7640\n",
      "Epoch 31, Batch 16/157, Loss: 2.7001\n",
      "Epoch 31, Batch 17/157, Loss: 2.7414\n",
      "Epoch 31, Batch 18/157, Loss: 2.7488\n",
      "Epoch 31, Batch 19/157, Loss: 2.7317\n",
      "Epoch 31, Batch 20/157, Loss: 2.7379\n",
      "Epoch 31, Batch 21/157, Loss: 2.7450\n",
      "Epoch 31, Batch 22/157, Loss: 2.7255\n",
      "Epoch 31, Batch 23/157, Loss: 2.6432\n",
      "Epoch 31, Batch 24/157, Loss: 2.7375\n",
      "Epoch 31, Batch 25/157, Loss: 2.7065\n",
      "Epoch 31, Batch 26/157, Loss: 2.7462\n",
      "Epoch 31, Batch 27/157, Loss: 2.7038\n",
      "Epoch 31, Batch 28/157, Loss: 2.7236\n",
      "Epoch 31, Batch 29/157, Loss: 2.6863\n",
      "Epoch 31, Batch 30/157, Loss: 2.7328\n",
      "Epoch 31, Batch 31/157, Loss: 2.7012\n",
      "Epoch 31, Batch 32/157, Loss: 2.6843\n",
      "Epoch 31, Batch 33/157, Loss: 2.7048\n",
      "Epoch 31, Batch 34/157, Loss: 2.7424\n",
      "Epoch 31, Batch 35/157, Loss: 2.6968\n",
      "Epoch 31, Batch 36/157, Loss: 2.6958\n",
      "Epoch 31, Batch 37/157, Loss: 2.7256\n",
      "Epoch 31, Batch 38/157, Loss: 2.7123\n",
      "Epoch 31, Batch 39/157, Loss: 2.7025\n",
      "Epoch 31, Batch 40/157, Loss: 2.6632\n",
      "Epoch 31, Batch 41/157, Loss: 2.6841\n",
      "Epoch 31, Batch 42/157, Loss: 2.6994\n",
      "Epoch 31, Batch 43/157, Loss: 2.7406\n",
      "Epoch 31, Batch 44/157, Loss: 2.7151\n",
      "Epoch 31, Batch 45/157, Loss: 2.7886\n",
      "Epoch 31, Batch 46/157, Loss: 2.6628\n",
      "Epoch 31, Batch 47/157, Loss: 2.7044\n",
      "Epoch 31, Batch 48/157, Loss: 2.7229\n",
      "Epoch 31, Batch 49/157, Loss: 2.7521\n",
      "Epoch 31, Batch 50/157, Loss: 2.7841\n",
      "Epoch 31, Batch 51/157, Loss: 2.7369\n",
      "Epoch 31, Batch 52/157, Loss: 2.6547\n",
      "Epoch 31, Batch 53/157, Loss: 2.7349\n",
      "Epoch 31, Batch 54/157, Loss: 2.7532\n",
      "Epoch 31, Batch 55/157, Loss: 2.7791\n",
      "Epoch 31, Batch 56/157, Loss: 2.6818\n",
      "Epoch 31, Batch 57/157, Loss: 2.7048\n",
      "Epoch 31, Batch 58/157, Loss: 2.7662\n",
      "Epoch 31, Batch 59/157, Loss: 2.7034\n",
      "Epoch 31, Batch 60/157, Loss: 2.7786\n",
      "Epoch 31, Batch 61/157, Loss: 2.8458\n",
      "Epoch 31, Batch 62/157, Loss: 2.7213\n",
      "Epoch 31, Batch 63/157, Loss: 2.7162\n",
      "Epoch 31, Batch 64/157, Loss: 2.7518\n",
      "Epoch 31, Batch 65/157, Loss: 2.7522\n",
      "Epoch 31, Batch 66/157, Loss: 2.7034\n",
      "Epoch 31, Batch 67/157, Loss: 2.7232\n",
      "Epoch 31, Batch 68/157, Loss: 2.7113\n",
      "Epoch 31, Batch 69/157, Loss: 2.6748\n",
      "Epoch 31, Batch 70/157, Loss: 2.7113\n",
      "Epoch 31, Batch 71/157, Loss: 2.6713\n",
      "Epoch 31, Batch 72/157, Loss: 2.7471\n",
      "Epoch 31, Batch 73/157, Loss: 2.7062\n",
      "Epoch 31, Batch 74/157, Loss: 2.7190\n",
      "Epoch 31, Batch 75/157, Loss: 2.7495\n",
      "Epoch 31, Batch 76/157, Loss: 2.7297\n",
      "Epoch 31, Batch 77/157, Loss: 2.7276\n",
      "Epoch 31, Batch 78/157, Loss: 2.7037\n",
      "Epoch 31, Batch 79/157, Loss: 2.6981\n",
      "Epoch 31, Batch 80/157, Loss: 2.7253\n",
      "Epoch 31, Batch 81/157, Loss: 2.7286\n",
      "Epoch 31, Batch 82/157, Loss: 2.7761\n",
      "Epoch 31, Batch 83/157, Loss: 2.7390\n",
      "Epoch 31, Batch 84/157, Loss: 2.7512\n",
      "Epoch 31, Batch 85/157, Loss: 2.7151\n",
      "Epoch 31, Batch 86/157, Loss: 2.7326\n",
      "Epoch 31, Batch 87/157, Loss: 2.7232\n",
      "Epoch 31, Batch 88/157, Loss: 2.7415\n",
      "Epoch 31, Batch 89/157, Loss: 2.7336\n",
      "Epoch 31, Batch 90/157, Loss: 2.7232\n",
      "Epoch 31, Batch 91/157, Loss: 2.7311\n",
      "Epoch 31, Batch 92/157, Loss: 2.6989\n",
      "Epoch 31, Batch 93/157, Loss: 2.7244\n",
      "Epoch 31, Batch 94/157, Loss: 2.7269\n",
      "Epoch 31, Batch 95/157, Loss: 2.7526\n",
      "Epoch 31, Batch 96/157, Loss: 2.7186\n",
      "Epoch 31, Batch 97/157, Loss: 2.7108\n",
      "Epoch 31, Batch 98/157, Loss: 2.7416\n",
      "Epoch 31, Batch 99/157, Loss: 2.7524\n",
      "Epoch 31, Batch 100/157, Loss: 2.6872\n",
      "Epoch 31, Batch 101/157, Loss: 2.7761\n",
      "Epoch 31, Batch 102/157, Loss: 2.6996\n",
      "Epoch 31, Batch 103/157, Loss: 2.6963\n",
      "Epoch 31, Batch 104/157, Loss: 2.7713\n",
      "Epoch 31, Batch 105/157, Loss: 2.7480\n",
      "Epoch 31, Batch 106/157, Loss: 2.7296\n",
      "Epoch 31, Batch 107/157, Loss: 2.7921\n",
      "Epoch 31, Batch 108/157, Loss: 2.7053\n",
      "Epoch 31, Batch 109/157, Loss: 2.6852\n",
      "Epoch 31, Batch 110/157, Loss: 2.7130\n",
      "Epoch 31, Batch 111/157, Loss: 2.7277\n",
      "Epoch 31, Batch 112/157, Loss: 2.6944\n",
      "Epoch 31, Batch 113/157, Loss: 2.7139\n",
      "Epoch 31, Batch 114/157, Loss: 2.7087\n",
      "Epoch 31, Batch 115/157, Loss: 2.7604\n",
      "Epoch 31, Batch 116/157, Loss: 2.7143\n",
      "Epoch 31, Batch 117/157, Loss: 2.7054\n",
      "Epoch 31, Batch 118/157, Loss: 2.6814\n",
      "Epoch 31, Batch 119/157, Loss: 2.6920\n",
      "Epoch 31, Batch 120/157, Loss: 2.7942\n",
      "Epoch 31, Batch 121/157, Loss: 2.7432\n",
      "Epoch 31, Batch 122/157, Loss: 2.7757\n",
      "Epoch 31, Batch 123/157, Loss: 2.7290\n",
      "Epoch 31, Batch 124/157, Loss: 2.7746\n",
      "Epoch 31, Batch 125/157, Loss: 2.7410\n",
      "Epoch 31, Batch 126/157, Loss: 2.7279\n",
      "Epoch 31, Batch 127/157, Loss: 2.7099\n",
      "Epoch 31, Batch 128/157, Loss: 2.7500\n",
      "Epoch 31, Batch 129/157, Loss: 2.7195\n",
      "Epoch 31, Batch 130/157, Loss: 2.7332\n",
      "Epoch 31, Batch 131/157, Loss: 2.7636\n",
      "Epoch 31, Batch 132/157, Loss: 2.7112\n",
      "Epoch 31, Batch 133/157, Loss: 2.6675\n",
      "Epoch 31, Batch 134/157, Loss: 2.7386\n",
      "Epoch 31, Batch 135/157, Loss: 2.7336\n",
      "Epoch 31, Batch 136/157, Loss: 2.7227\n",
      "Epoch 31, Batch 137/157, Loss: 2.7638\n",
      "Epoch 31, Batch 138/157, Loss: 2.6920\n",
      "Epoch 31, Batch 139/157, Loss: 2.6876\n",
      "Epoch 31, Batch 140/157, Loss: 2.7290\n",
      "Epoch 31, Batch 141/157, Loss: 2.7175\n",
      "Epoch 31, Batch 142/157, Loss: 2.6653\n",
      "Epoch 31, Batch 143/157, Loss: 2.7233\n",
      "Epoch 31, Batch 144/157, Loss: 2.7223\n",
      "Epoch 31, Batch 145/157, Loss: 2.6978\n",
      "Epoch 31, Batch 146/157, Loss: 2.6865\n",
      "Epoch 31, Batch 147/157, Loss: 2.7525\n",
      "Epoch 31, Batch 148/157, Loss: 2.7299\n",
      "Epoch 31, Batch 149/157, Loss: 2.6955\n",
      "Epoch 31, Batch 150/157, Loss: 2.7280\n",
      "Epoch 31, Batch 151/157, Loss: 2.7318\n",
      "Epoch 31, Batch 152/157, Loss: 2.6908\n",
      "Epoch 31, Batch 153/157, Loss: 2.6960\n",
      "Epoch 31, Batch 154/157, Loss: 2.7283\n",
      "Epoch 31, Batch 155/157, Loss: 2.7303\n",
      "Epoch 31, Batch 156/157, Loss: 2.7168\n",
      "Epoch 31, Batch 157/157, Loss: 2.8487\n",
      "Epoch 31/50, Average Loss: 2.7247\n",
      "Epoch 32, Batch 1/157, Loss: 2.7163\n",
      "Epoch 32, Batch 2/157, Loss: 2.7508\n",
      "Epoch 32, Batch 3/157, Loss: 2.7140\n",
      "Epoch 32, Batch 4/157, Loss: 2.7873\n",
      "Epoch 32, Batch 5/157, Loss: 2.7071\n",
      "Epoch 32, Batch 6/157, Loss: 2.7156\n",
      "Epoch 32, Batch 7/157, Loss: 2.7714\n",
      "Epoch 32, Batch 8/157, Loss: 2.7711\n",
      "Epoch 32, Batch 9/157, Loss: 2.7160\n",
      "Epoch 32, Batch 10/157, Loss: 2.7171\n",
      "Epoch 32, Batch 11/157, Loss: 2.7343\n",
      "Epoch 32, Batch 12/157, Loss: 2.7056\n",
      "Epoch 32, Batch 13/157, Loss: 2.7237\n",
      "Epoch 32, Batch 14/157, Loss: 2.7534\n",
      "Epoch 32, Batch 15/157, Loss: 2.7496\n",
      "Epoch 32, Batch 16/157, Loss: 2.7262\n",
      "Epoch 32, Batch 17/157, Loss: 2.7643\n",
      "Epoch 32, Batch 18/157, Loss: 2.6771\n",
      "Epoch 32, Batch 19/157, Loss: 2.7093\n",
      "Epoch 32, Batch 20/157, Loss: 2.7654\n",
      "Epoch 32, Batch 21/157, Loss: 2.7682\n",
      "Epoch 32, Batch 22/157, Loss: 2.6948\n",
      "Epoch 32, Batch 23/157, Loss: 2.7316\n",
      "Epoch 32, Batch 24/157, Loss: 2.7811\n",
      "Epoch 32, Batch 25/157, Loss: 2.7164\n",
      "Epoch 32, Batch 26/157, Loss: 2.7423\n",
      "Epoch 32, Batch 27/157, Loss: 2.7159\n",
      "Epoch 32, Batch 28/157, Loss: 2.7233\n",
      "Epoch 32, Batch 29/157, Loss: 2.7119\n",
      "Epoch 32, Batch 30/157, Loss: 2.7047\n",
      "Epoch 32, Batch 31/157, Loss: 2.7301\n",
      "Epoch 32, Batch 32/157, Loss: 2.7564\n",
      "Epoch 32, Batch 33/157, Loss: 2.7489\n",
      "Epoch 32, Batch 34/157, Loss: 2.6299\n",
      "Epoch 32, Batch 35/157, Loss: 2.6855\n",
      "Epoch 32, Batch 36/157, Loss: 2.7128\n",
      "Epoch 32, Batch 37/157, Loss: 2.6978\n",
      "Epoch 32, Batch 38/157, Loss: 2.7068\n",
      "Epoch 32, Batch 39/157, Loss: 2.6621\n",
      "Epoch 32, Batch 40/157, Loss: 2.7464\n",
      "Epoch 32, Batch 41/157, Loss: 2.6897\n",
      "Epoch 32, Batch 42/157, Loss: 2.7628\n",
      "Epoch 32, Batch 43/157, Loss: 2.7200\n",
      "Epoch 32, Batch 44/157, Loss: 2.6664\n",
      "Epoch 32, Batch 45/157, Loss: 2.7131\n",
      "Epoch 32, Batch 46/157, Loss: 2.6691\n",
      "Epoch 32, Batch 47/157, Loss: 2.7288\n",
      "Epoch 32, Batch 48/157, Loss: 2.7472\n",
      "Epoch 32, Batch 49/157, Loss: 2.6858\n",
      "Epoch 32, Batch 50/157, Loss: 2.6897\n",
      "Epoch 32, Batch 51/157, Loss: 2.7083\n",
      "Epoch 32, Batch 52/157, Loss: 2.7099\n",
      "Epoch 32, Batch 53/157, Loss: 2.7265\n",
      "Epoch 32, Batch 54/157, Loss: 2.6941\n",
      "Epoch 32, Batch 55/157, Loss: 2.7497\n",
      "Epoch 32, Batch 56/157, Loss: 2.7470\n",
      "Epoch 32, Batch 57/157, Loss: 2.7194\n",
      "Epoch 32, Batch 58/157, Loss: 2.7206\n",
      "Epoch 32, Batch 59/157, Loss: 2.7596\n",
      "Epoch 32, Batch 60/157, Loss: 2.7286\n",
      "Epoch 32, Batch 61/157, Loss: 2.7055\n",
      "Epoch 32, Batch 62/157, Loss: 2.6844\n",
      "Epoch 32, Batch 63/157, Loss: 2.7386\n",
      "Epoch 32, Batch 64/157, Loss: 2.6968\n",
      "Epoch 32, Batch 65/157, Loss: 2.7867\n",
      "Epoch 32, Batch 66/157, Loss: 2.7878\n",
      "Epoch 32, Batch 67/157, Loss: 2.7269\n",
      "Epoch 32, Batch 68/157, Loss: 2.7423\n",
      "Epoch 32, Batch 69/157, Loss: 2.7802\n",
      "Epoch 32, Batch 70/157, Loss: 2.6899\n",
      "Epoch 32, Batch 71/157, Loss: 2.7532\n",
      "Epoch 32, Batch 72/157, Loss: 2.7204\n",
      "Epoch 32, Batch 73/157, Loss: 2.7399\n",
      "Epoch 32, Batch 74/157, Loss: 2.7010\n",
      "Epoch 32, Batch 75/157, Loss: 2.7725\n",
      "Epoch 32, Batch 76/157, Loss: 2.6410\n",
      "Epoch 32, Batch 77/157, Loss: 2.7535\n",
      "Epoch 32, Batch 78/157, Loss: 2.7087\n",
      "Epoch 32, Batch 79/157, Loss: 2.7241\n",
      "Epoch 32, Batch 80/157, Loss: 2.6922\n",
      "Epoch 32, Batch 81/157, Loss: 2.7667\n",
      "Epoch 32, Batch 82/157, Loss: 2.7861\n",
      "Epoch 32, Batch 83/157, Loss: 2.7342\n",
      "Epoch 32, Batch 84/157, Loss: 2.6740\n",
      "Epoch 32, Batch 85/157, Loss: 2.7276\n",
      "Epoch 32, Batch 86/157, Loss: 2.7334\n",
      "Epoch 32, Batch 87/157, Loss: 2.7112\n",
      "Epoch 32, Batch 88/157, Loss: 2.7105\n",
      "Epoch 32, Batch 89/157, Loss: 2.7333\n",
      "Epoch 32, Batch 90/157, Loss: 2.7247\n",
      "Epoch 32, Batch 91/157, Loss: 2.7490\n",
      "Epoch 32, Batch 92/157, Loss: 2.7310\n",
      "Epoch 32, Batch 93/157, Loss: 2.7480\n",
      "Epoch 32, Batch 94/157, Loss: 2.7427\n",
      "Epoch 32, Batch 95/157, Loss: 2.7333\n",
      "Epoch 32, Batch 96/157, Loss: 2.7257\n",
      "Epoch 32, Batch 97/157, Loss: 2.6887\n",
      "Epoch 32, Batch 98/157, Loss: 2.7324\n",
      "Epoch 32, Batch 99/157, Loss: 2.7349\n",
      "Epoch 32, Batch 100/157, Loss: 2.7395\n",
      "Epoch 32, Batch 101/157, Loss: 2.7415\n",
      "Epoch 32, Batch 102/157, Loss: 2.7514\n",
      "Epoch 32, Batch 103/157, Loss: 2.7213\n",
      "Epoch 32, Batch 104/157, Loss: 2.7311\n",
      "Epoch 32, Batch 105/157, Loss: 2.6842\n",
      "Epoch 32, Batch 106/157, Loss: 2.6937\n",
      "Epoch 32, Batch 107/157, Loss: 2.7559\n",
      "Epoch 32, Batch 108/157, Loss: 2.7519\n",
      "Epoch 32, Batch 109/157, Loss: 2.7345\n",
      "Epoch 32, Batch 110/157, Loss: 2.6938\n",
      "Epoch 32, Batch 111/157, Loss: 2.7289\n",
      "Epoch 32, Batch 112/157, Loss: 2.7212\n",
      "Epoch 32, Batch 113/157, Loss: 2.7247\n",
      "Epoch 32, Batch 114/157, Loss: 2.7304\n",
      "Epoch 32, Batch 115/157, Loss: 2.7181\n",
      "Epoch 32, Batch 116/157, Loss: 2.7586\n",
      "Epoch 32, Batch 117/157, Loss: 2.7610\n",
      "Epoch 32, Batch 118/157, Loss: 2.7622\n",
      "Epoch 32, Batch 119/157, Loss: 2.7072\n",
      "Epoch 32, Batch 120/157, Loss: 2.7202\n",
      "Epoch 32, Batch 121/157, Loss: 2.7293\n",
      "Epoch 32, Batch 122/157, Loss: 2.7398\n",
      "Epoch 32, Batch 123/157, Loss: 2.6654\n",
      "Epoch 32, Batch 124/157, Loss: 2.7318\n",
      "Epoch 32, Batch 125/157, Loss: 2.7544\n",
      "Epoch 32, Batch 126/157, Loss: 2.6919\n",
      "Epoch 32, Batch 127/157, Loss: 2.7135\n",
      "Epoch 32, Batch 128/157, Loss: 2.7279\n",
      "Epoch 32, Batch 129/157, Loss: 2.6575\n",
      "Epoch 32, Batch 130/157, Loss: 2.7038\n",
      "Epoch 32, Batch 131/157, Loss: 2.7068\n",
      "Epoch 32, Batch 132/157, Loss: 2.7188\n",
      "Epoch 32, Batch 133/157, Loss: 2.7946\n",
      "Epoch 32, Batch 134/157, Loss: 2.7293\n",
      "Epoch 32, Batch 135/157, Loss: 2.7322\n",
      "Epoch 32, Batch 136/157, Loss: 2.7356\n",
      "Epoch 32, Batch 137/157, Loss: 2.7192\n",
      "Epoch 32, Batch 138/157, Loss: 2.7448\n",
      "Epoch 32, Batch 139/157, Loss: 2.6964\n",
      "Epoch 32, Batch 140/157, Loss: 2.7501\n",
      "Epoch 32, Batch 141/157, Loss: 2.7423\n",
      "Epoch 32, Batch 142/157, Loss: 2.7096\n",
      "Epoch 32, Batch 143/157, Loss: 2.6964\n",
      "Epoch 32, Batch 144/157, Loss: 2.6670\n",
      "Epoch 32, Batch 145/157, Loss: 2.6771\n",
      "Epoch 32, Batch 146/157, Loss: 2.6986\n",
      "Epoch 32, Batch 147/157, Loss: 2.6795\n",
      "Epoch 32, Batch 148/157, Loss: 2.7831\n",
      "Epoch 32, Batch 149/157, Loss: 2.6580\n",
      "Epoch 32, Batch 150/157, Loss: 2.7166\n",
      "Epoch 32, Batch 151/157, Loss: 2.8214\n",
      "Epoch 32, Batch 152/157, Loss: 2.7666\n",
      "Epoch 32, Batch 153/157, Loss: 2.8075\n",
      "Epoch 32, Batch 154/157, Loss: 2.7475\n",
      "Epoch 32, Batch 155/157, Loss: 2.7186\n",
      "Epoch 32, Batch 156/157, Loss: 2.7252\n",
      "Epoch 32, Batch 157/157, Loss: 2.7767\n",
      "Epoch 32/50, Average Loss: 2.7260\n",
      "Epoch 33, Batch 1/157, Loss: 2.6805\n",
      "Epoch 33, Batch 2/157, Loss: 2.7357\n",
      "Epoch 33, Batch 3/157, Loss: 2.7369\n",
      "Epoch 33, Batch 4/157, Loss: 2.7480\n",
      "Epoch 33, Batch 5/157, Loss: 2.7141\n",
      "Epoch 33, Batch 6/157, Loss: 2.6929\n",
      "Epoch 33, Batch 7/157, Loss: 2.7440\n",
      "Epoch 33, Batch 8/157, Loss: 2.7133\n",
      "Epoch 33, Batch 9/157, Loss: 2.7277\n",
      "Epoch 33, Batch 10/157, Loss: 2.7782\n",
      "Epoch 33, Batch 11/157, Loss: 2.7032\n",
      "Epoch 33, Batch 12/157, Loss: 2.7050\n",
      "Epoch 33, Batch 13/157, Loss: 2.7041\n",
      "Epoch 33, Batch 14/157, Loss: 2.7531\n",
      "Epoch 33, Batch 15/157, Loss: 2.7447\n",
      "Epoch 33, Batch 16/157, Loss: 2.7722\n",
      "Epoch 33, Batch 17/157, Loss: 2.7514\n",
      "Epoch 33, Batch 18/157, Loss: 2.7529\n",
      "Epoch 33, Batch 19/157, Loss: 2.6995\n",
      "Epoch 33, Batch 20/157, Loss: 2.7064\n",
      "Epoch 33, Batch 21/157, Loss: 2.7531\n",
      "Epoch 33, Batch 22/157, Loss: 2.7472\n",
      "Epoch 33, Batch 23/157, Loss: 2.7322\n",
      "Epoch 33, Batch 24/157, Loss: 2.7384\n",
      "Epoch 33, Batch 25/157, Loss: 2.7362\n",
      "Epoch 33, Batch 26/157, Loss: 2.7055\n",
      "Epoch 33, Batch 27/157, Loss: 2.7499\n",
      "Epoch 33, Batch 28/157, Loss: 2.6955\n",
      "Epoch 33, Batch 29/157, Loss: 2.7281\n",
      "Epoch 33, Batch 30/157, Loss: 2.7451\n",
      "Epoch 33, Batch 31/157, Loss: 2.7118\n",
      "Epoch 33, Batch 32/157, Loss: 2.7499\n",
      "Epoch 33, Batch 33/157, Loss: 2.6753\n",
      "Epoch 33, Batch 34/157, Loss: 2.7145\n",
      "Epoch 33, Batch 35/157, Loss: 2.6863\n",
      "Epoch 33, Batch 36/157, Loss: 2.7288\n",
      "Epoch 33, Batch 37/157, Loss: 2.7322\n",
      "Epoch 33, Batch 38/157, Loss: 2.6903\n",
      "Epoch 33, Batch 39/157, Loss: 2.7379\n",
      "Epoch 33, Batch 40/157, Loss: 2.7224\n",
      "Epoch 33, Batch 41/157, Loss: 2.6753\n",
      "Epoch 33, Batch 42/157, Loss: 2.7700\n",
      "Epoch 33, Batch 43/157, Loss: 2.7208\n",
      "Epoch 33, Batch 44/157, Loss: 2.7410\n",
      "Epoch 33, Batch 45/157, Loss: 2.6808\n",
      "Epoch 33, Batch 46/157, Loss: 2.6838\n",
      "Epoch 33, Batch 47/157, Loss: 2.7265\n",
      "Epoch 33, Batch 48/157, Loss: 2.7315\n",
      "Epoch 33, Batch 49/157, Loss: 2.7396\n",
      "Epoch 33, Batch 50/157, Loss: 2.7682\n",
      "Epoch 33, Batch 51/157, Loss: 2.7274\n",
      "Epoch 33, Batch 52/157, Loss: 2.7418\n",
      "Epoch 33, Batch 53/157, Loss: 2.7421\n",
      "Epoch 33, Batch 54/157, Loss: 2.7095\n",
      "Epoch 33, Batch 55/157, Loss: 2.7201\n",
      "Epoch 33, Batch 56/157, Loss: 2.7029\n",
      "Epoch 33, Batch 57/157, Loss: 2.7286\n",
      "Epoch 33, Batch 58/157, Loss: 2.7452\n",
      "Epoch 33, Batch 59/157, Loss: 2.7370\n",
      "Epoch 33, Batch 60/157, Loss: 2.7315\n",
      "Epoch 33, Batch 61/157, Loss: 2.7509\n",
      "Epoch 33, Batch 62/157, Loss: 2.6799\n",
      "Epoch 33, Batch 63/157, Loss: 2.7234\n",
      "Epoch 33, Batch 64/157, Loss: 2.7223\n",
      "Epoch 33, Batch 65/157, Loss: 2.6772\n",
      "Epoch 33, Batch 66/157, Loss: 2.7512\n",
      "Epoch 33, Batch 67/157, Loss: 2.6902\n",
      "Epoch 33, Batch 68/157, Loss: 2.6913\n",
      "Epoch 33, Batch 69/157, Loss: 2.7377\n",
      "Epoch 33, Batch 70/157, Loss: 2.7380\n",
      "Epoch 33, Batch 71/157, Loss: 2.7279\n",
      "Epoch 33, Batch 72/157, Loss: 2.7345\n",
      "Epoch 33, Batch 73/157, Loss: 2.6961\n",
      "Epoch 33, Batch 74/157, Loss: 2.6878\n",
      "Epoch 33, Batch 75/157, Loss: 2.7527\n",
      "Epoch 33, Batch 76/157, Loss: 2.6923\n",
      "Epoch 33, Batch 77/157, Loss: 2.7831\n",
      "Epoch 33, Batch 78/157, Loss: 2.7502\n",
      "Epoch 33, Batch 79/157, Loss: 2.7267\n",
      "Epoch 33, Batch 80/157, Loss: 2.6886\n",
      "Epoch 33, Batch 81/157, Loss: 2.6906\n",
      "Epoch 33, Batch 82/157, Loss: 2.7148\n",
      "Epoch 33, Batch 83/157, Loss: 2.7252\n",
      "Epoch 33, Batch 84/157, Loss: 2.7371\n",
      "Epoch 33, Batch 85/157, Loss: 2.6842\n",
      "Epoch 33, Batch 86/157, Loss: 2.6933\n",
      "Epoch 33, Batch 87/157, Loss: 2.7086\n",
      "Epoch 33, Batch 88/157, Loss: 2.7219\n",
      "Epoch 33, Batch 89/157, Loss: 2.7108\n",
      "Epoch 33, Batch 90/157, Loss: 2.7331\n",
      "Epoch 33, Batch 91/157, Loss: 2.7568\n",
      "Epoch 33, Batch 92/157, Loss: 2.6992\n",
      "Epoch 33, Batch 93/157, Loss: 2.7480\n",
      "Epoch 33, Batch 94/157, Loss: 2.6914\n",
      "Epoch 33, Batch 95/157, Loss: 2.7261\n",
      "Epoch 33, Batch 96/157, Loss: 2.6875\n",
      "Epoch 33, Batch 97/157, Loss: 2.6719\n",
      "Epoch 33, Batch 98/157, Loss: 2.7533\n",
      "Epoch 33, Batch 99/157, Loss: 2.7419\n",
      "Epoch 33, Batch 100/157, Loss: 2.6972\n",
      "Epoch 33, Batch 101/157, Loss: 2.7520\n",
      "Epoch 33, Batch 102/157, Loss: 2.6990\n",
      "Epoch 33, Batch 103/157, Loss: 2.7744\n",
      "Epoch 33, Batch 104/157, Loss: 2.6868\n",
      "Epoch 33, Batch 105/157, Loss: 2.7025\n",
      "Epoch 33, Batch 106/157, Loss: 2.6936\n",
      "Epoch 33, Batch 107/157, Loss: 2.7435\n",
      "Epoch 33, Batch 108/157, Loss: 2.6997\n",
      "Epoch 33, Batch 109/157, Loss: 2.7818\n",
      "Epoch 33, Batch 110/157, Loss: 2.7292\n",
      "Epoch 33, Batch 111/157, Loss: 2.7647\n",
      "Epoch 33, Batch 112/157, Loss: 2.7171\n",
      "Epoch 33, Batch 113/157, Loss: 2.7097\n",
      "Epoch 33, Batch 114/157, Loss: 2.7581\n",
      "Epoch 33, Batch 115/157, Loss: 2.7233\n",
      "Epoch 33, Batch 116/157, Loss: 2.7014\n",
      "Epoch 33, Batch 117/157, Loss: 2.7432\n",
      "Epoch 33, Batch 118/157, Loss: 2.7318\n",
      "Epoch 33, Batch 119/157, Loss: 2.7021\n",
      "Epoch 33, Batch 120/157, Loss: 2.7171\n",
      "Epoch 33, Batch 121/157, Loss: 2.7048\n",
      "Epoch 33, Batch 122/157, Loss: 2.7313\n",
      "Epoch 33, Batch 123/157, Loss: 2.7047\n",
      "Epoch 33, Batch 124/157, Loss: 2.6737\n",
      "Epoch 33, Batch 125/157, Loss: 2.7578\n",
      "Epoch 33, Batch 126/157, Loss: 2.7402\n",
      "Epoch 33, Batch 127/157, Loss: 2.7013\n",
      "Epoch 33, Batch 128/157, Loss: 2.6913\n",
      "Epoch 33, Batch 129/157, Loss: 2.7292\n",
      "Epoch 33, Batch 130/157, Loss: 2.7242\n",
      "Epoch 33, Batch 131/157, Loss: 2.7118\n",
      "Epoch 33, Batch 132/157, Loss: 2.7053\n",
      "Epoch 33, Batch 133/157, Loss: 2.7177\n",
      "Epoch 33, Batch 134/157, Loss: 2.6745\n",
      "Epoch 33, Batch 135/157, Loss: 2.7337\n",
      "Epoch 33, Batch 136/157, Loss: 2.7501\n",
      "Epoch 33, Batch 137/157, Loss: 2.6696\n",
      "Epoch 33, Batch 138/157, Loss: 2.6473\n",
      "Epoch 33, Batch 139/157, Loss: 2.7189\n",
      "Epoch 33, Batch 140/157, Loss: 2.7038\n",
      "Epoch 33, Batch 141/157, Loss: 2.7279\n",
      "Epoch 33, Batch 142/157, Loss: 2.7021\n",
      "Epoch 33, Batch 143/157, Loss: 2.7225\n",
      "Epoch 33, Batch 144/157, Loss: 2.6935\n",
      "Epoch 33, Batch 145/157, Loss: 2.7340\n",
      "Epoch 33, Batch 146/157, Loss: 2.8036\n",
      "Epoch 33, Batch 147/157, Loss: 2.7538\n",
      "Epoch 33, Batch 148/157, Loss: 2.6958\n",
      "Epoch 33, Batch 149/157, Loss: 2.7147\n",
      "Epoch 33, Batch 150/157, Loss: 2.6689\n",
      "Epoch 33, Batch 151/157, Loss: 2.7236\n",
      "Epoch 33, Batch 152/157, Loss: 2.7135\n",
      "Epoch 33, Batch 153/157, Loss: 2.7063\n",
      "Epoch 33, Batch 154/157, Loss: 2.7624\n",
      "Epoch 33, Batch 155/157, Loss: 2.6813\n",
      "Epoch 33, Batch 156/157, Loss: 2.7809\n",
      "Epoch 33, Batch 157/157, Loss: 2.6671\n",
      "Epoch 33/50, Average Loss: 2.7213\n",
      "Epoch 34, Batch 1/157, Loss: 2.6776\n",
      "Epoch 34, Batch 2/157, Loss: 2.7602\n",
      "Epoch 34, Batch 3/157, Loss: 2.7029\n",
      "Epoch 34, Batch 4/157, Loss: 2.6873\n",
      "Epoch 34, Batch 5/157, Loss: 2.6988\n",
      "Epoch 34, Batch 6/157, Loss: 2.7757\n",
      "Epoch 34, Batch 7/157, Loss: 2.7298\n",
      "Epoch 34, Batch 8/157, Loss: 2.7113\n",
      "Epoch 34, Batch 9/157, Loss: 2.7139\n",
      "Epoch 34, Batch 10/157, Loss: 2.7465\n",
      "Epoch 34, Batch 11/157, Loss: 2.7587\n",
      "Epoch 34, Batch 12/157, Loss: 2.7081\n",
      "Epoch 34, Batch 13/157, Loss: 2.7497\n",
      "Epoch 34, Batch 14/157, Loss: 2.7324\n",
      "Epoch 34, Batch 15/157, Loss: 2.6632\n",
      "Epoch 34, Batch 16/157, Loss: 2.7304\n",
      "Epoch 34, Batch 17/157, Loss: 2.7548\n",
      "Epoch 34, Batch 18/157, Loss: 2.6833\n",
      "Epoch 34, Batch 19/157, Loss: 2.7783\n",
      "Epoch 34, Batch 20/157, Loss: 2.7349\n",
      "Epoch 34, Batch 21/157, Loss: 2.6961\n",
      "Epoch 34, Batch 22/157, Loss: 2.6782\n",
      "Epoch 34, Batch 23/157, Loss: 2.7664\n",
      "Epoch 34, Batch 24/157, Loss: 2.7115\n",
      "Epoch 34, Batch 25/157, Loss: 2.6973\n",
      "Epoch 34, Batch 26/157, Loss: 2.7896\n",
      "Epoch 34, Batch 27/157, Loss: 2.7490\n",
      "Epoch 34, Batch 28/157, Loss: 2.7670\n",
      "Epoch 34, Batch 29/157, Loss: 2.7485\n",
      "Epoch 34, Batch 30/157, Loss: 2.7266\n",
      "Epoch 34, Batch 31/157, Loss: 2.6696\n",
      "Epoch 34, Batch 32/157, Loss: 2.7235\n",
      "Epoch 34, Batch 33/157, Loss: 2.7026\n",
      "Epoch 34, Batch 34/157, Loss: 2.7129\n",
      "Epoch 34, Batch 35/157, Loss: 2.7187\n",
      "Epoch 34, Batch 36/157, Loss: 2.7277\n",
      "Epoch 34, Batch 37/157, Loss: 2.7230\n",
      "Epoch 34, Batch 38/157, Loss: 2.6805\n",
      "Epoch 34, Batch 39/157, Loss: 2.7071\n",
      "Epoch 34, Batch 40/157, Loss: 2.7200\n",
      "Epoch 34, Batch 41/157, Loss: 2.7258\n",
      "Epoch 34, Batch 42/157, Loss: 2.6809\n",
      "Epoch 34, Batch 43/157, Loss: 2.7253\n",
      "Epoch 34, Batch 44/157, Loss: 2.7173\n",
      "Epoch 34, Batch 45/157, Loss: 2.7099\n",
      "Epoch 34, Batch 46/157, Loss: 2.7013\n",
      "Epoch 34, Batch 47/157, Loss: 2.6979\n",
      "Epoch 34, Batch 48/157, Loss: 2.7474\n",
      "Epoch 34, Batch 49/157, Loss: 2.7043\n",
      "Epoch 34, Batch 50/157, Loss: 2.6630\n",
      "Epoch 34, Batch 51/157, Loss: 2.6868\n",
      "Epoch 34, Batch 52/157, Loss: 2.7427\n",
      "Epoch 34, Batch 53/157, Loss: 2.7471\n",
      "Epoch 34, Batch 54/157, Loss: 2.7621\n",
      "Epoch 34, Batch 55/157, Loss: 2.7366\n",
      "Epoch 34, Batch 56/157, Loss: 2.6895\n",
      "Epoch 34, Batch 57/157, Loss: 2.7216\n",
      "Epoch 34, Batch 58/157, Loss: 2.7032\n",
      "Epoch 34, Batch 59/157, Loss: 2.7801\n",
      "Epoch 34, Batch 60/157, Loss: 2.7364\n",
      "Epoch 34, Batch 61/157, Loss: 2.7219\n",
      "Epoch 34, Batch 62/157, Loss: 2.7572\n",
      "Epoch 34, Batch 63/157, Loss: 2.7450\n",
      "Epoch 34, Batch 64/157, Loss: 2.7638\n",
      "Epoch 34, Batch 65/157, Loss: 2.7450\n",
      "Epoch 34, Batch 66/157, Loss: 2.7392\n",
      "Epoch 34, Batch 67/157, Loss: 2.7100\n",
      "Epoch 34, Batch 68/157, Loss: 2.7094\n",
      "Epoch 34, Batch 69/157, Loss: 2.6974\n",
      "Epoch 34, Batch 70/157, Loss: 2.7878\n",
      "Epoch 34, Batch 71/157, Loss: 2.6974\n",
      "Epoch 34, Batch 72/157, Loss: 2.7563\n",
      "Epoch 34, Batch 73/157, Loss: 2.7615\n",
      "Epoch 34, Batch 74/157, Loss: 2.6874\n",
      "Epoch 34, Batch 75/157, Loss: 2.7306\n",
      "Epoch 34, Batch 76/157, Loss: 2.7359\n",
      "Epoch 34, Batch 77/157, Loss: 2.7069\n",
      "Epoch 34, Batch 78/157, Loss: 2.7322\n",
      "Epoch 34, Batch 79/157, Loss: 2.7222\n",
      "Epoch 34, Batch 80/157, Loss: 2.7215\n",
      "Epoch 34, Batch 81/157, Loss: 2.7379\n",
      "Epoch 34, Batch 82/157, Loss: 2.7342\n",
      "Epoch 34, Batch 83/157, Loss: 2.6950\n",
      "Epoch 34, Batch 84/157, Loss: 2.7072\n",
      "Epoch 34, Batch 85/157, Loss: 2.7515\n",
      "Epoch 34, Batch 86/157, Loss: 2.6862\n",
      "Epoch 34, Batch 87/157, Loss: 2.7210\n",
      "Epoch 34, Batch 88/157, Loss: 2.7259\n",
      "Epoch 34, Batch 89/157, Loss: 2.6722\n",
      "Epoch 34, Batch 90/157, Loss: 2.7656\n",
      "Epoch 34, Batch 91/157, Loss: 2.7223\n",
      "Epoch 34, Batch 92/157, Loss: 2.7276\n",
      "Epoch 34, Batch 93/157, Loss: 2.7422\n",
      "Epoch 34, Batch 94/157, Loss: 2.6671\n",
      "Epoch 34, Batch 95/157, Loss: 2.7634\n",
      "Epoch 34, Batch 96/157, Loss: 2.7335\n",
      "Epoch 34, Batch 97/157, Loss: 2.6874\n",
      "Epoch 34, Batch 98/157, Loss: 2.7432\n",
      "Epoch 34, Batch 99/157, Loss: 2.6929\n",
      "Epoch 34, Batch 100/157, Loss: 2.7467\n",
      "Epoch 34, Batch 101/157, Loss: 2.7411\n",
      "Epoch 34, Batch 102/157, Loss: 2.7062\n",
      "Epoch 34, Batch 103/157, Loss: 2.7242\n",
      "Epoch 34, Batch 104/157, Loss: 2.7485\n",
      "Epoch 34, Batch 105/157, Loss: 2.7025\n",
      "Epoch 34, Batch 106/157, Loss: 2.7338\n",
      "Epoch 34, Batch 107/157, Loss: 2.6982\n",
      "Epoch 34, Batch 108/157, Loss: 2.7257\n",
      "Epoch 34, Batch 109/157, Loss: 2.7133\n",
      "Epoch 34, Batch 110/157, Loss: 2.7309\n",
      "Epoch 34, Batch 111/157, Loss: 2.6894\n",
      "Epoch 34, Batch 112/157, Loss: 2.6997\n",
      "Epoch 34, Batch 113/157, Loss: 2.6917\n",
      "Epoch 34, Batch 114/157, Loss: 2.7370\n",
      "Epoch 34, Batch 115/157, Loss: 2.7171\n",
      "Epoch 34, Batch 116/157, Loss: 2.7607\n",
      "Epoch 34, Batch 117/157, Loss: 2.7348\n",
      "Epoch 34, Batch 118/157, Loss: 2.7015\n",
      "Epoch 34, Batch 119/157, Loss: 2.7286\n",
      "Epoch 34, Batch 120/157, Loss: 2.7167\n",
      "Epoch 34, Batch 121/157, Loss: 2.7293\n",
      "Epoch 34, Batch 122/157, Loss: 2.6985\n",
      "Epoch 34, Batch 123/157, Loss: 2.7028\n",
      "Epoch 34, Batch 124/157, Loss: 2.7791\n",
      "Epoch 34, Batch 125/157, Loss: 2.7385\n",
      "Epoch 34, Batch 126/157, Loss: 2.7498\n",
      "Epoch 34, Batch 127/157, Loss: 2.6911\n",
      "Epoch 34, Batch 128/157, Loss: 2.7222\n",
      "Epoch 34, Batch 129/157, Loss: 2.7090\n",
      "Epoch 34, Batch 130/157, Loss: 2.7056\n",
      "Epoch 34, Batch 131/157, Loss: 2.7223\n",
      "Epoch 34, Batch 132/157, Loss: 2.7051\n",
      "Epoch 34, Batch 133/157, Loss: 2.6748\n",
      "Epoch 34, Batch 134/157, Loss: 2.7144\n",
      "Epoch 34, Batch 135/157, Loss: 2.7100\n",
      "Epoch 34, Batch 136/157, Loss: 2.6849\n",
      "Epoch 34, Batch 137/157, Loss: 2.6913\n",
      "Epoch 34, Batch 138/157, Loss: 2.6923\n",
      "Epoch 34, Batch 139/157, Loss: 2.6793\n",
      "Epoch 34, Batch 140/157, Loss: 2.7038\n",
      "Epoch 34, Batch 141/157, Loss: 2.7206\n",
      "Epoch 34, Batch 142/157, Loss: 2.7007\n",
      "Epoch 34, Batch 143/157, Loss: 2.6824\n",
      "Epoch 34, Batch 144/157, Loss: 2.6997\n",
      "Epoch 34, Batch 145/157, Loss: 2.7527\n",
      "Epoch 34, Batch 146/157, Loss: 2.7542\n",
      "Epoch 34, Batch 147/157, Loss: 2.7314\n",
      "Epoch 34, Batch 148/157, Loss: 2.6750\n",
      "Epoch 34, Batch 149/157, Loss: 2.7387\n",
      "Epoch 34, Batch 150/157, Loss: 2.6940\n",
      "Epoch 34, Batch 151/157, Loss: 2.7104\n",
      "Epoch 34, Batch 152/157, Loss: 2.7525\n",
      "Epoch 34, Batch 153/157, Loss: 2.6945\n",
      "Epoch 34, Batch 154/157, Loss: 2.7229\n",
      "Epoch 34, Batch 155/157, Loss: 2.7056\n",
      "Epoch 34, Batch 156/157, Loss: 2.7034\n",
      "Epoch 34, Batch 157/157, Loss: 2.6375\n",
      "Epoch 34/50, Average Loss: 2.7199\n",
      "Epoch 35, Batch 1/157, Loss: 2.7167\n",
      "Epoch 35, Batch 2/157, Loss: 2.6819\n",
      "Epoch 35, Batch 3/157, Loss: 2.7400\n",
      "Epoch 35, Batch 4/157, Loss: 2.7751\n",
      "Epoch 35, Batch 5/157, Loss: 2.6382\n",
      "Epoch 35, Batch 6/157, Loss: 2.6571\n",
      "Epoch 35, Batch 7/157, Loss: 2.7840\n",
      "Epoch 35, Batch 8/157, Loss: 2.7176\n",
      "Epoch 35, Batch 9/157, Loss: 2.7243\n",
      "Epoch 35, Batch 10/157, Loss: 2.6715\n",
      "Epoch 35, Batch 11/157, Loss: 2.7017\n",
      "Epoch 35, Batch 12/157, Loss: 2.7216\n",
      "Epoch 35, Batch 13/157, Loss: 2.7022\n",
      "Epoch 35, Batch 14/157, Loss: 2.7302\n",
      "Epoch 35, Batch 15/157, Loss: 2.7857\n",
      "Epoch 35, Batch 16/157, Loss: 2.6702\n",
      "Epoch 35, Batch 17/157, Loss: 2.6390\n",
      "Epoch 35, Batch 18/157, Loss: 2.8156\n",
      "Epoch 35, Batch 19/157, Loss: 2.7411\n",
      "Epoch 35, Batch 20/157, Loss: 2.7150\n",
      "Epoch 35, Batch 21/157, Loss: 2.7236\n",
      "Epoch 35, Batch 22/157, Loss: 2.7541\n",
      "Epoch 35, Batch 23/157, Loss: 2.6948\n",
      "Epoch 35, Batch 24/157, Loss: 2.7826\n",
      "Epoch 35, Batch 25/157, Loss: 2.7268\n",
      "Epoch 35, Batch 26/157, Loss: 2.7335\n",
      "Epoch 35, Batch 27/157, Loss: 2.7218\n",
      "Epoch 35, Batch 28/157, Loss: 2.7213\n",
      "Epoch 35, Batch 29/157, Loss: 2.7371\n",
      "Epoch 35, Batch 30/157, Loss: 2.7095\n",
      "Epoch 35, Batch 31/157, Loss: 2.6915\n",
      "Epoch 35, Batch 32/157, Loss: 2.6941\n",
      "Epoch 35, Batch 33/157, Loss: 2.7447\n",
      "Epoch 35, Batch 34/157, Loss: 2.7027\n",
      "Epoch 35, Batch 35/157, Loss: 2.7083\n",
      "Epoch 35, Batch 36/157, Loss: 2.7168\n",
      "Epoch 35, Batch 37/157, Loss: 2.6833\n",
      "Epoch 35, Batch 38/157, Loss: 2.7098\n",
      "Epoch 35, Batch 39/157, Loss: 2.6862\n",
      "Epoch 35, Batch 40/157, Loss: 2.7291\n",
      "Epoch 35, Batch 41/157, Loss: 2.7103\n",
      "Epoch 35, Batch 42/157, Loss: 2.7307\n",
      "Epoch 35, Batch 43/157, Loss: 2.7015\n",
      "Epoch 35, Batch 44/157, Loss: 2.7575\n",
      "Epoch 35, Batch 45/157, Loss: 2.6986\n",
      "Epoch 35, Batch 46/157, Loss: 2.7190\n",
      "Epoch 35, Batch 47/157, Loss: 2.7185\n",
      "Epoch 35, Batch 48/157, Loss: 2.6885\n",
      "Epoch 35, Batch 49/157, Loss: 2.7507\n",
      "Epoch 35, Batch 50/157, Loss: 2.7285\n",
      "Epoch 35, Batch 51/157, Loss: 2.7317\n",
      "Epoch 35, Batch 52/157, Loss: 2.7369\n",
      "Epoch 35, Batch 53/157, Loss: 2.6922\n",
      "Epoch 35, Batch 54/157, Loss: 2.6816\n",
      "Epoch 35, Batch 55/157, Loss: 2.7342\n",
      "Epoch 35, Batch 56/157, Loss: 2.7333\n",
      "Epoch 35, Batch 57/157, Loss: 2.7071\n",
      "Epoch 35, Batch 58/157, Loss: 2.7297\n",
      "Epoch 35, Batch 59/157, Loss: 2.7185\n",
      "Epoch 35, Batch 60/157, Loss: 2.7055\n",
      "Epoch 35, Batch 61/157, Loss: 2.7338\n",
      "Epoch 35, Batch 62/157, Loss: 2.6981\n",
      "Epoch 35, Batch 63/157, Loss: 2.7359\n",
      "Epoch 35, Batch 64/157, Loss: 2.7193\n",
      "Epoch 35, Batch 65/157, Loss: 2.7342\n",
      "Epoch 35, Batch 66/157, Loss: 2.7645\n",
      "Epoch 35, Batch 67/157, Loss: 2.7099\n",
      "Epoch 35, Batch 68/157, Loss: 2.6821\n",
      "Epoch 35, Batch 69/157, Loss: 2.7117\n",
      "Epoch 35, Batch 70/157, Loss: 2.7311\n",
      "Epoch 35, Batch 71/157, Loss: 2.7064\n",
      "Epoch 35, Batch 72/157, Loss: 2.7420\n",
      "Epoch 35, Batch 73/157, Loss: 2.7142\n",
      "Epoch 35, Batch 74/157, Loss: 2.7167\n",
      "Epoch 35, Batch 75/157, Loss: 2.6808\n",
      "Epoch 35, Batch 76/157, Loss: 2.7351\n",
      "Epoch 35, Batch 77/157, Loss: 2.7015\n",
      "Epoch 35, Batch 78/157, Loss: 2.7062\n",
      "Epoch 35, Batch 79/157, Loss: 2.6945\n",
      "Epoch 35, Batch 80/157, Loss: 2.7114\n",
      "Epoch 35, Batch 81/157, Loss: 2.7353\n",
      "Epoch 35, Batch 82/157, Loss: 2.7441\n",
      "Epoch 35, Batch 83/157, Loss: 2.7090\n",
      "Epoch 35, Batch 84/157, Loss: 2.7077\n",
      "Epoch 35, Batch 85/157, Loss: 2.7028\n",
      "Epoch 35, Batch 86/157, Loss: 2.6659\n",
      "Epoch 35, Batch 87/157, Loss: 2.7435\n",
      "Epoch 35, Batch 88/157, Loss: 2.7522\n",
      "Epoch 35, Batch 89/157, Loss: 2.6547\n",
      "Epoch 35, Batch 90/157, Loss: 2.7299\n",
      "Epoch 35, Batch 91/157, Loss: 2.6454\n",
      "Epoch 35, Batch 92/157, Loss: 2.6888\n",
      "Epoch 35, Batch 93/157, Loss: 2.7516\n",
      "Epoch 35, Batch 94/157, Loss: 2.7017\n",
      "Epoch 35, Batch 95/157, Loss: 2.7815\n",
      "Epoch 35, Batch 96/157, Loss: 2.6830\n",
      "Epoch 35, Batch 97/157, Loss: 2.7384\n",
      "Epoch 35, Batch 98/157, Loss: 2.6539\n",
      "Epoch 35, Batch 99/157, Loss: 2.7663\n",
      "Epoch 35, Batch 100/157, Loss: 2.7536\n",
      "Epoch 35, Batch 101/157, Loss: 2.7121\n",
      "Epoch 35, Batch 102/157, Loss: 2.7503\n",
      "Epoch 35, Batch 103/157, Loss: 2.6676\n",
      "Epoch 35, Batch 104/157, Loss: 2.7658\n",
      "Epoch 35, Batch 105/157, Loss: 2.7529\n",
      "Epoch 35, Batch 106/157, Loss: 2.6960\n",
      "Epoch 35, Batch 107/157, Loss: 2.6898\n",
      "Epoch 35, Batch 108/157, Loss: 2.7211\n",
      "Epoch 35, Batch 109/157, Loss: 2.7276\n",
      "Epoch 35, Batch 110/157, Loss: 2.6891\n",
      "Epoch 35, Batch 111/157, Loss: 2.7629\n",
      "Epoch 35, Batch 112/157, Loss: 2.7152\n",
      "Epoch 35, Batch 113/157, Loss: 2.7354\n",
      "Epoch 35, Batch 114/157, Loss: 2.7105\n",
      "Epoch 35, Batch 115/157, Loss: 2.7378\n",
      "Epoch 35, Batch 116/157, Loss: 2.7089\n",
      "Epoch 35, Batch 117/157, Loss: 2.7852\n",
      "Epoch 35, Batch 118/157, Loss: 2.7445\n",
      "Epoch 35, Batch 119/157, Loss: 2.7296\n",
      "Epoch 35, Batch 120/157, Loss: 2.7289\n",
      "Epoch 35, Batch 121/157, Loss: 2.7189\n",
      "Epoch 35, Batch 122/157, Loss: 2.7138\n",
      "Epoch 35, Batch 123/157, Loss: 2.6951\n",
      "Epoch 35, Batch 124/157, Loss: 2.7217\n",
      "Epoch 35, Batch 125/157, Loss: 2.6826\n",
      "Epoch 35, Batch 126/157, Loss: 2.7654\n",
      "Epoch 35, Batch 127/157, Loss: 2.7277\n",
      "Epoch 35, Batch 128/157, Loss: 2.7378\n",
      "Epoch 35, Batch 129/157, Loss: 2.7039\n",
      "Epoch 35, Batch 130/157, Loss: 2.7142\n",
      "Epoch 35, Batch 131/157, Loss: 2.7352\n",
      "Epoch 35, Batch 132/157, Loss: 2.7200\n",
      "Epoch 35, Batch 133/157, Loss: 2.7188\n",
      "Epoch 35, Batch 134/157, Loss: 2.7342\n",
      "Epoch 35, Batch 135/157, Loss: 2.7243\n",
      "Epoch 35, Batch 136/157, Loss: 2.7328\n",
      "Epoch 35, Batch 137/157, Loss: 2.7194\n",
      "Epoch 35, Batch 138/157, Loss: 2.7216\n",
      "Epoch 35, Batch 139/157, Loss: 2.7412\n",
      "Epoch 35, Batch 140/157, Loss: 2.6995\n",
      "Epoch 35, Batch 141/157, Loss: 2.6672\n",
      "Epoch 35, Batch 142/157, Loss: 2.7150\n",
      "Epoch 35, Batch 143/157, Loss: 2.6687\n",
      "Epoch 35, Batch 144/157, Loss: 2.7161\n",
      "Epoch 35, Batch 145/157, Loss: 2.7104\n",
      "Epoch 35, Batch 146/157, Loss: 2.7123\n",
      "Epoch 35, Batch 147/157, Loss: 2.7789\n",
      "Epoch 35, Batch 148/157, Loss: 2.6932\n",
      "Epoch 35, Batch 149/157, Loss: 2.7137\n",
      "Epoch 35, Batch 150/157, Loss: 2.7231\n",
      "Epoch 35, Batch 151/157, Loss: 2.7321\n",
      "Epoch 35, Batch 152/157, Loss: 2.6937\n",
      "Epoch 35, Batch 153/157, Loss: 2.6942\n",
      "Epoch 35, Batch 154/157, Loss: 2.7421\n",
      "Epoch 35, Batch 155/157, Loss: 2.6927\n",
      "Epoch 35, Batch 156/157, Loss: 2.7210\n",
      "Epoch 35, Batch 157/157, Loss: 2.7213\n",
      "Epoch 35/50, Average Loss: 2.7183\n",
      "Epoch 36, Batch 1/157, Loss: 2.8015\n",
      "Epoch 36, Batch 2/157, Loss: 2.7323\n",
      "Epoch 36, Batch 3/157, Loss: 2.7313\n",
      "Epoch 36, Batch 4/157, Loss: 2.7354\n",
      "Epoch 36, Batch 5/157, Loss: 2.7457\n",
      "Epoch 36, Batch 6/157, Loss: 2.7396\n",
      "Epoch 36, Batch 7/157, Loss: 2.7349\n",
      "Epoch 36, Batch 8/157, Loss: 2.7246\n",
      "Epoch 36, Batch 9/157, Loss: 2.7248\n",
      "Epoch 36, Batch 10/157, Loss: 2.7202\n",
      "Epoch 36, Batch 11/157, Loss: 2.7487\n",
      "Epoch 36, Batch 12/157, Loss: 2.7228\n",
      "Epoch 36, Batch 13/157, Loss: 2.7569\n",
      "Epoch 36, Batch 14/157, Loss: 2.7379\n",
      "Epoch 36, Batch 15/157, Loss: 2.6867\n",
      "Epoch 36, Batch 16/157, Loss: 2.7494\n",
      "Epoch 36, Batch 17/157, Loss: 2.7046\n",
      "Epoch 36, Batch 18/157, Loss: 2.7409\n",
      "Epoch 36, Batch 19/157, Loss: 2.7123\n",
      "Epoch 36, Batch 20/157, Loss: 2.6880\n",
      "Epoch 36, Batch 21/157, Loss: 2.7194\n",
      "Epoch 36, Batch 22/157, Loss: 2.7014\n",
      "Epoch 36, Batch 23/157, Loss: 2.7265\n",
      "Epoch 36, Batch 24/157, Loss: 2.7076\n",
      "Epoch 36, Batch 25/157, Loss: 2.6997\n",
      "Epoch 36, Batch 26/157, Loss: 2.7122\n",
      "Epoch 36, Batch 27/157, Loss: 2.7568\n",
      "Epoch 36, Batch 28/157, Loss: 2.7382\n",
      "Epoch 36, Batch 29/157, Loss: 2.6737\n",
      "Epoch 36, Batch 30/157, Loss: 2.6850\n",
      "Epoch 36, Batch 31/157, Loss: 2.7059\n",
      "Epoch 36, Batch 32/157, Loss: 2.7625\n",
      "Epoch 36, Batch 33/157, Loss: 2.6644\n",
      "Epoch 36, Batch 34/157, Loss: 2.7574\n",
      "Epoch 36, Batch 35/157, Loss: 2.7509\n",
      "Epoch 36, Batch 36/157, Loss: 2.7092\n",
      "Epoch 36, Batch 37/157, Loss: 2.7693\n",
      "Epoch 36, Batch 38/157, Loss: 2.6981\n",
      "Epoch 36, Batch 39/157, Loss: 2.7837\n",
      "Epoch 36, Batch 40/157, Loss: 2.6750\n",
      "Epoch 36, Batch 41/157, Loss: 2.7300\n",
      "Epoch 36, Batch 42/157, Loss: 2.7705\n",
      "Epoch 36, Batch 43/157, Loss: 2.6752\n",
      "Epoch 36, Batch 44/157, Loss: 2.7204\n",
      "Epoch 36, Batch 45/157, Loss: 2.7037\n",
      "Epoch 36, Batch 46/157, Loss: 2.7259\n",
      "Epoch 36, Batch 47/157, Loss: 2.6604\n",
      "Epoch 36, Batch 48/157, Loss: 2.7141\n",
      "Epoch 36, Batch 49/157, Loss: 2.7225\n",
      "Epoch 36, Batch 50/157, Loss: 2.6992\n",
      "Epoch 36, Batch 51/157, Loss: 2.7124\n",
      "Epoch 36, Batch 52/157, Loss: 2.7739\n",
      "Epoch 36, Batch 53/157, Loss: 2.7589\n",
      "Epoch 36, Batch 54/157, Loss: 2.7523\n",
      "Epoch 36, Batch 55/157, Loss: 2.7318\n",
      "Epoch 36, Batch 56/157, Loss: 2.6973\n",
      "Epoch 36, Batch 57/157, Loss: 2.7257\n",
      "Epoch 36, Batch 58/157, Loss: 2.7007\n",
      "Epoch 36, Batch 59/157, Loss: 2.7415\n",
      "Epoch 36, Batch 60/157, Loss: 2.7353\n",
      "Epoch 36, Batch 61/157, Loss: 2.7238\n",
      "Epoch 36, Batch 62/157, Loss: 2.6999\n",
      "Epoch 36, Batch 63/157, Loss: 2.7341\n",
      "Epoch 36, Batch 64/157, Loss: 2.7412\n",
      "Epoch 36, Batch 65/157, Loss: 2.7382\n",
      "Epoch 36, Batch 66/157, Loss: 2.7199\n",
      "Epoch 36, Batch 67/157, Loss: 2.7833\n",
      "Epoch 36, Batch 68/157, Loss: 2.6691\n",
      "Epoch 36, Batch 69/157, Loss: 2.7832\n",
      "Epoch 36, Batch 70/157, Loss: 2.7384\n",
      "Epoch 36, Batch 71/157, Loss: 2.7260\n",
      "Epoch 36, Batch 72/157, Loss: 2.7242\n",
      "Epoch 36, Batch 73/157, Loss: 2.7233\n",
      "Epoch 36, Batch 74/157, Loss: 2.7487\n",
      "Epoch 36, Batch 75/157, Loss: 2.7450\n",
      "Epoch 36, Batch 76/157, Loss: 2.7219\n",
      "Epoch 36, Batch 77/157, Loss: 2.7233\n",
      "Epoch 36, Batch 78/157, Loss: 2.7283\n",
      "Epoch 36, Batch 79/157, Loss: 2.7301\n",
      "Epoch 36, Batch 80/157, Loss: 2.7303\n",
      "Epoch 36, Batch 81/157, Loss: 2.7198\n",
      "Epoch 36, Batch 82/157, Loss: 2.7382\n",
      "Epoch 36, Batch 83/157, Loss: 2.7281\n",
      "Epoch 36, Batch 84/157, Loss: 2.7080\n",
      "Epoch 36, Batch 85/157, Loss: 2.7189\n",
      "Epoch 36, Batch 86/157, Loss: 2.7493\n",
      "Epoch 36, Batch 87/157, Loss: 2.7227\n",
      "Epoch 36, Batch 88/157, Loss: 2.6845\n",
      "Epoch 36, Batch 89/157, Loss: 2.7331\n",
      "Epoch 36, Batch 90/157, Loss: 2.7152\n",
      "Epoch 36, Batch 91/157, Loss: 2.6871\n",
      "Epoch 36, Batch 92/157, Loss: 2.6839\n",
      "Epoch 36, Batch 93/157, Loss: 2.7192\n",
      "Epoch 36, Batch 94/157, Loss: 2.7264\n",
      "Epoch 36, Batch 95/157, Loss: 2.6971\n",
      "Epoch 36, Batch 96/157, Loss: 2.7298\n",
      "Epoch 36, Batch 97/157, Loss: 2.6914\n",
      "Epoch 36, Batch 98/157, Loss: 2.7151\n",
      "Epoch 36, Batch 99/157, Loss: 2.7251\n",
      "Epoch 36, Batch 100/157, Loss: 2.7131\n",
      "Epoch 36, Batch 101/157, Loss: 2.7120\n",
      "Epoch 36, Batch 102/157, Loss: 2.7671\n",
      "Epoch 36, Batch 103/157, Loss: 2.6769\n",
      "Epoch 36, Batch 104/157, Loss: 2.6846\n",
      "Epoch 36, Batch 105/157, Loss: 2.7363\n",
      "Epoch 36, Batch 106/157, Loss: 2.7390\n",
      "Epoch 36, Batch 107/157, Loss: 2.7213\n",
      "Epoch 36, Batch 108/157, Loss: 2.6841\n",
      "Epoch 36, Batch 109/157, Loss: 2.7356\n",
      "Epoch 36, Batch 110/157, Loss: 2.7223\n",
      "Epoch 36, Batch 111/157, Loss: 2.7119\n",
      "Epoch 36, Batch 112/157, Loss: 2.7313\n",
      "Epoch 36, Batch 113/157, Loss: 2.7074\n",
      "Epoch 36, Batch 114/157, Loss: 2.7154\n",
      "Epoch 36, Batch 115/157, Loss: 2.7364\n",
      "Epoch 36, Batch 116/157, Loss: 2.6913\n",
      "Epoch 36, Batch 117/157, Loss: 2.6833\n",
      "Epoch 36, Batch 118/157, Loss: 2.6789\n",
      "Epoch 36, Batch 119/157, Loss: 2.7571\n",
      "Epoch 36, Batch 120/157, Loss: 2.7023\n",
      "Epoch 36, Batch 121/157, Loss: 2.7492\n",
      "Epoch 36, Batch 122/157, Loss: 2.7032\n",
      "Epoch 36, Batch 123/157, Loss: 2.6809\n",
      "Epoch 36, Batch 124/157, Loss: 2.7578\n",
      "Epoch 36, Batch 125/157, Loss: 2.7703\n",
      "Epoch 36, Batch 126/157, Loss: 2.7543\n",
      "Epoch 36, Batch 127/157, Loss: 2.7039\n",
      "Epoch 36, Batch 128/157, Loss: 2.7452\n",
      "Epoch 36, Batch 129/157, Loss: 2.7102\n",
      "Epoch 36, Batch 130/157, Loss: 2.7344\n",
      "Epoch 36, Batch 131/157, Loss: 2.7788\n",
      "Epoch 36, Batch 132/157, Loss: 2.7114\n",
      "Epoch 36, Batch 133/157, Loss: 2.7561\n",
      "Epoch 36, Batch 134/157, Loss: 2.7025\n",
      "Epoch 36, Batch 135/157, Loss: 2.7158\n",
      "Epoch 36, Batch 136/157, Loss: 2.7457\n",
      "Epoch 36, Batch 137/157, Loss: 2.7433\n",
      "Epoch 36, Batch 138/157, Loss: 2.7242\n",
      "Epoch 36, Batch 139/157, Loss: 2.7261\n",
      "Epoch 36, Batch 140/157, Loss: 2.7364\n",
      "Epoch 36, Batch 141/157, Loss: 2.7091\n",
      "Epoch 36, Batch 142/157, Loss: 2.7233\n",
      "Epoch 36, Batch 143/157, Loss: 2.7042\n",
      "Epoch 36, Batch 144/157, Loss: 2.6881\n",
      "Epoch 36, Batch 145/157, Loss: 2.7426\n",
      "Epoch 36, Batch 146/157, Loss: 2.7324\n",
      "Epoch 36, Batch 147/157, Loss: 2.7135\n",
      "Epoch 36, Batch 148/157, Loss: 2.7194\n",
      "Epoch 36, Batch 149/157, Loss: 2.6947\n",
      "Epoch 36, Batch 150/157, Loss: 2.7272\n",
      "Epoch 36, Batch 151/157, Loss: 2.7714\n",
      "Epoch 36, Batch 152/157, Loss: 2.6987\n",
      "Epoch 36, Batch 153/157, Loss: 2.7478\n",
      "Epoch 36, Batch 154/157, Loss: 2.6954\n",
      "Epoch 36, Batch 155/157, Loss: 2.7212\n",
      "Epoch 36, Batch 156/157, Loss: 2.6875\n",
      "Epoch 36, Batch 157/157, Loss: 2.7568\n",
      "Epoch 36/50, Average Loss: 2.7236\n",
      "Epoch 37, Batch 1/157, Loss: 2.7078\n",
      "Epoch 37, Batch 2/157, Loss: 2.7364\n",
      "Epoch 37, Batch 3/157, Loss: 2.7065\n",
      "Epoch 37, Batch 4/157, Loss: 2.6989\n",
      "Epoch 37, Batch 5/157, Loss: 2.7138\n",
      "Epoch 37, Batch 6/157, Loss: 2.7460\n",
      "Epoch 37, Batch 7/157, Loss: 2.6816\n",
      "Epoch 37, Batch 8/157, Loss: 2.7044\n",
      "Epoch 37, Batch 9/157, Loss: 2.7045\n",
      "Epoch 37, Batch 10/157, Loss: 2.7143\n",
      "Epoch 37, Batch 11/157, Loss: 2.7448\n",
      "Epoch 37, Batch 12/157, Loss: 2.7022\n",
      "Epoch 37, Batch 13/157, Loss: 2.6966\n",
      "Epoch 37, Batch 14/157, Loss: 2.6730\n",
      "Epoch 37, Batch 15/157, Loss: 2.7470\n",
      "Epoch 37, Batch 16/157, Loss: 2.6923\n",
      "Epoch 37, Batch 17/157, Loss: 2.7314\n",
      "Epoch 37, Batch 18/157, Loss: 2.7519\n",
      "Epoch 37, Batch 19/157, Loss: 2.7403\n",
      "Epoch 37, Batch 20/157, Loss: 2.7376\n",
      "Epoch 37, Batch 21/157, Loss: 2.6804\n",
      "Epoch 37, Batch 22/157, Loss: 2.7155\n",
      "Epoch 37, Batch 23/157, Loss: 2.7236\n",
      "Epoch 37, Batch 24/157, Loss: 2.7270\n",
      "Epoch 37, Batch 25/157, Loss: 2.7349\n",
      "Epoch 37, Batch 26/157, Loss: 2.7345\n",
      "Epoch 37, Batch 27/157, Loss: 2.6864\n",
      "Epoch 37, Batch 28/157, Loss: 2.6845\n",
      "Epoch 37, Batch 29/157, Loss: 2.7607\n",
      "Epoch 37, Batch 30/157, Loss: 2.7146\n",
      "Epoch 37, Batch 31/157, Loss: 2.6513\n",
      "Epoch 37, Batch 32/157, Loss: 2.7192\n",
      "Epoch 37, Batch 33/157, Loss: 2.7202\n",
      "Epoch 37, Batch 34/157, Loss: 2.7007\n",
      "Epoch 37, Batch 35/157, Loss: 2.7112\n",
      "Epoch 37, Batch 36/157, Loss: 2.7434\n",
      "Epoch 37, Batch 37/157, Loss: 2.6978\n",
      "Epoch 37, Batch 38/157, Loss: 2.7420\n",
      "Epoch 37, Batch 39/157, Loss: 2.7276\n",
      "Epoch 37, Batch 40/157, Loss: 2.7590\n",
      "Epoch 37, Batch 41/157, Loss: 2.7073\n",
      "Epoch 37, Batch 42/157, Loss: 2.7652\n",
      "Epoch 37, Batch 43/157, Loss: 2.7184\n",
      "Epoch 37, Batch 44/157, Loss: 2.7157\n",
      "Epoch 37, Batch 45/157, Loss: 2.7189\n",
      "Epoch 37, Batch 46/157, Loss: 2.7109\n",
      "Epoch 37, Batch 47/157, Loss: 2.7482\n",
      "Epoch 37, Batch 48/157, Loss: 2.7189\n",
      "Epoch 37, Batch 49/157, Loss: 2.6942\n",
      "Epoch 37, Batch 50/157, Loss: 2.6774\n",
      "Epoch 37, Batch 51/157, Loss: 2.6636\n",
      "Epoch 37, Batch 52/157, Loss: 2.7315\n",
      "Epoch 37, Batch 53/157, Loss: 2.7135\n",
      "Epoch 37, Batch 54/157, Loss: 2.7367\n",
      "Epoch 37, Batch 55/157, Loss: 2.7440\n",
      "Epoch 37, Batch 56/157, Loss: 2.7213\n",
      "Epoch 37, Batch 57/157, Loss: 2.7033\n",
      "Epoch 37, Batch 58/157, Loss: 2.7050\n",
      "Epoch 37, Batch 59/157, Loss: 2.7087\n",
      "Epoch 37, Batch 60/157, Loss: 2.6950\n",
      "Epoch 37, Batch 61/157, Loss: 2.7031\n",
      "Epoch 37, Batch 62/157, Loss: 2.7289\n",
      "Epoch 37, Batch 63/157, Loss: 2.7446\n",
      "Epoch 37, Batch 64/157, Loss: 2.7112\n",
      "Epoch 37, Batch 65/157, Loss: 2.7001\n",
      "Epoch 37, Batch 66/157, Loss: 2.7411\n",
      "Epoch 37, Batch 67/157, Loss: 2.7586\n",
      "Epoch 37, Batch 68/157, Loss: 2.6763\n",
      "Epoch 37, Batch 69/157, Loss: 2.7335\n",
      "Epoch 37, Batch 70/157, Loss: 2.7444\n",
      "Epoch 37, Batch 71/157, Loss: 2.7336\n",
      "Epoch 37, Batch 72/157, Loss: 2.6755\n",
      "Epoch 37, Batch 73/157, Loss: 2.7141\n",
      "Epoch 37, Batch 74/157, Loss: 2.7447\n",
      "Epoch 37, Batch 75/157, Loss: 2.7397\n",
      "Epoch 37, Batch 76/157, Loss: 2.7298\n",
      "Epoch 37, Batch 77/157, Loss: 2.6793\n",
      "Epoch 37, Batch 78/157, Loss: 2.7556\n",
      "Epoch 37, Batch 79/157, Loss: 2.7264\n",
      "Epoch 37, Batch 80/157, Loss: 2.6967\n",
      "Epoch 37, Batch 81/157, Loss: 2.7136\n",
      "Epoch 37, Batch 82/157, Loss: 2.7346\n",
      "Epoch 37, Batch 83/157, Loss: 2.7089\n",
      "Epoch 37, Batch 84/157, Loss: 2.7223\n",
      "Epoch 37, Batch 85/157, Loss: 2.7108\n",
      "Epoch 37, Batch 86/157, Loss: 2.7302\n",
      "Epoch 37, Batch 87/157, Loss: 2.6669\n",
      "Epoch 37, Batch 88/157, Loss: 2.7346\n",
      "Epoch 37, Batch 89/157, Loss: 2.7144\n",
      "Epoch 37, Batch 90/157, Loss: 2.7033\n",
      "Epoch 37, Batch 91/157, Loss: 2.7522\n",
      "Epoch 37, Batch 92/157, Loss: 2.6951\n",
      "Epoch 37, Batch 93/157, Loss: 2.7329\n",
      "Epoch 37, Batch 94/157, Loss: 2.7058\n",
      "Epoch 37, Batch 95/157, Loss: 2.7599\n",
      "Epoch 37, Batch 96/157, Loss: 2.7106\n",
      "Epoch 37, Batch 97/157, Loss: 2.6913\n",
      "Epoch 37, Batch 98/157, Loss: 2.7035\n",
      "Epoch 37, Batch 99/157, Loss: 2.7216\n",
      "Epoch 37, Batch 100/157, Loss: 2.7179\n",
      "Epoch 37, Batch 101/157, Loss: 2.7536\n",
      "Epoch 37, Batch 102/157, Loss: 2.7490\n",
      "Epoch 37, Batch 103/157, Loss: 2.7298\n",
      "Epoch 37, Batch 104/157, Loss: 2.7286\n",
      "Epoch 37, Batch 105/157, Loss: 2.7009\n",
      "Epoch 37, Batch 106/157, Loss: 2.6721\n",
      "Epoch 37, Batch 107/157, Loss: 2.7044\n",
      "Epoch 37, Batch 108/157, Loss: 2.7096\n",
      "Epoch 37, Batch 109/157, Loss: 2.7103\n",
      "Epoch 37, Batch 110/157, Loss: 2.6962\n",
      "Epoch 37, Batch 111/157, Loss: 2.7077\n",
      "Epoch 37, Batch 112/157, Loss: 2.6763\n",
      "Epoch 37, Batch 113/157, Loss: 2.7051\n",
      "Epoch 37, Batch 114/157, Loss: 2.7368\n",
      "Epoch 37, Batch 115/157, Loss: 2.7600\n",
      "Epoch 37, Batch 116/157, Loss: 2.7003\n",
      "Epoch 37, Batch 117/157, Loss: 2.6607\n",
      "Epoch 37, Batch 118/157, Loss: 2.7625\n",
      "Epoch 37, Batch 119/157, Loss: 2.6654\n",
      "Epoch 37, Batch 120/157, Loss: 2.6805\n",
      "Epoch 37, Batch 121/157, Loss: 2.7353\n",
      "Epoch 37, Batch 122/157, Loss: 2.6818\n",
      "Epoch 37, Batch 123/157, Loss: 2.7300\n",
      "Epoch 37, Batch 124/157, Loss: 2.6908\n",
      "Epoch 37, Batch 125/157, Loss: 2.7079\n",
      "Epoch 37, Batch 126/157, Loss: 2.6978\n",
      "Epoch 37, Batch 127/157, Loss: 2.6960\n",
      "Epoch 37, Batch 128/157, Loss: 2.7274\n",
      "Epoch 37, Batch 129/157, Loss: 2.6936\n",
      "Epoch 37, Batch 130/157, Loss: 2.7407\n",
      "Epoch 37, Batch 131/157, Loss: 2.7057\n",
      "Epoch 37, Batch 132/157, Loss: 2.7598\n",
      "Epoch 37, Batch 133/157, Loss: 2.7361\n",
      "Epoch 37, Batch 134/157, Loss: 2.7407\n",
      "Epoch 37, Batch 135/157, Loss: 2.7358\n",
      "Epoch 37, Batch 136/157, Loss: 2.7201\n",
      "Epoch 37, Batch 137/157, Loss: 2.7449\n",
      "Epoch 37, Batch 138/157, Loss: 2.7776\n",
      "Epoch 37, Batch 139/157, Loss: 2.7066\n",
      "Epoch 37, Batch 140/157, Loss: 2.7168\n",
      "Epoch 37, Batch 141/157, Loss: 2.7689\n",
      "Epoch 37, Batch 142/157, Loss: 2.7214\n",
      "Epoch 37, Batch 143/157, Loss: 2.6856\n",
      "Epoch 37, Batch 144/157, Loss: 2.7060\n",
      "Epoch 37, Batch 145/157, Loss: 2.6939\n",
      "Epoch 37, Batch 146/157, Loss: 2.7122\n",
      "Epoch 37, Batch 147/157, Loss: 2.7414\n",
      "Epoch 37, Batch 148/157, Loss: 2.6999\n",
      "Epoch 37, Batch 149/157, Loss: 2.7230\n",
      "Epoch 37, Batch 150/157, Loss: 2.6927\n",
      "Epoch 37, Batch 151/157, Loss: 2.7257\n",
      "Epoch 37, Batch 152/157, Loss: 2.7353\n",
      "Epoch 37, Batch 153/157, Loss: 2.7920\n",
      "Epoch 37, Batch 154/157, Loss: 2.7346\n",
      "Epoch 37, Batch 155/157, Loss: 2.6626\n",
      "Epoch 37, Batch 156/157, Loss: 2.7527\n",
      "Epoch 37, Batch 157/157, Loss: 2.7622\n",
      "Epoch 37/50, Average Loss: 2.7178\n",
      "Epoch 38, Batch 1/157, Loss: 2.7064\n",
      "Epoch 38, Batch 2/157, Loss: 2.6869\n",
      "Epoch 38, Batch 3/157, Loss: 2.7459\n",
      "Epoch 38, Batch 4/157, Loss: 2.6799\n",
      "Epoch 38, Batch 5/157, Loss: 2.6856\n",
      "Epoch 38, Batch 6/157, Loss: 2.6893\n",
      "Epoch 38, Batch 7/157, Loss: 2.6857\n",
      "Epoch 38, Batch 8/157, Loss: 2.7182\n",
      "Epoch 38, Batch 9/157, Loss: 2.7351\n",
      "Epoch 38, Batch 10/157, Loss: 2.6931\n",
      "Epoch 38, Batch 11/157, Loss: 2.7235\n",
      "Epoch 38, Batch 12/157, Loss: 2.8055\n",
      "Epoch 38, Batch 13/157, Loss: 2.7287\n",
      "Epoch 38, Batch 14/157, Loss: 2.6926\n",
      "Epoch 38, Batch 15/157, Loss: 2.7397\n",
      "Epoch 38, Batch 16/157, Loss: 2.7674\n",
      "Epoch 38, Batch 17/157, Loss: 2.6794\n",
      "Epoch 38, Batch 18/157, Loss: 2.7714\n",
      "Epoch 38, Batch 19/157, Loss: 2.6423\n",
      "Epoch 38, Batch 20/157, Loss: 2.7029\n",
      "Epoch 38, Batch 21/157, Loss: 2.7174\n",
      "Epoch 38, Batch 22/157, Loss: 2.7131\n",
      "Epoch 38, Batch 23/157, Loss: 2.8132\n",
      "Epoch 38, Batch 24/157, Loss: 2.7129\n",
      "Epoch 38, Batch 25/157, Loss: 2.7324\n",
      "Epoch 38, Batch 26/157, Loss: 2.7094\n",
      "Epoch 38, Batch 27/157, Loss: 2.7427\n",
      "Epoch 38, Batch 28/157, Loss: 2.7332\n",
      "Epoch 38, Batch 29/157, Loss: 2.7627\n",
      "Epoch 38, Batch 30/157, Loss: 2.7128\n",
      "Epoch 38, Batch 31/157, Loss: 2.7219\n",
      "Epoch 38, Batch 32/157, Loss: 2.7553\n",
      "Epoch 38, Batch 33/157, Loss: 2.7277\n",
      "Epoch 38, Batch 34/157, Loss: 2.7042\n",
      "Epoch 38, Batch 35/157, Loss: 2.7096\n",
      "Epoch 38, Batch 36/157, Loss: 2.7129\n",
      "Epoch 38, Batch 37/157, Loss: 2.7360\n",
      "Epoch 38, Batch 38/157, Loss: 2.7085\n",
      "Epoch 38, Batch 39/157, Loss: 2.7478\n",
      "Epoch 38, Batch 40/157, Loss: 2.7162\n",
      "Epoch 38, Batch 41/157, Loss: 2.7161\n",
      "Epoch 38, Batch 42/157, Loss: 2.7126\n",
      "Epoch 38, Batch 43/157, Loss: 2.7779\n",
      "Epoch 38, Batch 44/157, Loss: 2.7453\n",
      "Epoch 38, Batch 45/157, Loss: 2.7146\n",
      "Epoch 38, Batch 46/157, Loss: 2.7050\n",
      "Epoch 38, Batch 47/157, Loss: 2.7079\n",
      "Epoch 38, Batch 48/157, Loss: 2.6863\n",
      "Epoch 38, Batch 49/157, Loss: 2.7097\n",
      "Epoch 38, Batch 50/157, Loss: 2.7188\n",
      "Epoch 38, Batch 51/157, Loss: 2.7282\n",
      "Epoch 38, Batch 52/157, Loss: 2.7340\n",
      "Epoch 38, Batch 53/157, Loss: 2.7172\n",
      "Epoch 38, Batch 54/157, Loss: 2.7140\n",
      "Epoch 38, Batch 55/157, Loss: 2.7183\n",
      "Epoch 38, Batch 56/157, Loss: 2.6863\n",
      "Epoch 38, Batch 57/157, Loss: 2.6982\n",
      "Epoch 38, Batch 58/157, Loss: 2.7104\n",
      "Epoch 38, Batch 59/157, Loss: 2.7559\n",
      "Epoch 38, Batch 60/157, Loss: 2.7319\n",
      "Epoch 38, Batch 61/157, Loss: 2.7146\n",
      "Epoch 38, Batch 62/157, Loss: 2.7362\n",
      "Epoch 38, Batch 63/157, Loss: 2.7652\n",
      "Epoch 38, Batch 64/157, Loss: 2.7319\n",
      "Epoch 38, Batch 65/157, Loss: 2.7158\n",
      "Epoch 38, Batch 66/157, Loss: 2.7069\n",
      "Epoch 38, Batch 67/157, Loss: 2.6878\n",
      "Epoch 38, Batch 68/157, Loss: 2.7249\n",
      "Epoch 38, Batch 69/157, Loss: 2.6999\n",
      "Epoch 38, Batch 70/157, Loss: 2.7324\n",
      "Epoch 38, Batch 71/157, Loss: 2.6835\n",
      "Epoch 38, Batch 72/157, Loss: 2.7182\n",
      "Epoch 38, Batch 73/157, Loss: 2.7413\n",
      "Epoch 38, Batch 74/157, Loss: 2.7061\n",
      "Epoch 38, Batch 75/157, Loss: 2.7058\n",
      "Epoch 38, Batch 76/157, Loss: 2.7314\n",
      "Epoch 38, Batch 77/157, Loss: 2.7282\n",
      "Epoch 38, Batch 78/157, Loss: 2.6620\n",
      "Epoch 38, Batch 79/157, Loss: 2.7318\n",
      "Epoch 38, Batch 80/157, Loss: 2.7153\n",
      "Epoch 38, Batch 81/157, Loss: 2.6943\n",
      "Epoch 38, Batch 82/157, Loss: 2.7811\n",
      "Epoch 38, Batch 83/157, Loss: 2.7689\n",
      "Epoch 38, Batch 84/157, Loss: 2.7277\n",
      "Epoch 38, Batch 85/157, Loss: 2.7120\n",
      "Epoch 38, Batch 86/157, Loss: 2.7452\n",
      "Epoch 38, Batch 87/157, Loss: 2.7460\n",
      "Epoch 38, Batch 88/157, Loss: 2.7351\n",
      "Epoch 38, Batch 89/157, Loss: 2.7293\n",
      "Epoch 38, Batch 90/157, Loss: 2.6702\n",
      "Epoch 38, Batch 91/157, Loss: 2.7447\n",
      "Epoch 38, Batch 92/157, Loss: 2.7257\n",
      "Epoch 38, Batch 93/157, Loss: 2.7598\n",
      "Epoch 38, Batch 94/157, Loss: 2.7027\n",
      "Epoch 38, Batch 95/157, Loss: 2.7012\n",
      "Epoch 38, Batch 96/157, Loss: 2.7245\n",
      "Epoch 38, Batch 97/157, Loss: 2.6951\n",
      "Epoch 38, Batch 98/157, Loss: 2.6836\n",
      "Epoch 38, Batch 99/157, Loss: 2.7236\n",
      "Epoch 38, Batch 100/157, Loss: 2.7450\n",
      "Epoch 38, Batch 101/157, Loss: 2.7154\n",
      "Epoch 38, Batch 102/157, Loss: 2.6998\n",
      "Epoch 38, Batch 103/157, Loss: 2.6947\n",
      "Epoch 38, Batch 104/157, Loss: 2.6633\n",
      "Epoch 38, Batch 105/157, Loss: 2.6917\n",
      "Epoch 38, Batch 106/157, Loss: 2.6944\n",
      "Epoch 38, Batch 107/157, Loss: 2.6874\n",
      "Epoch 38, Batch 108/157, Loss: 2.7160\n",
      "Epoch 38, Batch 109/157, Loss: 2.7565\n",
      "Epoch 38, Batch 110/157, Loss: 2.6852\n",
      "Epoch 38, Batch 111/157, Loss: 2.7211\n",
      "Epoch 38, Batch 112/157, Loss: 2.7131\n",
      "Epoch 38, Batch 113/157, Loss: 2.6625\n",
      "Epoch 38, Batch 114/157, Loss: 2.6963\n",
      "Epoch 38, Batch 115/157, Loss: 2.6876\n",
      "Epoch 38, Batch 116/157, Loss: 2.6518\n",
      "Epoch 38, Batch 117/157, Loss: 2.7681\n",
      "Epoch 38, Batch 118/157, Loss: 2.6747\n",
      "Epoch 38, Batch 119/157, Loss: 2.6942\n",
      "Epoch 38, Batch 120/157, Loss: 2.8042\n",
      "Epoch 38, Batch 121/157, Loss: 2.7204\n",
      "Epoch 38, Batch 122/157, Loss: 2.7438\n",
      "Epoch 38, Batch 123/157, Loss: 2.7407\n",
      "Epoch 38, Batch 124/157, Loss: 2.7396\n",
      "Epoch 38, Batch 125/157, Loss: 2.6639\n",
      "Epoch 38, Batch 126/157, Loss: 2.7447\n",
      "Epoch 38, Batch 127/157, Loss: 2.6690\n",
      "Epoch 38, Batch 128/157, Loss: 2.7002\n",
      "Epoch 38, Batch 129/157, Loss: 2.6883\n",
      "Epoch 38, Batch 130/157, Loss: 2.7891\n",
      "Epoch 38, Batch 131/157, Loss: 2.7056\n",
      "Epoch 38, Batch 132/157, Loss: 2.7312\n",
      "Epoch 38, Batch 133/157, Loss: 2.7257\n",
      "Epoch 38, Batch 134/157, Loss: 2.7308\n",
      "Epoch 38, Batch 135/157, Loss: 2.7412\n",
      "Epoch 38, Batch 136/157, Loss: 2.7358\n",
      "Epoch 38, Batch 137/157, Loss: 2.7090\n",
      "Epoch 38, Batch 138/157, Loss: 2.7037\n",
      "Epoch 38, Batch 139/157, Loss: 2.7109\n",
      "Epoch 38, Batch 140/157, Loss: 2.7334\n",
      "Epoch 38, Batch 141/157, Loss: 2.7205\n",
      "Epoch 38, Batch 142/157, Loss: 2.6907\n",
      "Epoch 38, Batch 143/157, Loss: 2.7081\n",
      "Epoch 38, Batch 144/157, Loss: 2.7330\n",
      "Epoch 38, Batch 145/157, Loss: 2.7061\n",
      "Epoch 38, Batch 146/157, Loss: 2.7127\n",
      "Epoch 38, Batch 147/157, Loss: 2.7279\n",
      "Epoch 38, Batch 148/157, Loss: 2.7150\n",
      "Epoch 38, Batch 149/157, Loss: 2.6935\n",
      "Epoch 38, Batch 150/157, Loss: 2.7004\n",
      "Epoch 38, Batch 151/157, Loss: 2.7069\n",
      "Epoch 38, Batch 152/157, Loss: 2.7214\n",
      "Epoch 38, Batch 153/157, Loss: 2.7226\n",
      "Epoch 38, Batch 154/157, Loss: 2.6996\n",
      "Epoch 38, Batch 155/157, Loss: 2.7324\n",
      "Epoch 38, Batch 156/157, Loss: 2.6970\n",
      "Epoch 38, Batch 157/157, Loss: 2.7140\n",
      "Epoch 38/50, Average Loss: 2.7180\n",
      "Epoch 39, Batch 1/157, Loss: 2.7106\n",
      "Epoch 39, Batch 2/157, Loss: 2.6768\n",
      "Epoch 39, Batch 3/157, Loss: 2.7600\n",
      "Epoch 39, Batch 4/157, Loss: 2.7143\n",
      "Epoch 39, Batch 5/157, Loss: 2.7572\n",
      "Epoch 39, Batch 6/157, Loss: 2.7138\n",
      "Epoch 39, Batch 7/157, Loss: 2.7309\n",
      "Epoch 39, Batch 8/157, Loss: 2.7299\n",
      "Epoch 39, Batch 9/157, Loss: 2.7262\n",
      "Epoch 39, Batch 10/157, Loss: 2.7054\n",
      "Epoch 39, Batch 11/157, Loss: 2.7077\n",
      "Epoch 39, Batch 12/157, Loss: 2.7377\n",
      "Epoch 39, Batch 13/157, Loss: 2.7233\n",
      "Epoch 39, Batch 14/157, Loss: 2.6911\n",
      "Epoch 39, Batch 15/157, Loss: 2.6918\n",
      "Epoch 39, Batch 16/157, Loss: 2.6900\n",
      "Epoch 39, Batch 17/157, Loss: 2.6718\n",
      "Epoch 39, Batch 18/157, Loss: 2.7170\n",
      "Epoch 39, Batch 19/157, Loss: 2.7106\n",
      "Epoch 39, Batch 20/157, Loss: 2.6921\n",
      "Epoch 39, Batch 21/157, Loss: 2.7123\n",
      "Epoch 39, Batch 22/157, Loss: 2.7109\n",
      "Epoch 39, Batch 23/157, Loss: 2.7619\n",
      "Epoch 39, Batch 24/157, Loss: 2.7118\n",
      "Epoch 39, Batch 25/157, Loss: 2.7815\n",
      "Epoch 39, Batch 26/157, Loss: 2.7846\n",
      "Epoch 39, Batch 27/157, Loss: 2.7107\n",
      "Epoch 39, Batch 28/157, Loss: 2.6782\n",
      "Epoch 39, Batch 29/157, Loss: 2.6772\n",
      "Epoch 39, Batch 30/157, Loss: 2.7147\n",
      "Epoch 39, Batch 31/157, Loss: 2.6961\n",
      "Epoch 39, Batch 32/157, Loss: 2.7440\n",
      "Epoch 39, Batch 33/157, Loss: 2.7151\n",
      "Epoch 39, Batch 34/157, Loss: 2.6937\n",
      "Epoch 39, Batch 35/157, Loss: 2.6971\n",
      "Epoch 39, Batch 36/157, Loss: 2.7474\n",
      "Epoch 39, Batch 37/157, Loss: 2.7262\n",
      "Epoch 39, Batch 38/157, Loss: 2.7326\n",
      "Epoch 39, Batch 39/157, Loss: 2.7486\n",
      "Epoch 39, Batch 40/157, Loss: 2.7647\n",
      "Epoch 39, Batch 41/157, Loss: 2.6906\n",
      "Epoch 39, Batch 42/157, Loss: 2.7171\n",
      "Epoch 39, Batch 43/157, Loss: 2.7421\n",
      "Epoch 39, Batch 44/157, Loss: 2.7485\n",
      "Epoch 39, Batch 45/157, Loss: 2.6937\n",
      "Epoch 39, Batch 46/157, Loss: 2.7278\n",
      "Epoch 39, Batch 47/157, Loss: 2.7130\n",
      "Epoch 39, Batch 48/157, Loss: 2.6972\n",
      "Epoch 39, Batch 49/157, Loss: 2.7497\n",
      "Epoch 39, Batch 50/157, Loss: 2.6877\n",
      "Epoch 39, Batch 51/157, Loss: 2.7481\n",
      "Epoch 39, Batch 52/157, Loss: 2.7227\n",
      "Epoch 39, Batch 53/157, Loss: 2.7136\n",
      "Epoch 39, Batch 54/157, Loss: 2.7633\n",
      "Epoch 39, Batch 55/157, Loss: 2.7455\n",
      "Epoch 39, Batch 56/157, Loss: 2.7022\n",
      "Epoch 39, Batch 57/157, Loss: 2.7179\n",
      "Epoch 39, Batch 58/157, Loss: 2.7160\n",
      "Epoch 39, Batch 59/157, Loss: 2.7278\n",
      "Epoch 39, Batch 60/157, Loss: 2.7464\n",
      "Epoch 39, Batch 61/157, Loss: 2.6929\n",
      "Epoch 39, Batch 62/157, Loss: 2.7434\n",
      "Epoch 39, Batch 63/157, Loss: 2.7003\n",
      "Epoch 39, Batch 64/157, Loss: 2.7023\n",
      "Epoch 39, Batch 65/157, Loss: 2.6796\n",
      "Epoch 39, Batch 66/157, Loss: 2.6979\n",
      "Epoch 39, Batch 67/157, Loss: 2.7184\n",
      "Epoch 39, Batch 68/157, Loss: 2.7870\n",
      "Epoch 39, Batch 69/157, Loss: 2.6908\n",
      "Epoch 39, Batch 70/157, Loss: 2.7161\n",
      "Epoch 39, Batch 71/157, Loss: 2.6888\n",
      "Epoch 39, Batch 72/157, Loss: 2.7354\n",
      "Epoch 39, Batch 73/157, Loss: 2.7502\n",
      "Epoch 39, Batch 74/157, Loss: 2.7204\n",
      "Epoch 39, Batch 75/157, Loss: 2.7340\n",
      "Epoch 39, Batch 76/157, Loss: 2.7237\n",
      "Epoch 39, Batch 77/157, Loss: 2.7118\n",
      "Epoch 39, Batch 78/157, Loss: 2.7157\n",
      "Epoch 39, Batch 79/157, Loss: 2.7090\n",
      "Epoch 39, Batch 80/157, Loss: 2.7029\n",
      "Epoch 39, Batch 81/157, Loss: 2.7426\n",
      "Epoch 39, Batch 82/157, Loss: 2.7003\n",
      "Epoch 39, Batch 83/157, Loss: 2.7235\n",
      "Epoch 39, Batch 84/157, Loss: 2.7214\n",
      "Epoch 39, Batch 85/157, Loss: 2.7284\n",
      "Epoch 39, Batch 86/157, Loss: 2.7120\n",
      "Epoch 39, Batch 87/157, Loss: 2.7310\n",
      "Epoch 39, Batch 88/157, Loss: 2.7446\n",
      "Epoch 39, Batch 89/157, Loss: 2.7397\n",
      "Epoch 39, Batch 90/157, Loss: 2.7094\n",
      "Epoch 39, Batch 91/157, Loss: 2.7293\n",
      "Epoch 39, Batch 92/157, Loss: 2.6752\n",
      "Epoch 39, Batch 93/157, Loss: 2.7154\n",
      "Epoch 39, Batch 94/157, Loss: 2.7287\n",
      "Epoch 39, Batch 95/157, Loss: 2.6760\n",
      "Epoch 39, Batch 96/157, Loss: 2.7066\n",
      "Epoch 39, Batch 97/157, Loss: 2.7166\n",
      "Epoch 39, Batch 98/157, Loss: 2.7181\n",
      "Epoch 39, Batch 99/157, Loss: 2.7288\n",
      "Epoch 39, Batch 100/157, Loss: 2.7026\n",
      "Epoch 39, Batch 101/157, Loss: 2.7123\n",
      "Epoch 39, Batch 102/157, Loss: 2.6865\n",
      "Epoch 39, Batch 103/157, Loss: 2.7248\n",
      "Epoch 39, Batch 104/157, Loss: 2.7300\n",
      "Epoch 39, Batch 105/157, Loss: 2.7358\n",
      "Epoch 39, Batch 106/157, Loss: 2.7353\n",
      "Epoch 39, Batch 107/157, Loss: 2.7192\n",
      "Epoch 39, Batch 108/157, Loss: 2.7110\n",
      "Epoch 39, Batch 109/157, Loss: 2.7111\n",
      "Epoch 39, Batch 110/157, Loss: 2.7606\n",
      "Epoch 39, Batch 111/157, Loss: 2.6948\n",
      "Epoch 39, Batch 112/157, Loss: 2.7422\n",
      "Epoch 39, Batch 113/157, Loss: 2.7362\n",
      "Epoch 39, Batch 114/157, Loss: 2.7270\n",
      "Epoch 39, Batch 115/157, Loss: 2.7187\n",
      "Epoch 39, Batch 116/157, Loss: 2.7381\n",
      "Epoch 39, Batch 117/157, Loss: 2.7710\n",
      "Epoch 39, Batch 118/157, Loss: 2.7294\n",
      "Epoch 39, Batch 119/157, Loss: 2.6941\n",
      "Epoch 39, Batch 120/157, Loss: 2.7028\n",
      "Epoch 39, Batch 121/157, Loss: 2.7247\n",
      "Epoch 39, Batch 122/157, Loss: 2.6942\n",
      "Epoch 39, Batch 123/157, Loss: 2.7251\n",
      "Epoch 39, Batch 124/157, Loss: 2.7231\n",
      "Epoch 39, Batch 125/157, Loss: 2.7251\n",
      "Epoch 39, Batch 126/157, Loss: 2.7141\n",
      "Epoch 39, Batch 127/157, Loss: 2.7146\n",
      "Epoch 39, Batch 128/157, Loss: 2.7406\n",
      "Epoch 39, Batch 129/157, Loss: 2.7064\n",
      "Epoch 39, Batch 130/157, Loss: 2.6948\n",
      "Epoch 39, Batch 131/157, Loss: 2.7744\n",
      "Epoch 39, Batch 132/157, Loss: 2.7179\n",
      "Epoch 39, Batch 133/157, Loss: 2.7391\n",
      "Epoch 39, Batch 134/157, Loss: 2.7150\n",
      "Epoch 39, Batch 135/157, Loss: 2.7174\n",
      "Epoch 39, Batch 136/157, Loss: 2.7517\n",
      "Epoch 39, Batch 137/157, Loss: 2.7094\n",
      "Epoch 39, Batch 138/157, Loss: 2.7403\n",
      "Epoch 39, Batch 139/157, Loss: 2.7253\n",
      "Epoch 39, Batch 140/157, Loss: 2.7318\n",
      "Epoch 39, Batch 141/157, Loss: 2.7412\n",
      "Epoch 39, Batch 142/157, Loss: 2.7110\n",
      "Epoch 39, Batch 143/157, Loss: 2.7315\n",
      "Epoch 39, Batch 144/157, Loss: 2.7349\n",
      "Epoch 39, Batch 145/157, Loss: 2.7348\n",
      "Epoch 39, Batch 146/157, Loss: 2.7089\n",
      "Epoch 39, Batch 147/157, Loss: 2.7113\n",
      "Epoch 39, Batch 148/157, Loss: 2.7014\n",
      "Epoch 39, Batch 149/157, Loss: 2.7084\n",
      "Epoch 39, Batch 150/157, Loss: 2.7109\n",
      "Epoch 39, Batch 151/157, Loss: 2.7141\n",
      "Epoch 39, Batch 152/157, Loss: 2.6980\n",
      "Epoch 39, Batch 153/157, Loss: 2.7121\n",
      "Epoch 39, Batch 154/157, Loss: 2.7173\n",
      "Epoch 39, Batch 155/157, Loss: 2.7123\n",
      "Epoch 39, Batch 156/157, Loss: 2.7087\n",
      "Epoch 39, Batch 157/157, Loss: 2.7292\n",
      "Epoch 39/50, Average Loss: 2.7201\n",
      "Epoch 40, Batch 1/157, Loss: 2.6776\n",
      "Epoch 40, Batch 2/157, Loss: 2.6948\n",
      "Epoch 40, Batch 3/157, Loss: 2.7572\n",
      "Epoch 40, Batch 4/157, Loss: 2.6915\n",
      "Epoch 40, Batch 5/157, Loss: 2.7176\n",
      "Epoch 40, Batch 6/157, Loss: 2.6810\n",
      "Epoch 40, Batch 7/157, Loss: 2.6947\n",
      "Epoch 40, Batch 8/157, Loss: 2.7610\n",
      "Epoch 40, Batch 9/157, Loss: 2.7114\n",
      "Epoch 40, Batch 10/157, Loss: 2.7159\n",
      "Epoch 40, Batch 11/157, Loss: 2.6927\n",
      "Epoch 40, Batch 12/157, Loss: 2.7140\n",
      "Epoch 40, Batch 13/157, Loss: 2.7172\n",
      "Epoch 40, Batch 14/157, Loss: 2.7481\n",
      "Epoch 40, Batch 15/157, Loss: 2.7200\n",
      "Epoch 40, Batch 16/157, Loss: 2.7902\n",
      "Epoch 40, Batch 17/157, Loss: 2.7187\n",
      "Epoch 40, Batch 18/157, Loss: 2.7453\n",
      "Epoch 40, Batch 19/157, Loss: 2.6723\n",
      "Epoch 40, Batch 20/157, Loss: 2.6880\n",
      "Epoch 40, Batch 21/157, Loss: 2.6817\n",
      "Epoch 40, Batch 22/157, Loss: 2.7205\n",
      "Epoch 40, Batch 23/157, Loss: 2.7227\n",
      "Epoch 40, Batch 24/157, Loss: 2.7237\n",
      "Epoch 40, Batch 25/157, Loss: 2.6839\n",
      "Epoch 40, Batch 26/157, Loss: 2.7353\n",
      "Epoch 40, Batch 27/157, Loss: 2.6655\n",
      "Epoch 40, Batch 28/157, Loss: 2.7580\n",
      "Epoch 40, Batch 29/157, Loss: 2.6969\n",
      "Epoch 40, Batch 30/157, Loss: 2.7474\n",
      "Epoch 40, Batch 31/157, Loss: 2.7416\n",
      "Epoch 40, Batch 32/157, Loss: 2.6753\n",
      "Epoch 40, Batch 33/157, Loss: 2.7098\n",
      "Epoch 40, Batch 34/157, Loss: 2.6750\n",
      "Epoch 40, Batch 35/157, Loss: 2.7177\n",
      "Epoch 40, Batch 36/157, Loss: 2.6955\n",
      "Epoch 40, Batch 37/157, Loss: 2.7299\n",
      "Epoch 40, Batch 38/157, Loss: 2.7223\n",
      "Epoch 40, Batch 39/157, Loss: 2.7031\n",
      "Epoch 40, Batch 40/157, Loss: 2.6661\n",
      "Epoch 40, Batch 41/157, Loss: 2.7421\n",
      "Epoch 40, Batch 42/157, Loss: 2.7801\n",
      "Epoch 40, Batch 43/157, Loss: 2.7019\n",
      "Epoch 40, Batch 44/157, Loss: 2.7273\n",
      "Epoch 40, Batch 45/157, Loss: 2.7169\n",
      "Epoch 40, Batch 46/157, Loss: 2.6681\n",
      "Epoch 40, Batch 47/157, Loss: 2.7073\n",
      "Epoch 40, Batch 48/157, Loss: 2.8076\n",
      "Epoch 40, Batch 49/157, Loss: 2.6826\n",
      "Epoch 40, Batch 50/157, Loss: 2.7292\n",
      "Epoch 40, Batch 51/157, Loss: 2.7557\n",
      "Epoch 40, Batch 52/157, Loss: 2.6943\n",
      "Epoch 40, Batch 53/157, Loss: 2.7769\n",
      "Epoch 40, Batch 54/157, Loss: 2.7700\n",
      "Epoch 40, Batch 55/157, Loss: 2.6965\n",
      "Epoch 40, Batch 56/157, Loss: 2.7625\n",
      "Epoch 40, Batch 57/157, Loss: 2.6739\n",
      "Epoch 40, Batch 58/157, Loss: 2.7011\n",
      "Epoch 40, Batch 59/157, Loss: 2.6846\n",
      "Epoch 40, Batch 60/157, Loss: 2.7361\n",
      "Epoch 40, Batch 61/157, Loss: 2.7366\n",
      "Epoch 40, Batch 62/157, Loss: 2.7092\n",
      "Epoch 40, Batch 63/157, Loss: 2.6899\n",
      "Epoch 40, Batch 64/157, Loss: 2.6906\n",
      "Epoch 40, Batch 65/157, Loss: 2.6746\n",
      "Epoch 40, Batch 66/157, Loss: 2.7321\n",
      "Epoch 40, Batch 67/157, Loss: 2.7169\n",
      "Epoch 40, Batch 68/157, Loss: 2.6859\n",
      "Epoch 40, Batch 69/157, Loss: 2.7429\n",
      "Epoch 40, Batch 70/157, Loss: 2.8101\n",
      "Epoch 40, Batch 71/157, Loss: 2.6669\n",
      "Epoch 40, Batch 72/157, Loss: 2.7468\n",
      "Epoch 40, Batch 73/157, Loss: 2.7284\n",
      "Epoch 40, Batch 74/157, Loss: 2.7030\n",
      "Epoch 40, Batch 75/157, Loss: 2.7403\n",
      "Epoch 40, Batch 76/157, Loss: 2.7048\n",
      "Epoch 40, Batch 77/157, Loss: 2.7261\n",
      "Epoch 40, Batch 78/157, Loss: 2.7439\n",
      "Epoch 40, Batch 79/157, Loss: 2.6969\n",
      "Epoch 40, Batch 80/157, Loss: 2.6796\n",
      "Epoch 40, Batch 81/157, Loss: 2.6639\n",
      "Epoch 40, Batch 82/157, Loss: 2.7241\n",
      "Epoch 40, Batch 83/157, Loss: 2.7171\n",
      "Epoch 40, Batch 84/157, Loss: 2.6830\n",
      "Epoch 40, Batch 85/157, Loss: 2.7381\n",
      "Epoch 40, Batch 86/157, Loss: 2.6990\n",
      "Epoch 40, Batch 87/157, Loss: 2.7577\n",
      "Epoch 40, Batch 88/157, Loss: 2.6793\n",
      "Epoch 40, Batch 89/157, Loss: 2.7321\n",
      "Epoch 40, Batch 90/157, Loss: 2.7098\n",
      "Epoch 40, Batch 91/157, Loss: 2.6991\n",
      "Epoch 40, Batch 92/157, Loss: 2.7342\n",
      "Epoch 40, Batch 93/157, Loss: 2.6934\n",
      "Epoch 40, Batch 94/157, Loss: 2.7205\n",
      "Epoch 40, Batch 95/157, Loss: 2.6852\n",
      "Epoch 40, Batch 96/157, Loss: 2.7223\n",
      "Epoch 40, Batch 97/157, Loss: 2.6864\n",
      "Epoch 40, Batch 98/157, Loss: 2.7380\n",
      "Epoch 40, Batch 99/157, Loss: 2.7190\n",
      "Epoch 40, Batch 100/157, Loss: 2.7402\n",
      "Epoch 40, Batch 101/157, Loss: 2.7811\n",
      "Epoch 40, Batch 102/157, Loss: 2.6409\n",
      "Epoch 40, Batch 103/157, Loss: 2.6789\n",
      "Epoch 40, Batch 104/157, Loss: 2.6922\n",
      "Epoch 40, Batch 105/157, Loss: 2.7208\n",
      "Epoch 40, Batch 106/157, Loss: 2.6867\n",
      "Epoch 40, Batch 107/157, Loss: 2.7166\n",
      "Epoch 40, Batch 108/157, Loss: 2.7659\n",
      "Epoch 40, Batch 109/157, Loss: 2.7278\n",
      "Epoch 40, Batch 110/157, Loss: 2.7637\n",
      "Epoch 40, Batch 111/157, Loss: 2.6849\n",
      "Epoch 40, Batch 112/157, Loss: 2.7046\n",
      "Epoch 40, Batch 113/157, Loss: 2.6908\n",
      "Epoch 40, Batch 114/157, Loss: 2.7562\n",
      "Epoch 40, Batch 115/157, Loss: 2.6978\n",
      "Epoch 40, Batch 116/157, Loss: 2.6792\n",
      "Epoch 40, Batch 117/157, Loss: 2.6967\n",
      "Epoch 40, Batch 118/157, Loss: 2.7364\n",
      "Epoch 40, Batch 119/157, Loss: 2.7456\n",
      "Epoch 40, Batch 120/157, Loss: 2.6865\n",
      "Epoch 40, Batch 121/157, Loss: 2.6731\n",
      "Epoch 40, Batch 122/157, Loss: 2.7239\n",
      "Epoch 40, Batch 123/157, Loss: 2.7386\n",
      "Epoch 40, Batch 124/157, Loss: 2.7446\n",
      "Epoch 40, Batch 125/157, Loss: 2.7500\n",
      "Epoch 40, Batch 126/157, Loss: 2.7257\n",
      "Epoch 40, Batch 127/157, Loss: 2.6846\n",
      "Epoch 40, Batch 128/157, Loss: 2.7275\n",
      "Epoch 40, Batch 129/157, Loss: 2.7395\n",
      "Epoch 40, Batch 130/157, Loss: 2.7130\n",
      "Epoch 40, Batch 131/157, Loss: 2.7383\n",
      "Epoch 40, Batch 132/157, Loss: 2.6964\n",
      "Epoch 40, Batch 133/157, Loss: 2.7302\n",
      "Epoch 40, Batch 134/157, Loss: 2.6391\n",
      "Epoch 40, Batch 135/157, Loss: 2.7383\n",
      "Epoch 40, Batch 136/157, Loss: 2.7021\n",
      "Epoch 40, Batch 137/157, Loss: 2.7247\n",
      "Epoch 40, Batch 138/157, Loss: 2.7059\n",
      "Epoch 40, Batch 139/157, Loss: 2.7092\n",
      "Epoch 40, Batch 140/157, Loss: 2.7703\n",
      "Epoch 40, Batch 141/157, Loss: 2.6929\n",
      "Epoch 40, Batch 142/157, Loss: 2.7003\n",
      "Epoch 40, Batch 143/157, Loss: 2.7558\n",
      "Epoch 40, Batch 144/157, Loss: 2.7468\n",
      "Epoch 40, Batch 145/157, Loss: 2.7621\n",
      "Epoch 40, Batch 146/157, Loss: 2.7212\n",
      "Epoch 40, Batch 147/157, Loss: 2.7485\n",
      "Epoch 40, Batch 148/157, Loss: 2.7363\n",
      "Epoch 40, Batch 149/157, Loss: 2.7270\n",
      "Epoch 40, Batch 150/157, Loss: 2.6835\n",
      "Epoch 40, Batch 151/157, Loss: 2.7025\n",
      "Epoch 40, Batch 152/157, Loss: 2.7249\n",
      "Epoch 40, Batch 153/157, Loss: 2.7071\n",
      "Epoch 40, Batch 154/157, Loss: 2.7045\n",
      "Epoch 40, Batch 155/157, Loss: 2.7382\n",
      "Epoch 40, Batch 156/157, Loss: 2.6906\n",
      "Epoch 40, Batch 157/157, Loss: 2.7750\n",
      "Epoch 40/50, Average Loss: 2.7166\n",
      "Epoch 41, Batch 1/157, Loss: 2.6942\n",
      "Epoch 41, Batch 2/157, Loss: 2.6819\n",
      "Epoch 41, Batch 3/157, Loss: 2.6839\n",
      "Epoch 41, Batch 4/157, Loss: 2.7459\n",
      "Epoch 41, Batch 5/157, Loss: 2.7069\n",
      "Epoch 41, Batch 6/157, Loss: 2.6932\n",
      "Epoch 41, Batch 7/157, Loss: 2.7138\n",
      "Epoch 41, Batch 8/157, Loss: 2.7493\n",
      "Epoch 41, Batch 9/157, Loss: 2.7287\n",
      "Epoch 41, Batch 10/157, Loss: 2.7170\n",
      "Epoch 41, Batch 11/157, Loss: 2.7343\n",
      "Epoch 41, Batch 12/157, Loss: 2.7110\n",
      "Epoch 41, Batch 13/157, Loss: 2.7434\n",
      "Epoch 41, Batch 14/157, Loss: 2.7063\n",
      "Epoch 41, Batch 15/157, Loss: 2.7093\n",
      "Epoch 41, Batch 16/157, Loss: 2.7075\n",
      "Epoch 41, Batch 17/157, Loss: 2.6994\n",
      "Epoch 41, Batch 18/157, Loss: 2.7086\n",
      "Epoch 41, Batch 19/157, Loss: 2.6757\n",
      "Epoch 41, Batch 20/157, Loss: 2.6972\n",
      "Epoch 41, Batch 21/157, Loss: 2.7131\n",
      "Epoch 41, Batch 22/157, Loss: 2.7040\n",
      "Epoch 41, Batch 23/157, Loss: 2.7424\n",
      "Epoch 41, Batch 24/157, Loss: 2.7081\n",
      "Epoch 41, Batch 25/157, Loss: 2.7405\n",
      "Epoch 41, Batch 26/157, Loss: 2.7589\n",
      "Epoch 41, Batch 27/157, Loss: 2.7201\n",
      "Epoch 41, Batch 28/157, Loss: 2.7233\n",
      "Epoch 41, Batch 29/157, Loss: 2.7297\n",
      "Epoch 41, Batch 30/157, Loss: 2.7073\n",
      "Epoch 41, Batch 31/157, Loss: 2.7614\n",
      "Epoch 41, Batch 32/157, Loss: 2.7089\n",
      "Epoch 41, Batch 33/157, Loss: 2.7582\n",
      "Epoch 41, Batch 34/157, Loss: 2.7197\n",
      "Epoch 41, Batch 35/157, Loss: 2.7186\n",
      "Epoch 41, Batch 36/157, Loss: 2.7265\n",
      "Epoch 41, Batch 37/157, Loss: 2.7240\n",
      "Epoch 41, Batch 38/157, Loss: 2.7329\n",
      "Epoch 41, Batch 39/157, Loss: 2.7257\n",
      "Epoch 41, Batch 40/157, Loss: 2.7205\n",
      "Epoch 41, Batch 41/157, Loss: 2.6886\n",
      "Epoch 41, Batch 42/157, Loss: 2.7210\n",
      "Epoch 41, Batch 43/157, Loss: 2.7025\n",
      "Epoch 41, Batch 44/157, Loss: 2.6954\n",
      "Epoch 41, Batch 45/157, Loss: 2.7114\n",
      "Epoch 41, Batch 46/157, Loss: 2.7106\n",
      "Epoch 41, Batch 47/157, Loss: 2.7215\n",
      "Epoch 41, Batch 48/157, Loss: 2.7146\n",
      "Epoch 41, Batch 49/157, Loss: 2.7070\n",
      "Epoch 41, Batch 50/157, Loss: 2.7099\n",
      "Epoch 41, Batch 51/157, Loss: 2.7158\n",
      "Epoch 41, Batch 52/157, Loss: 2.7438\n",
      "Epoch 41, Batch 53/157, Loss: 2.6751\n",
      "Epoch 41, Batch 54/157, Loss: 2.7008\n",
      "Epoch 41, Batch 55/157, Loss: 2.7238\n",
      "Epoch 41, Batch 56/157, Loss: 2.7346\n",
      "Epoch 41, Batch 57/157, Loss: 2.7284\n",
      "Epoch 41, Batch 58/157, Loss: 2.6923\n",
      "Epoch 41, Batch 59/157, Loss: 2.7089\n",
      "Epoch 41, Batch 60/157, Loss: 2.7128\n",
      "Epoch 41, Batch 61/157, Loss: 2.7175\n",
      "Epoch 41, Batch 62/157, Loss: 2.6959\n",
      "Epoch 41, Batch 63/157, Loss: 2.7222\n",
      "Epoch 41, Batch 64/157, Loss: 2.6897\n",
      "Epoch 41, Batch 65/157, Loss: 2.6704\n",
      "Epoch 41, Batch 66/157, Loss: 2.6707\n",
      "Epoch 41, Batch 67/157, Loss: 2.7253\n",
      "Epoch 41, Batch 68/157, Loss: 2.6923\n",
      "Epoch 41, Batch 69/157, Loss: 2.6671\n",
      "Epoch 41, Batch 70/157, Loss: 2.6683\n",
      "Epoch 41, Batch 71/157, Loss: 2.7122\n",
      "Epoch 41, Batch 72/157, Loss: 2.7492\n",
      "Epoch 41, Batch 73/157, Loss: 2.7918\n",
      "Epoch 41, Batch 74/157, Loss: 2.7282\n",
      "Epoch 41, Batch 75/157, Loss: 2.7321\n",
      "Epoch 41, Batch 76/157, Loss: 2.7231\n",
      "Epoch 41, Batch 77/157, Loss: 2.7370\n",
      "Epoch 41, Batch 78/157, Loss: 2.7400\n",
      "Epoch 41, Batch 79/157, Loss: 2.7304\n",
      "Epoch 41, Batch 80/157, Loss: 2.7098\n",
      "Epoch 41, Batch 81/157, Loss: 2.7238\n",
      "Epoch 41, Batch 82/157, Loss: 2.7174\n",
      "Epoch 41, Batch 83/157, Loss: 2.7436\n",
      "Epoch 41, Batch 84/157, Loss: 2.7001\n",
      "Epoch 41, Batch 85/157, Loss: 2.6940\n",
      "Epoch 41, Batch 86/157, Loss: 2.7605\n",
      "Epoch 41, Batch 87/157, Loss: 2.7429\n",
      "Epoch 41, Batch 88/157, Loss: 2.7466\n",
      "Epoch 41, Batch 89/157, Loss: 2.7217\n",
      "Epoch 41, Batch 90/157, Loss: 2.7193\n",
      "Epoch 41, Batch 91/157, Loss: 2.7162\n",
      "Epoch 41, Batch 92/157, Loss: 2.7343\n",
      "Epoch 41, Batch 93/157, Loss: 2.7406\n",
      "Epoch 41, Batch 94/157, Loss: 2.7270\n",
      "Epoch 41, Batch 95/157, Loss: 2.7121\n",
      "Epoch 41, Batch 96/157, Loss: 2.7131\n",
      "Epoch 41, Batch 97/157, Loss: 2.7366\n",
      "Epoch 41, Batch 98/157, Loss: 2.7268\n",
      "Epoch 41, Batch 99/157, Loss: 2.7270\n",
      "Epoch 41, Batch 100/157, Loss: 2.7070\n",
      "Epoch 41, Batch 101/157, Loss: 2.7063\n",
      "Epoch 41, Batch 102/157, Loss: 2.7242\n",
      "Epoch 41, Batch 103/157, Loss: 2.7343\n",
      "Epoch 41, Batch 104/157, Loss: 2.7114\n",
      "Epoch 41, Batch 105/157, Loss: 2.7047\n",
      "Epoch 41, Batch 106/157, Loss: 2.6933\n",
      "Epoch 41, Batch 107/157, Loss: 2.7006\n",
      "Epoch 41, Batch 108/157, Loss: 2.7161\n",
      "Epoch 41, Batch 109/157, Loss: 2.7264\n",
      "Epoch 41, Batch 110/157, Loss: 2.7353\n",
      "Epoch 41, Batch 111/157, Loss: 2.7245\n",
      "Epoch 41, Batch 112/157, Loss: 2.7204\n",
      "Epoch 41, Batch 113/157, Loss: 2.7033\n",
      "Epoch 41, Batch 114/157, Loss: 2.7476\n",
      "Epoch 41, Batch 115/157, Loss: 2.7147\n",
      "Epoch 41, Batch 116/157, Loss: 2.7291\n",
      "Epoch 41, Batch 117/157, Loss: 2.7305\n",
      "Epoch 41, Batch 118/157, Loss: 2.7171\n",
      "Epoch 41, Batch 119/157, Loss: 2.7097\n",
      "Epoch 41, Batch 120/157, Loss: 2.7020\n",
      "Epoch 41, Batch 121/157, Loss: 2.7162\n",
      "Epoch 41, Batch 122/157, Loss: 2.7188\n",
      "Epoch 41, Batch 123/157, Loss: 2.7099\n",
      "Epoch 41, Batch 124/157, Loss: 2.7284\n",
      "Epoch 41, Batch 125/157, Loss: 2.6980\n",
      "Epoch 41, Batch 126/157, Loss: 2.7136\n",
      "Epoch 41, Batch 127/157, Loss: 2.7297\n",
      "Epoch 41, Batch 128/157, Loss: 2.7292\n",
      "Epoch 41, Batch 129/157, Loss: 2.7404\n",
      "Epoch 41, Batch 130/157, Loss: 2.6904\n",
      "Epoch 41, Batch 131/157, Loss: 2.7021\n",
      "Epoch 41, Batch 132/157, Loss: 2.7117\n",
      "Epoch 41, Batch 133/157, Loss: 2.7322\n",
      "Epoch 41, Batch 134/157, Loss: 2.7113\n",
      "Epoch 41, Batch 135/157, Loss: 2.7203\n",
      "Epoch 41, Batch 136/157, Loss: 2.6900\n",
      "Epoch 41, Batch 137/157, Loss: 2.7095\n",
      "Epoch 41, Batch 138/157, Loss: 2.7234\n",
      "Epoch 41, Batch 139/157, Loss: 2.7065\n",
      "Epoch 41, Batch 140/157, Loss: 2.7413\n",
      "Epoch 41, Batch 141/157, Loss: 2.7287\n",
      "Epoch 41, Batch 142/157, Loss: 2.7475\n",
      "Epoch 41, Batch 143/157, Loss: 2.7209\n",
      "Epoch 41, Batch 144/157, Loss: 2.6992\n",
      "Epoch 41, Batch 145/157, Loss: 2.7419\n",
      "Epoch 41, Batch 146/157, Loss: 2.6920\n",
      "Epoch 41, Batch 147/157, Loss: 2.7258\n",
      "Epoch 41, Batch 148/157, Loss: 2.7031\n",
      "Epoch 41, Batch 149/157, Loss: 2.7275\n",
      "Epoch 41, Batch 150/157, Loss: 2.7298\n",
      "Epoch 41, Batch 151/157, Loss: 2.6999\n",
      "Epoch 41, Batch 152/157, Loss: 2.7261\n",
      "Epoch 41, Batch 153/157, Loss: 2.6896\n",
      "Epoch 41, Batch 154/157, Loss: 2.7427\n",
      "Epoch 41, Batch 155/157, Loss: 2.6986\n",
      "Epoch 41, Batch 156/157, Loss: 2.6961\n",
      "Epoch 41, Batch 157/157, Loss: 2.6866\n",
      "Epoch 41/50, Average Loss: 2.7171\n",
      "Epoch 42, Batch 1/157, Loss: 2.7146\n",
      "Epoch 42, Batch 2/157, Loss: 2.7112\n",
      "Epoch 42, Batch 3/157, Loss: 2.7183\n",
      "Epoch 42, Batch 4/157, Loss: 2.7331\n",
      "Epoch 42, Batch 5/157, Loss: 2.7251\n",
      "Epoch 42, Batch 6/157, Loss: 2.6929\n",
      "Epoch 42, Batch 7/157, Loss: 2.7230\n",
      "Epoch 42, Batch 8/157, Loss: 2.7488\n",
      "Epoch 42, Batch 9/157, Loss: 2.7045\n",
      "Epoch 42, Batch 10/157, Loss: 2.7233\n",
      "Epoch 42, Batch 11/157, Loss: 2.7345\n",
      "Epoch 42, Batch 12/157, Loss: 2.6966\n",
      "Epoch 42, Batch 13/157, Loss: 2.7447\n",
      "Epoch 42, Batch 14/157, Loss: 2.7068\n",
      "Epoch 42, Batch 15/157, Loss: 2.7085\n",
      "Epoch 42, Batch 16/157, Loss: 2.7000\n",
      "Epoch 42, Batch 17/157, Loss: 2.7023\n",
      "Epoch 42, Batch 18/157, Loss: 2.7145\n",
      "Epoch 42, Batch 19/157, Loss: 2.7192\n",
      "Epoch 42, Batch 20/157, Loss: 2.7099\n",
      "Epoch 42, Batch 21/157, Loss: 2.6900\n",
      "Epoch 42, Batch 22/157, Loss: 2.7004\n",
      "Epoch 42, Batch 23/157, Loss: 2.6719\n",
      "Epoch 42, Batch 24/157, Loss: 2.7011\n",
      "Epoch 42, Batch 25/157, Loss: 2.6967\n",
      "Epoch 42, Batch 26/157, Loss: 2.7023\n",
      "Epoch 42, Batch 27/157, Loss: 2.7246\n",
      "Epoch 42, Batch 28/157, Loss: 2.6772\n",
      "Epoch 42, Batch 29/157, Loss: 2.7004\n",
      "Epoch 42, Batch 30/157, Loss: 2.7262\n",
      "Epoch 42, Batch 31/157, Loss: 2.7375\n",
      "Epoch 42, Batch 32/157, Loss: 2.7090\n",
      "Epoch 42, Batch 33/157, Loss: 2.7314\n",
      "Epoch 42, Batch 34/157, Loss: 2.6817\n",
      "Epoch 42, Batch 35/157, Loss: 2.6816\n",
      "Epoch 42, Batch 36/157, Loss: 2.6888\n",
      "Epoch 42, Batch 37/157, Loss: 2.7370\n",
      "Epoch 42, Batch 38/157, Loss: 2.7484\n",
      "Epoch 42, Batch 39/157, Loss: 2.6795\n",
      "Epoch 42, Batch 40/157, Loss: 2.7712\n",
      "Epoch 42, Batch 41/157, Loss: 2.7745\n",
      "Epoch 42, Batch 42/157, Loss: 2.7483\n",
      "Epoch 42, Batch 43/157, Loss: 2.7138\n",
      "Epoch 42, Batch 44/157, Loss: 2.7270\n",
      "Epoch 42, Batch 45/157, Loss: 2.7257\n",
      "Epoch 42, Batch 46/157, Loss: 2.7419\n",
      "Epoch 42, Batch 47/157, Loss: 2.7381\n",
      "Epoch 42, Batch 48/157, Loss: 2.7288\n",
      "Epoch 42, Batch 49/157, Loss: 2.7257\n",
      "Epoch 42, Batch 50/157, Loss: 2.7064\n",
      "Epoch 42, Batch 51/157, Loss: 2.6822\n",
      "Epoch 42, Batch 52/157, Loss: 2.7052\n",
      "Epoch 42, Batch 53/157, Loss: 2.7155\n",
      "Epoch 42, Batch 54/157, Loss: 2.7180\n",
      "Epoch 42, Batch 55/157, Loss: 2.6745\n",
      "Epoch 42, Batch 56/157, Loss: 2.7228\n",
      "Epoch 42, Batch 57/157, Loss: 2.7172\n",
      "Epoch 42, Batch 58/157, Loss: 2.7192\n",
      "Epoch 42, Batch 59/157, Loss: 2.7159\n",
      "Epoch 42, Batch 60/157, Loss: 2.6960\n",
      "Epoch 42, Batch 61/157, Loss: 2.7131\n",
      "Epoch 42, Batch 62/157, Loss: 2.6989\n",
      "Epoch 42, Batch 63/157, Loss: 2.7563\n",
      "Epoch 42, Batch 64/157, Loss: 2.7475\n",
      "Epoch 42, Batch 65/157, Loss: 2.6983\n",
      "Epoch 42, Batch 66/157, Loss: 2.7280\n",
      "Epoch 42, Batch 67/157, Loss: 2.7245\n",
      "Epoch 42, Batch 68/157, Loss: 2.7301\n",
      "Epoch 42, Batch 69/157, Loss: 2.7270\n",
      "Epoch 42, Batch 70/157, Loss: 2.7260\n",
      "Epoch 42, Batch 71/157, Loss: 2.7437\n",
      "Epoch 42, Batch 72/157, Loss: 2.6877\n",
      "Epoch 42, Batch 73/157, Loss: 2.6793\n",
      "Epoch 42, Batch 74/157, Loss: 2.7329\n",
      "Epoch 42, Batch 75/157, Loss: 2.7796\n",
      "Epoch 42, Batch 76/157, Loss: 2.7012\n",
      "Epoch 42, Batch 77/157, Loss: 2.6900\n",
      "Epoch 42, Batch 78/157, Loss: 2.7074\n",
      "Epoch 42, Batch 79/157, Loss: 2.7589\n",
      "Epoch 42, Batch 80/157, Loss: 2.7178\n",
      "Epoch 42, Batch 81/157, Loss: 2.7157\n",
      "Epoch 42, Batch 82/157, Loss: 2.6987\n",
      "Epoch 42, Batch 83/157, Loss: 2.6810\n",
      "Epoch 42, Batch 84/157, Loss: 2.6960\n",
      "Epoch 42, Batch 85/157, Loss: 2.7041\n",
      "Epoch 42, Batch 86/157, Loss: 2.7041\n",
      "Epoch 42, Batch 87/157, Loss: 2.7107\n",
      "Epoch 42, Batch 88/157, Loss: 2.6794\n",
      "Epoch 42, Batch 89/157, Loss: 2.7201\n",
      "Epoch 42, Batch 90/157, Loss: 2.6695\n",
      "Epoch 42, Batch 91/157, Loss: 2.7608\n",
      "Epoch 42, Batch 92/157, Loss: 2.6923\n",
      "Epoch 42, Batch 93/157, Loss: 2.7138\n",
      "Epoch 42, Batch 94/157, Loss: 2.6966\n",
      "Epoch 42, Batch 95/157, Loss: 2.7442\n",
      "Epoch 42, Batch 96/157, Loss: 2.7323\n",
      "Epoch 42, Batch 97/157, Loss: 2.6704\n",
      "Epoch 42, Batch 98/157, Loss: 2.7523\n",
      "Epoch 42, Batch 99/157, Loss: 2.7546\n",
      "Epoch 42, Batch 100/157, Loss: 2.7026\n",
      "Epoch 42, Batch 101/157, Loss: 2.7391\n",
      "Epoch 42, Batch 102/157, Loss: 2.7006\n",
      "Epoch 42, Batch 103/157, Loss: 2.7215\n",
      "Epoch 42, Batch 104/157, Loss: 2.7226\n",
      "Epoch 42, Batch 105/157, Loss: 2.6875\n",
      "Epoch 42, Batch 106/157, Loss: 2.7275\n",
      "Epoch 42, Batch 107/157, Loss: 2.7311\n",
      "Epoch 42, Batch 108/157, Loss: 2.7142\n",
      "Epoch 42, Batch 109/157, Loss: 2.7359\n",
      "Epoch 42, Batch 110/157, Loss: 2.7355\n",
      "Epoch 42, Batch 111/157, Loss: 2.6941\n",
      "Epoch 42, Batch 112/157, Loss: 2.7295\n",
      "Epoch 42, Batch 113/157, Loss: 2.6670\n",
      "Epoch 42, Batch 114/157, Loss: 2.7192\n",
      "Epoch 42, Batch 115/157, Loss: 2.6836\n",
      "Epoch 42, Batch 116/157, Loss: 2.7253\n",
      "Epoch 42, Batch 117/157, Loss: 2.6988\n",
      "Epoch 42, Batch 118/157, Loss: 2.7194\n",
      "Epoch 42, Batch 119/157, Loss: 2.6977\n",
      "Epoch 42, Batch 120/157, Loss: 2.7487\n",
      "Epoch 42, Batch 121/157, Loss: 2.7270\n",
      "Epoch 42, Batch 122/157, Loss: 2.7147\n",
      "Epoch 42, Batch 123/157, Loss: 2.7058\n",
      "Epoch 42, Batch 124/157, Loss: 2.7304\n",
      "Epoch 42, Batch 125/157, Loss: 2.6873\n",
      "Epoch 42, Batch 126/157, Loss: 2.6993\n",
      "Epoch 42, Batch 127/157, Loss: 2.7416\n",
      "Epoch 42, Batch 128/157, Loss: 2.7127\n",
      "Epoch 42, Batch 129/157, Loss: 2.6659\n",
      "Epoch 42, Batch 130/157, Loss: 2.7645\n",
      "Epoch 42, Batch 131/157, Loss: 2.7429\n",
      "Epoch 42, Batch 132/157, Loss: 2.7354\n",
      "Epoch 42, Batch 133/157, Loss: 2.6860\n",
      "Epoch 42, Batch 134/157, Loss: 2.7219\n",
      "Epoch 42, Batch 135/157, Loss: 2.6981\n",
      "Epoch 42, Batch 136/157, Loss: 2.6997\n",
      "Epoch 42, Batch 137/157, Loss: 2.7138\n",
      "Epoch 42, Batch 138/157, Loss: 2.7136\n",
      "Epoch 42, Batch 139/157, Loss: 2.7142\n",
      "Epoch 42, Batch 140/157, Loss: 2.7142\n",
      "Epoch 42, Batch 141/157, Loss: 2.7137\n",
      "Epoch 42, Batch 142/157, Loss: 2.7248\n",
      "Epoch 42, Batch 143/157, Loss: 2.7213\n",
      "Epoch 42, Batch 144/157, Loss: 2.7223\n",
      "Epoch 42, Batch 145/157, Loss: 2.7051\n",
      "Epoch 42, Batch 146/157, Loss: 2.7104\n",
      "Epoch 42, Batch 147/157, Loss: 2.6830\n",
      "Epoch 42, Batch 148/157, Loss: 2.7226\n",
      "Epoch 42, Batch 149/157, Loss: 2.7062\n",
      "Epoch 42, Batch 150/157, Loss: 2.7160\n",
      "Epoch 42, Batch 151/157, Loss: 2.7503\n",
      "Epoch 42, Batch 152/157, Loss: 2.7517\n",
      "Epoch 42, Batch 153/157, Loss: 2.7237\n",
      "Epoch 42, Batch 154/157, Loss: 2.6905\n",
      "Epoch 42, Batch 155/157, Loss: 2.7276\n",
      "Epoch 42, Batch 156/157, Loss: 2.7335\n",
      "Epoch 42, Batch 157/157, Loss: 2.8046\n",
      "Epoch 42/50, Average Loss: 2.7161\n",
      "Epoch 43, Batch 1/157, Loss: 2.6745\n",
      "Epoch 43, Batch 2/157, Loss: 2.6977\n",
      "Epoch 43, Batch 3/157, Loss: 2.7344\n",
      "Epoch 43, Batch 4/157, Loss: 2.7233\n",
      "Epoch 43, Batch 5/157, Loss: 2.7353\n",
      "Epoch 43, Batch 6/157, Loss: 2.6860\n",
      "Epoch 43, Batch 7/157, Loss: 2.6865\n",
      "Epoch 43, Batch 8/157, Loss: 2.7012\n",
      "Epoch 43, Batch 9/157, Loss: 2.6984\n",
      "Epoch 43, Batch 10/157, Loss: 2.6971\n",
      "Epoch 43, Batch 11/157, Loss: 2.7259\n",
      "Epoch 43, Batch 12/157, Loss: 2.7056\n",
      "Epoch 43, Batch 13/157, Loss: 2.7216\n",
      "Epoch 43, Batch 14/157, Loss: 2.7248\n",
      "Epoch 43, Batch 15/157, Loss: 2.7529\n",
      "Epoch 43, Batch 16/157, Loss: 2.6972\n",
      "Epoch 43, Batch 17/157, Loss: 2.7344\n",
      "Epoch 43, Batch 18/157, Loss: 2.7312\n",
      "Epoch 43, Batch 19/157, Loss: 2.7082\n",
      "Epoch 43, Batch 20/157, Loss: 2.7285\n",
      "Epoch 43, Batch 21/157, Loss: 2.7299\n",
      "Epoch 43, Batch 22/157, Loss: 2.7300\n",
      "Epoch 43, Batch 23/157, Loss: 2.7024\n",
      "Epoch 43, Batch 24/157, Loss: 2.7066\n",
      "Epoch 43, Batch 25/157, Loss: 2.7465\n",
      "Epoch 43, Batch 26/157, Loss: 2.7283\n",
      "Epoch 43, Batch 27/157, Loss: 2.7090\n",
      "Epoch 43, Batch 28/157, Loss: 2.7020\n",
      "Epoch 43, Batch 29/157, Loss: 2.6980\n",
      "Epoch 43, Batch 30/157, Loss: 2.7102\n",
      "Epoch 43, Batch 31/157, Loss: 2.7224\n",
      "Epoch 43, Batch 32/157, Loss: 2.6958\n",
      "Epoch 43, Batch 33/157, Loss: 2.7156\n",
      "Epoch 43, Batch 34/157, Loss: 2.7169\n",
      "Epoch 43, Batch 35/157, Loss: 2.7219\n",
      "Epoch 43, Batch 36/157, Loss: 2.7095\n",
      "Epoch 43, Batch 37/157, Loss: 2.7351\n",
      "Epoch 43, Batch 38/157, Loss: 2.7070\n",
      "Epoch 43, Batch 39/157, Loss: 2.7268\n",
      "Epoch 43, Batch 40/157, Loss: 2.7405\n",
      "Epoch 43, Batch 41/157, Loss: 2.6641\n",
      "Epoch 43, Batch 42/157, Loss: 2.7003\n",
      "Epoch 43, Batch 43/157, Loss: 2.6914\n",
      "Epoch 43, Batch 44/157, Loss: 2.6696\n",
      "Epoch 43, Batch 45/157, Loss: 2.6968\n",
      "Epoch 43, Batch 46/157, Loss: 2.6978\n",
      "Epoch 43, Batch 47/157, Loss: 2.7383\n",
      "Epoch 43, Batch 48/157, Loss: 2.6918\n",
      "Epoch 43, Batch 49/157, Loss: 2.7091\n",
      "Epoch 43, Batch 50/157, Loss: 2.7284\n",
      "Epoch 43, Batch 51/157, Loss: 2.7225\n",
      "Epoch 43, Batch 52/157, Loss: 2.7522\n",
      "Epoch 43, Batch 53/157, Loss: 2.7228\n",
      "Epoch 43, Batch 54/157, Loss: 2.7339\n",
      "Epoch 43, Batch 55/157, Loss: 2.6834\n",
      "Epoch 43, Batch 56/157, Loss: 2.7292\n",
      "Epoch 43, Batch 57/157, Loss: 2.7367\n",
      "Epoch 43, Batch 58/157, Loss: 2.7174\n",
      "Epoch 43, Batch 59/157, Loss: 2.7079\n",
      "Epoch 43, Batch 60/157, Loss: 2.6703\n",
      "Epoch 43, Batch 61/157, Loss: 2.7351\n",
      "Epoch 43, Batch 62/157, Loss: 2.6970\n",
      "Epoch 43, Batch 63/157, Loss: 2.7082\n",
      "Epoch 43, Batch 64/157, Loss: 2.6923\n",
      "Epoch 43, Batch 65/157, Loss: 2.7174\n",
      "Epoch 43, Batch 66/157, Loss: 2.7339\n",
      "Epoch 43, Batch 67/157, Loss: 2.7777\n",
      "Epoch 43, Batch 68/157, Loss: 2.7284\n",
      "Epoch 43, Batch 69/157, Loss: 2.7768\n",
      "Epoch 43, Batch 70/157, Loss: 2.7748\n",
      "Epoch 43, Batch 71/157, Loss: 2.7206\n",
      "Epoch 43, Batch 72/157, Loss: 2.6776\n",
      "Epoch 43, Batch 73/157, Loss: 2.6860\n",
      "Epoch 43, Batch 74/157, Loss: 2.7528\n",
      "Epoch 43, Batch 75/157, Loss: 2.7339\n",
      "Epoch 43, Batch 76/157, Loss: 2.7163\n",
      "Epoch 43, Batch 77/157, Loss: 2.7077\n",
      "Epoch 43, Batch 78/157, Loss: 2.6895\n",
      "Epoch 43, Batch 79/157, Loss: 2.7240\n",
      "Epoch 43, Batch 80/157, Loss: 2.7173\n",
      "Epoch 43, Batch 81/157, Loss: 2.7082\n",
      "Epoch 43, Batch 82/157, Loss: 2.7013\n",
      "Epoch 43, Batch 83/157, Loss: 2.7147\n",
      "Epoch 43, Batch 84/157, Loss: 2.7019\n",
      "Epoch 43, Batch 85/157, Loss: 2.7147\n",
      "Epoch 43, Batch 86/157, Loss: 2.6936\n",
      "Epoch 43, Batch 87/157, Loss: 2.7219\n",
      "Epoch 43, Batch 88/157, Loss: 2.7139\n",
      "Epoch 43, Batch 89/157, Loss: 2.7096\n",
      "Epoch 43, Batch 90/157, Loss: 2.6798\n",
      "Epoch 43, Batch 91/157, Loss: 2.7245\n",
      "Epoch 43, Batch 92/157, Loss: 2.7007\n",
      "Epoch 43, Batch 93/157, Loss: 2.6479\n",
      "Epoch 43, Batch 94/157, Loss: 2.7191\n",
      "Epoch 43, Batch 95/157, Loss: 2.7584\n",
      "Epoch 43, Batch 96/157, Loss: 2.7333\n",
      "Epoch 43, Batch 97/157, Loss: 2.6857\n",
      "Epoch 43, Batch 98/157, Loss: 2.6861\n",
      "Epoch 43, Batch 99/157, Loss: 2.7076\n",
      "Epoch 43, Batch 100/157, Loss: 2.7572\n",
      "Epoch 43, Batch 101/157, Loss: 2.7112\n",
      "Epoch 43, Batch 102/157, Loss: 2.7494\n",
      "Epoch 43, Batch 103/157, Loss: 2.7615\n",
      "Epoch 43, Batch 104/157, Loss: 2.7378\n",
      "Epoch 43, Batch 105/157, Loss: 2.7311\n",
      "Epoch 43, Batch 106/157, Loss: 2.7505\n",
      "Epoch 43, Batch 107/157, Loss: 2.7274\n",
      "Epoch 43, Batch 108/157, Loss: 2.6788\n",
      "Epoch 43, Batch 109/157, Loss: 2.7496\n",
      "Epoch 43, Batch 110/157, Loss: 2.6884\n",
      "Epoch 43, Batch 111/157, Loss: 2.7053\n",
      "Epoch 43, Batch 112/157, Loss: 2.7217\n",
      "Epoch 43, Batch 113/157, Loss: 2.6767\n",
      "Epoch 43, Batch 114/157, Loss: 2.6994\n",
      "Epoch 43, Batch 115/157, Loss: 2.7135\n",
      "Epoch 43, Batch 116/157, Loss: 2.7347\n",
      "Epoch 43, Batch 117/157, Loss: 2.6845\n",
      "Epoch 43, Batch 118/157, Loss: 2.7329\n",
      "Epoch 43, Batch 119/157, Loss: 2.6988\n",
      "Epoch 43, Batch 120/157, Loss: 2.7102\n",
      "Epoch 43, Batch 121/157, Loss: 2.7473\n",
      "Epoch 43, Batch 122/157, Loss: 2.6844\n",
      "Epoch 43, Batch 123/157, Loss: 2.7198\n",
      "Epoch 43, Batch 124/157, Loss: 2.6892\n",
      "Epoch 43, Batch 125/157, Loss: 2.7142\n",
      "Epoch 43, Batch 126/157, Loss: 2.7540\n",
      "Epoch 43, Batch 127/157, Loss: 2.7120\n",
      "Epoch 43, Batch 128/157, Loss: 2.7368\n",
      "Epoch 43, Batch 129/157, Loss: 2.7078\n",
      "Epoch 43, Batch 130/157, Loss: 2.7429\n",
      "Epoch 43, Batch 131/157, Loss: 2.7265\n",
      "Epoch 43, Batch 132/157, Loss: 2.7235\n",
      "Epoch 43, Batch 133/157, Loss: 2.6916\n",
      "Epoch 43, Batch 134/157, Loss: 2.6583\n",
      "Epoch 43, Batch 135/157, Loss: 2.7063\n",
      "Epoch 43, Batch 136/157, Loss: 2.7212\n",
      "Epoch 43, Batch 137/157, Loss: 2.7066\n",
      "Epoch 43, Batch 138/157, Loss: 2.7150\n",
      "Epoch 43, Batch 139/157, Loss: 2.6617\n",
      "Epoch 43, Batch 140/157, Loss: 2.7290\n",
      "Epoch 43, Batch 141/157, Loss: 2.7207\n",
      "Epoch 43, Batch 142/157, Loss: 2.7456\n",
      "Epoch 43, Batch 143/157, Loss: 2.7186\n",
      "Epoch 43, Batch 144/157, Loss: 2.7564\n",
      "Epoch 43, Batch 145/157, Loss: 2.6898\n",
      "Epoch 43, Batch 146/157, Loss: 2.7467\n",
      "Epoch 43, Batch 147/157, Loss: 2.6940\n",
      "Epoch 43, Batch 148/157, Loss: 2.6922\n",
      "Epoch 43, Batch 149/157, Loss: 2.7444\n",
      "Epoch 43, Batch 150/157, Loss: 2.7274\n",
      "Epoch 43, Batch 151/157, Loss: 2.7208\n",
      "Epoch 43, Batch 152/157, Loss: 2.7532\n",
      "Epoch 43, Batch 153/157, Loss: 2.6573\n",
      "Epoch 43, Batch 154/157, Loss: 2.7078\n",
      "Epoch 43, Batch 155/157, Loss: 2.7263\n",
      "Epoch 43, Batch 156/157, Loss: 2.7032\n",
      "Epoch 43, Batch 157/157, Loss: 2.6462\n",
      "Epoch 43/50, Average Loss: 2.7146\n",
      "Epoch 44, Batch 1/157, Loss: 2.6782\n",
      "Epoch 44, Batch 2/157, Loss: 2.7024\n",
      "Epoch 44, Batch 3/157, Loss: 2.7077\n",
      "Epoch 44, Batch 4/157, Loss: 2.6791\n",
      "Epoch 44, Batch 5/157, Loss: 2.7504\n",
      "Epoch 44, Batch 6/157, Loss: 2.7228\n",
      "Epoch 44, Batch 7/157, Loss: 2.7289\n",
      "Epoch 44, Batch 8/157, Loss: 2.7442\n",
      "Epoch 44, Batch 9/157, Loss: 2.6728\n",
      "Epoch 44, Batch 10/157, Loss: 2.7265\n",
      "Epoch 44, Batch 11/157, Loss: 2.7200\n",
      "Epoch 44, Batch 12/157, Loss: 2.7581\n",
      "Epoch 44, Batch 13/157, Loss: 2.7236\n",
      "Epoch 44, Batch 14/157, Loss: 2.7508\n",
      "Epoch 44, Batch 15/157, Loss: 2.7567\n",
      "Epoch 44, Batch 16/157, Loss: 2.6682\n",
      "Epoch 44, Batch 17/157, Loss: 2.7354\n",
      "Epoch 44, Batch 18/157, Loss: 2.7296\n",
      "Epoch 44, Batch 19/157, Loss: 2.7349\n",
      "Epoch 44, Batch 20/157, Loss: 2.7040\n",
      "Epoch 44, Batch 21/157, Loss: 2.7043\n",
      "Epoch 44, Batch 22/157, Loss: 2.7333\n",
      "Epoch 44, Batch 23/157, Loss: 2.7302\n",
      "Epoch 44, Batch 24/157, Loss: 2.7025\n",
      "Epoch 44, Batch 25/157, Loss: 2.7201\n",
      "Epoch 44, Batch 26/157, Loss: 2.7160\n",
      "Epoch 44, Batch 27/157, Loss: 2.6777\n",
      "Epoch 44, Batch 28/157, Loss: 2.7272\n",
      "Epoch 44, Batch 29/157, Loss: 2.7227\n",
      "Epoch 44, Batch 30/157, Loss: 2.7181\n",
      "Epoch 44, Batch 31/157, Loss: 2.6838\n",
      "Epoch 44, Batch 32/157, Loss: 2.7404\n",
      "Epoch 44, Batch 33/157, Loss: 2.7334\n",
      "Epoch 44, Batch 34/157, Loss: 2.7348\n",
      "Epoch 44, Batch 35/157, Loss: 2.6951\n",
      "Epoch 44, Batch 36/157, Loss: 2.7507\n",
      "Epoch 44, Batch 37/157, Loss: 2.7232\n",
      "Epoch 44, Batch 38/157, Loss: 2.7484\n",
      "Epoch 44, Batch 39/157, Loss: 2.7011\n",
      "Epoch 44, Batch 40/157, Loss: 2.7031\n",
      "Epoch 44, Batch 41/157, Loss: 2.7409\n",
      "Epoch 44, Batch 42/157, Loss: 2.7311\n",
      "Epoch 44, Batch 43/157, Loss: 2.6857\n",
      "Epoch 44, Batch 44/157, Loss: 2.7011\n",
      "Epoch 44, Batch 45/157, Loss: 2.7616\n",
      "Epoch 44, Batch 46/157, Loss: 2.7044\n",
      "Epoch 44, Batch 47/157, Loss: 2.7786\n",
      "Epoch 44, Batch 48/157, Loss: 2.7029\n",
      "Epoch 44, Batch 49/157, Loss: 2.7248\n",
      "Epoch 44, Batch 50/157, Loss: 2.7719\n",
      "Epoch 44, Batch 51/157, Loss: 2.6981\n",
      "Epoch 44, Batch 52/157, Loss: 2.7194\n",
      "Epoch 44, Batch 53/157, Loss: 2.7186\n",
      "Epoch 44, Batch 54/157, Loss: 2.7526\n",
      "Epoch 44, Batch 55/157, Loss: 2.7154\n",
      "Epoch 44, Batch 56/157, Loss: 2.7123\n",
      "Epoch 44, Batch 57/157, Loss: 2.7076\n",
      "Epoch 44, Batch 58/157, Loss: 2.7173\n",
      "Epoch 44, Batch 59/157, Loss: 2.7440\n",
      "Epoch 44, Batch 60/157, Loss: 2.7201\n",
      "Epoch 44, Batch 61/157, Loss: 2.7369\n",
      "Epoch 44, Batch 62/157, Loss: 2.6915\n",
      "Epoch 44, Batch 63/157, Loss: 2.7062\n",
      "Epoch 44, Batch 64/157, Loss: 2.7017\n",
      "Epoch 44, Batch 65/157, Loss: 2.7172\n",
      "Epoch 44, Batch 66/157, Loss: 2.7280\n",
      "Epoch 44, Batch 67/157, Loss: 2.6934\n",
      "Epoch 44, Batch 68/157, Loss: 2.7398\n",
      "Epoch 44, Batch 69/157, Loss: 2.6563\n",
      "Epoch 44, Batch 70/157, Loss: 2.6891\n",
      "Epoch 44, Batch 71/157, Loss: 2.6902\n",
      "Epoch 44, Batch 72/157, Loss: 2.7272\n",
      "Epoch 44, Batch 73/157, Loss: 2.7165\n",
      "Epoch 44, Batch 74/157, Loss: 2.7193\n",
      "Epoch 44, Batch 75/157, Loss: 2.7455\n",
      "Epoch 44, Batch 76/157, Loss: 2.6885\n",
      "Epoch 44, Batch 77/157, Loss: 2.7063\n",
      "Epoch 44, Batch 78/157, Loss: 2.7583\n",
      "Epoch 44, Batch 79/157, Loss: 2.7408\n",
      "Epoch 44, Batch 80/157, Loss: 2.7297\n",
      "Epoch 44, Batch 81/157, Loss: 2.7347\n",
      "Epoch 44, Batch 82/157, Loss: 2.7257\n",
      "Epoch 44, Batch 83/157, Loss: 2.6867\n",
      "Epoch 44, Batch 84/157, Loss: 2.6979\n",
      "Epoch 44, Batch 85/157, Loss: 2.7169\n",
      "Epoch 44, Batch 86/157, Loss: 2.7009\n",
      "Epoch 44, Batch 87/157, Loss: 2.7203\n",
      "Epoch 44, Batch 88/157, Loss: 2.7286\n",
      "Epoch 44, Batch 89/157, Loss: 2.7291\n",
      "Epoch 44, Batch 90/157, Loss: 2.7177\n",
      "Epoch 44, Batch 91/157, Loss: 2.7325\n",
      "Epoch 44, Batch 92/157, Loss: 2.7153\n",
      "Epoch 44, Batch 93/157, Loss: 2.7542\n",
      "Epoch 44, Batch 94/157, Loss: 2.7065\n",
      "Epoch 44, Batch 95/157, Loss: 2.7065\n",
      "Epoch 44, Batch 96/157, Loss: 2.6987\n",
      "Epoch 44, Batch 97/157, Loss: 2.7248\n",
      "Epoch 44, Batch 98/157, Loss: 2.7378\n",
      "Epoch 44, Batch 99/157, Loss: 2.6749\n",
      "Epoch 44, Batch 100/157, Loss: 2.7312\n",
      "Epoch 44, Batch 101/157, Loss: 2.6927\n",
      "Epoch 44, Batch 102/157, Loss: 2.7040\n",
      "Epoch 44, Batch 103/157, Loss: 2.7281\n",
      "Epoch 44, Batch 104/157, Loss: 2.6889\n",
      "Epoch 44, Batch 105/157, Loss: 2.6983\n",
      "Epoch 44, Batch 106/157, Loss: 2.6882\n",
      "Epoch 44, Batch 107/157, Loss: 2.7199\n",
      "Epoch 44, Batch 108/157, Loss: 2.7331\n",
      "Epoch 44, Batch 109/157, Loss: 2.7299\n",
      "Epoch 44, Batch 110/157, Loss: 2.7046\n",
      "Epoch 44, Batch 111/157, Loss: 2.7252\n",
      "Epoch 44, Batch 112/157, Loss: 2.7443\n",
      "Epoch 44, Batch 113/157, Loss: 2.7434\n",
      "Epoch 44, Batch 114/157, Loss: 2.8102\n",
      "Epoch 44, Batch 115/157, Loss: 2.7104\n",
      "Epoch 44, Batch 116/157, Loss: 2.7421\n",
      "Epoch 44, Batch 117/157, Loss: 2.7405\n",
      "Epoch 44, Batch 118/157, Loss: 2.7354\n",
      "Epoch 44, Batch 119/157, Loss: 2.7047\n",
      "Epoch 44, Batch 120/157, Loss: 2.6937\n",
      "Epoch 44, Batch 121/157, Loss: 2.6893\n",
      "Epoch 44, Batch 122/157, Loss: 2.7211\n",
      "Epoch 44, Batch 123/157, Loss: 2.7058\n",
      "Epoch 44, Batch 124/157, Loss: 2.7299\n",
      "Epoch 44, Batch 125/157, Loss: 2.7006\n",
      "Epoch 44, Batch 126/157, Loss: 2.7174\n",
      "Epoch 44, Batch 127/157, Loss: 2.7151\n",
      "Epoch 44, Batch 128/157, Loss: 2.7614\n",
      "Epoch 44, Batch 129/157, Loss: 2.7094\n",
      "Epoch 44, Batch 130/157, Loss: 2.6963\n",
      "Epoch 44, Batch 131/157, Loss: 2.7065\n",
      "Epoch 44, Batch 132/157, Loss: 2.7253\n",
      "Epoch 44, Batch 133/157, Loss: 2.7109\n",
      "Epoch 44, Batch 134/157, Loss: 2.7087\n",
      "Epoch 44, Batch 135/157, Loss: 2.6894\n",
      "Epoch 44, Batch 136/157, Loss: 2.7382\n",
      "Epoch 44, Batch 137/157, Loss: 2.6999\n",
      "Epoch 44, Batch 138/157, Loss: 2.7022\n",
      "Epoch 44, Batch 139/157, Loss: 2.6887\n",
      "Epoch 44, Batch 140/157, Loss: 2.7044\n",
      "Epoch 44, Batch 141/157, Loss: 2.7320\n",
      "Epoch 44, Batch 142/157, Loss: 2.7213\n",
      "Epoch 44, Batch 143/157, Loss: 2.7031\n",
      "Epoch 44, Batch 144/157, Loss: 2.7277\n",
      "Epoch 44, Batch 145/157, Loss: 2.7438\n",
      "Epoch 44, Batch 146/157, Loss: 2.7064\n",
      "Epoch 44, Batch 147/157, Loss: 2.7200\n",
      "Epoch 44, Batch 148/157, Loss: 2.6717\n",
      "Epoch 44, Batch 149/157, Loss: 2.6773\n",
      "Epoch 44, Batch 150/157, Loss: 2.7018\n",
      "Epoch 44, Batch 151/157, Loss: 2.6929\n",
      "Epoch 44, Batch 152/157, Loss: 2.7030\n",
      "Epoch 44, Batch 153/157, Loss: 2.6987\n",
      "Epoch 44, Batch 154/157, Loss: 2.6972\n",
      "Epoch 44, Batch 155/157, Loss: 2.7176\n",
      "Epoch 44, Batch 156/157, Loss: 2.7459\n",
      "Epoch 44, Batch 157/157, Loss: 2.7474\n",
      "Epoch 44/50, Average Loss: 2.7176\n",
      "Epoch 45, Batch 1/157, Loss: 2.7109\n",
      "Epoch 45, Batch 2/157, Loss: 2.7236\n",
      "Epoch 45, Batch 3/157, Loss: 2.7468\n",
      "Epoch 45, Batch 4/157, Loss: 2.6666\n",
      "Epoch 45, Batch 5/157, Loss: 2.7228\n",
      "Epoch 45, Batch 6/157, Loss: 2.7106\n",
      "Epoch 45, Batch 7/157, Loss: 2.6781\n",
      "Epoch 45, Batch 8/157, Loss: 2.7334\n",
      "Epoch 45, Batch 9/157, Loss: 2.7104\n",
      "Epoch 45, Batch 10/157, Loss: 2.6696\n",
      "Epoch 45, Batch 11/157, Loss: 2.7224\n",
      "Epoch 45, Batch 12/157, Loss: 2.7232\n",
      "Epoch 45, Batch 13/157, Loss: 2.6941\n",
      "Epoch 45, Batch 14/157, Loss: 2.7095\n",
      "Epoch 45, Batch 15/157, Loss: 2.7334\n",
      "Epoch 45, Batch 16/157, Loss: 2.7765\n",
      "Epoch 45, Batch 17/157, Loss: 2.7386\n",
      "Epoch 45, Batch 18/157, Loss: 2.6816\n",
      "Epoch 45, Batch 19/157, Loss: 2.7636\n",
      "Epoch 45, Batch 20/157, Loss: 2.7090\n",
      "Epoch 45, Batch 21/157, Loss: 2.7192\n",
      "Epoch 45, Batch 22/157, Loss: 2.7463\n",
      "Epoch 45, Batch 23/157, Loss: 2.7352\n",
      "Epoch 45, Batch 24/157, Loss: 2.7464\n",
      "Epoch 45, Batch 25/157, Loss: 2.6956\n",
      "Epoch 45, Batch 26/157, Loss: 2.7075\n",
      "Epoch 45, Batch 27/157, Loss: 2.7214\n",
      "Epoch 45, Batch 28/157, Loss: 2.7088\n",
      "Epoch 45, Batch 29/157, Loss: 2.7108\n",
      "Epoch 45, Batch 30/157, Loss: 2.7310\n",
      "Epoch 45, Batch 31/157, Loss: 2.7228\n",
      "Epoch 45, Batch 32/157, Loss: 2.7342\n",
      "Epoch 45, Batch 33/157, Loss: 2.7007\n",
      "Epoch 45, Batch 34/157, Loss: 2.7156\n",
      "Epoch 45, Batch 35/157, Loss: 2.7181\n",
      "Epoch 45, Batch 36/157, Loss: 2.7066\n",
      "Epoch 45, Batch 37/157, Loss: 2.7142\n",
      "Epoch 45, Batch 38/157, Loss: 2.7104\n",
      "Epoch 45, Batch 39/157, Loss: 2.7034\n",
      "Epoch 45, Batch 40/157, Loss: 2.7184\n",
      "Epoch 45, Batch 41/157, Loss: 2.6954\n",
      "Epoch 45, Batch 42/157, Loss: 2.7374\n",
      "Epoch 45, Batch 43/157, Loss: 2.7141\n",
      "Epoch 45, Batch 44/157, Loss: 2.7323\n",
      "Epoch 45, Batch 45/157, Loss: 2.7004\n",
      "Epoch 45, Batch 46/157, Loss: 2.6939\n",
      "Epoch 45, Batch 47/157, Loss: 2.7282\n",
      "Epoch 45, Batch 48/157, Loss: 2.6986\n",
      "Epoch 45, Batch 49/157, Loss: 2.7137\n",
      "Epoch 45, Batch 50/157, Loss: 2.7002\n",
      "Epoch 45, Batch 51/157, Loss: 2.7227\n",
      "Epoch 45, Batch 52/157, Loss: 2.7169\n",
      "Epoch 45, Batch 53/157, Loss: 2.7396\n",
      "Epoch 45, Batch 54/157, Loss: 2.7324\n",
      "Epoch 45, Batch 55/157, Loss: 2.7190\n",
      "Epoch 45, Batch 56/157, Loss: 2.7375\n",
      "Epoch 45, Batch 57/157, Loss: 2.7059\n",
      "Epoch 45, Batch 58/157, Loss: 2.7177\n",
      "Epoch 45, Batch 59/157, Loss: 2.7373\n",
      "Epoch 45, Batch 60/157, Loss: 2.7151\n",
      "Epoch 45, Batch 61/157, Loss: 2.7555\n",
      "Epoch 45, Batch 62/157, Loss: 2.7076\n",
      "Epoch 45, Batch 63/157, Loss: 2.7418\n",
      "Epoch 45, Batch 64/157, Loss: 2.6927\n",
      "Epoch 45, Batch 65/157, Loss: 2.7062\n",
      "Epoch 45, Batch 66/157, Loss: 2.7139\n",
      "Epoch 45, Batch 67/157, Loss: 2.7160\n",
      "Epoch 45, Batch 68/157, Loss: 2.6933\n",
      "Epoch 45, Batch 69/157, Loss: 2.7166\n",
      "Epoch 45, Batch 70/157, Loss: 2.7453\n",
      "Epoch 45, Batch 71/157, Loss: 2.7205\n",
      "Epoch 45, Batch 72/157, Loss: 2.7303\n",
      "Epoch 45, Batch 73/157, Loss: 2.7350\n",
      "Epoch 45, Batch 74/157, Loss: 2.7332\n",
      "Epoch 45, Batch 75/157, Loss: 2.6624\n",
      "Epoch 45, Batch 76/157, Loss: 2.6882\n",
      "Epoch 45, Batch 77/157, Loss: 2.7821\n",
      "Epoch 45, Batch 78/157, Loss: 2.7251\n",
      "Epoch 45, Batch 79/157, Loss: 2.7137\n",
      "Epoch 45, Batch 80/157, Loss: 2.7454\n",
      "Epoch 45, Batch 81/157, Loss: 2.7139\n",
      "Epoch 45, Batch 82/157, Loss: 2.7570\n",
      "Epoch 45, Batch 83/157, Loss: 2.7227\n",
      "Epoch 45, Batch 84/157, Loss: 2.7354\n",
      "Epoch 45, Batch 85/157, Loss: 2.7554\n",
      "Epoch 45, Batch 86/157, Loss: 2.7145\n",
      "Epoch 45, Batch 87/157, Loss: 2.7215\n",
      "Epoch 45, Batch 88/157, Loss: 2.7533\n",
      "Epoch 45, Batch 89/157, Loss: 2.6941\n",
      "Epoch 45, Batch 90/157, Loss: 2.7073\n",
      "Epoch 45, Batch 91/157, Loss: 2.7042\n",
      "Epoch 45, Batch 92/157, Loss: 2.6824\n",
      "Epoch 45, Batch 93/157, Loss: 2.7058\n",
      "Epoch 45, Batch 94/157, Loss: 2.6874\n",
      "Epoch 45, Batch 95/157, Loss: 2.6977\n",
      "Epoch 45, Batch 96/157, Loss: 2.6914\n",
      "Epoch 45, Batch 97/157, Loss: 2.6926\n",
      "Epoch 45, Batch 98/157, Loss: 2.7076\n",
      "Epoch 45, Batch 99/157, Loss: 2.7127\n",
      "Epoch 45, Batch 100/157, Loss: 2.6758\n",
      "Epoch 45, Batch 101/157, Loss: 2.7213\n",
      "Epoch 45, Batch 102/157, Loss: 2.7375\n",
      "Epoch 45, Batch 103/157, Loss: 2.6847\n",
      "Epoch 45, Batch 104/157, Loss: 2.7007\n",
      "Epoch 45, Batch 105/157, Loss: 2.7188\n",
      "Epoch 45, Batch 106/157, Loss: 2.6882\n",
      "Epoch 45, Batch 107/157, Loss: 2.7604\n",
      "Epoch 45, Batch 108/157, Loss: 2.6892\n",
      "Epoch 45, Batch 109/157, Loss: 2.6589\n",
      "Epoch 45, Batch 110/157, Loss: 2.7240\n",
      "Epoch 45, Batch 111/157, Loss: 2.7167\n",
      "Epoch 45, Batch 112/157, Loss: 2.7042\n",
      "Epoch 45, Batch 113/157, Loss: 2.7052\n",
      "Epoch 45, Batch 114/157, Loss: 2.7061\n",
      "Epoch 45, Batch 115/157, Loss: 2.6573\n",
      "Epoch 45, Batch 116/157, Loss: 2.7278\n",
      "Epoch 45, Batch 117/157, Loss: 2.6797\n",
      "Epoch 45, Batch 118/157, Loss: 2.7091\n",
      "Epoch 45, Batch 119/157, Loss: 2.6661\n",
      "Epoch 45, Batch 120/157, Loss: 2.7074\n",
      "Epoch 45, Batch 121/157, Loss: 2.7227\n",
      "Epoch 45, Batch 122/157, Loss: 2.7482\n",
      "Epoch 45, Batch 123/157, Loss: 2.7050\n",
      "Epoch 45, Batch 124/157, Loss: 2.7036\n",
      "Epoch 45, Batch 125/157, Loss: 2.7010\n",
      "Epoch 45, Batch 126/157, Loss: 2.7424\n",
      "Epoch 45, Batch 127/157, Loss: 2.7319\n",
      "Epoch 45, Batch 128/157, Loss: 2.7195\n",
      "Epoch 45, Batch 129/157, Loss: 2.6843\n",
      "Epoch 45, Batch 130/157, Loss: 2.7321\n",
      "Epoch 45, Batch 131/157, Loss: 2.7697\n",
      "Epoch 45, Batch 132/157, Loss: 2.6935\n",
      "Epoch 45, Batch 133/157, Loss: 2.7186\n",
      "Epoch 45, Batch 134/157, Loss: 2.7007\n",
      "Epoch 45, Batch 135/157, Loss: 2.7667\n",
      "Epoch 45, Batch 136/157, Loss: 2.7459\n",
      "Epoch 45, Batch 137/157, Loss: 2.7762\n",
      "Epoch 45, Batch 138/157, Loss: 2.7241\n",
      "Epoch 45, Batch 139/157, Loss: 2.7105\n",
      "Epoch 45, Batch 140/157, Loss: 2.7230\n",
      "Epoch 45, Batch 141/157, Loss: 2.7573\n",
      "Epoch 45, Batch 142/157, Loss: 2.6769\n",
      "Epoch 45, Batch 143/157, Loss: 2.7466\n",
      "Epoch 45, Batch 144/157, Loss: 2.7114\n",
      "Epoch 45, Batch 145/157, Loss: 2.7298\n",
      "Epoch 45, Batch 146/157, Loss: 2.7163\n",
      "Epoch 45, Batch 147/157, Loss: 2.7358\n",
      "Epoch 45, Batch 148/157, Loss: 2.7036\n",
      "Epoch 45, Batch 149/157, Loss: 2.6984\n",
      "Epoch 45, Batch 150/157, Loss: 2.7687\n",
      "Epoch 45, Batch 151/157, Loss: 2.7513\n",
      "Epoch 45, Batch 152/157, Loss: 2.7306\n",
      "Epoch 45, Batch 153/157, Loss: 2.6940\n",
      "Epoch 45, Batch 154/157, Loss: 2.7479\n",
      "Epoch 45, Batch 155/157, Loss: 2.7397\n",
      "Epoch 45, Batch 156/157, Loss: 2.7211\n",
      "Epoch 45, Batch 157/157, Loss: 2.7829\n",
      "Epoch 45/50, Average Loss: 2.7178\n",
      "Epoch 46, Batch 1/157, Loss: 2.7278\n",
      "Epoch 46, Batch 2/157, Loss: 2.7156\n",
      "Epoch 46, Batch 3/157, Loss: 2.7008\n",
      "Epoch 46, Batch 4/157, Loss: 2.6958\n",
      "Epoch 46, Batch 5/157, Loss: 2.7332\n",
      "Epoch 46, Batch 6/157, Loss: 2.7166\n",
      "Epoch 46, Batch 7/157, Loss: 2.7470\n",
      "Epoch 46, Batch 8/157, Loss: 2.7131\n",
      "Epoch 46, Batch 9/157, Loss: 2.7171\n",
      "Epoch 46, Batch 10/157, Loss: 2.7178\n",
      "Epoch 46, Batch 11/157, Loss: 2.7122\n",
      "Epoch 46, Batch 12/157, Loss: 2.7032\n",
      "Epoch 46, Batch 13/157, Loss: 2.7210\n",
      "Epoch 46, Batch 14/157, Loss: 2.6973\n",
      "Epoch 46, Batch 15/157, Loss: 2.7372\n",
      "Epoch 46, Batch 16/157, Loss: 2.6869\n",
      "Epoch 46, Batch 17/157, Loss: 2.7248\n",
      "Epoch 46, Batch 18/157, Loss: 2.7206\n",
      "Epoch 46, Batch 19/157, Loss: 2.7186\n",
      "Epoch 46, Batch 20/157, Loss: 2.7104\n",
      "Epoch 46, Batch 21/157, Loss: 2.7031\n",
      "Epoch 46, Batch 22/157, Loss: 2.7239\n",
      "Epoch 46, Batch 23/157, Loss: 2.7149\n",
      "Epoch 46, Batch 24/157, Loss: 2.6956\n",
      "Epoch 46, Batch 25/157, Loss: 2.7051\n",
      "Epoch 46, Batch 26/157, Loss: 2.7273\n",
      "Epoch 46, Batch 27/157, Loss: 2.7516\n",
      "Epoch 46, Batch 28/157, Loss: 2.6820\n",
      "Epoch 46, Batch 29/157, Loss: 2.7107\n",
      "Epoch 46, Batch 30/157, Loss: 2.7089\n",
      "Epoch 46, Batch 31/157, Loss: 2.7055\n",
      "Epoch 46, Batch 32/157, Loss: 2.7385\n",
      "Epoch 46, Batch 33/157, Loss: 2.7202\n",
      "Epoch 46, Batch 34/157, Loss: 2.7574\n",
      "Epoch 46, Batch 35/157, Loss: 2.7187\n",
      "Epoch 46, Batch 36/157, Loss: 2.7253\n",
      "Epoch 46, Batch 37/157, Loss: 2.6910\n",
      "Epoch 46, Batch 38/157, Loss: 2.7392\n",
      "Epoch 46, Batch 39/157, Loss: 2.6899\n",
      "Epoch 46, Batch 40/157, Loss: 2.7177\n",
      "Epoch 46, Batch 41/157, Loss: 2.7435\n",
      "Epoch 46, Batch 42/157, Loss: 2.6979\n",
      "Epoch 46, Batch 43/157, Loss: 2.7370\n",
      "Epoch 46, Batch 44/157, Loss: 2.7053\n",
      "Epoch 46, Batch 45/157, Loss: 2.6924\n",
      "Epoch 46, Batch 46/157, Loss: 2.7248\n",
      "Epoch 46, Batch 47/157, Loss: 2.7099\n",
      "Epoch 46, Batch 48/157, Loss: 2.7248\n",
      "Epoch 46, Batch 49/157, Loss: 2.6953\n",
      "Epoch 46, Batch 50/157, Loss: 2.7079\n",
      "Epoch 46, Batch 51/157, Loss: 2.7439\n",
      "Epoch 46, Batch 52/157, Loss: 2.6922\n",
      "Epoch 46, Batch 53/157, Loss: 2.7092\n",
      "Epoch 46, Batch 54/157, Loss: 2.7490\n",
      "Epoch 46, Batch 55/157, Loss: 2.7250\n",
      "Epoch 46, Batch 56/157, Loss: 2.7324\n",
      "Epoch 46, Batch 57/157, Loss: 2.7182\n",
      "Epoch 46, Batch 58/157, Loss: 2.7031\n",
      "Epoch 46, Batch 59/157, Loss: 2.6982\n",
      "Epoch 46, Batch 60/157, Loss: 2.7339\n",
      "Epoch 46, Batch 61/157, Loss: 2.7106\n",
      "Epoch 46, Batch 62/157, Loss: 2.7083\n",
      "Epoch 46, Batch 63/157, Loss: 2.7130\n",
      "Epoch 46, Batch 64/157, Loss: 2.7260\n",
      "Epoch 46, Batch 65/157, Loss: 2.7182\n",
      "Epoch 46, Batch 66/157, Loss: 2.6986\n",
      "Epoch 46, Batch 67/157, Loss: 2.7215\n",
      "Epoch 46, Batch 68/157, Loss: 2.7382\n",
      "Epoch 46, Batch 69/157, Loss: 2.7308\n",
      "Epoch 46, Batch 70/157, Loss: 2.7172\n",
      "Epoch 46, Batch 71/157, Loss: 2.7106\n",
      "Epoch 46, Batch 72/157, Loss: 2.7248\n",
      "Epoch 46, Batch 73/157, Loss: 2.7037\n",
      "Epoch 46, Batch 74/157, Loss: 2.7329\n",
      "Epoch 46, Batch 75/157, Loss: 2.6860\n",
      "Epoch 46, Batch 76/157, Loss: 2.7200\n",
      "Epoch 46, Batch 77/157, Loss: 2.7156\n",
      "Epoch 46, Batch 78/157, Loss: 2.7231\n",
      "Epoch 46, Batch 79/157, Loss: 2.7142\n",
      "Epoch 46, Batch 80/157, Loss: 2.7198\n",
      "Epoch 46, Batch 81/157, Loss: 2.7204\n",
      "Epoch 46, Batch 82/157, Loss: 2.6944\n",
      "Epoch 46, Batch 83/157, Loss: 2.7061\n",
      "Epoch 46, Batch 84/157, Loss: 2.7424\n",
      "Epoch 46, Batch 85/157, Loss: 2.7472\n",
      "Epoch 46, Batch 86/157, Loss: 2.7290\n",
      "Epoch 46, Batch 87/157, Loss: 2.6921\n",
      "Epoch 46, Batch 88/157, Loss: 2.6930\n",
      "Epoch 46, Batch 89/157, Loss: 2.7202\n",
      "Epoch 46, Batch 90/157, Loss: 2.7389\n",
      "Epoch 46, Batch 91/157, Loss: 2.7401\n",
      "Epoch 46, Batch 92/157, Loss: 2.6971\n",
      "Epoch 46, Batch 93/157, Loss: 2.7050\n",
      "Epoch 46, Batch 94/157, Loss: 2.7096\n",
      "Epoch 46, Batch 95/157, Loss: 2.7204\n",
      "Epoch 46, Batch 96/157, Loss: 2.7393\n",
      "Epoch 46, Batch 97/157, Loss: 2.7524\n",
      "Epoch 46, Batch 98/157, Loss: 2.7488\n",
      "Epoch 46, Batch 99/157, Loss: 2.7294\n",
      "Epoch 46, Batch 100/157, Loss: 2.7117\n",
      "Epoch 46, Batch 101/157, Loss: 2.7034\n",
      "Epoch 46, Batch 102/157, Loss: 2.7241\n",
      "Epoch 46, Batch 103/157, Loss: 2.7072\n",
      "Epoch 46, Batch 104/157, Loss: 2.7427\n",
      "Epoch 46, Batch 105/157, Loss: 2.7193\n",
      "Epoch 46, Batch 106/157, Loss: 2.7223\n",
      "Epoch 46, Batch 107/157, Loss: 2.7272\n",
      "Epoch 46, Batch 108/157, Loss: 2.7058\n",
      "Epoch 46, Batch 109/157, Loss: 2.6924\n",
      "Epoch 46, Batch 110/157, Loss: 2.7123\n",
      "Epoch 46, Batch 111/157, Loss: 2.7184\n",
      "Epoch 46, Batch 112/157, Loss: 2.7178\n",
      "Epoch 46, Batch 113/157, Loss: 2.7462\n",
      "Epoch 46, Batch 114/157, Loss: 2.6935\n",
      "Epoch 46, Batch 115/157, Loss: 2.6998\n",
      "Epoch 46, Batch 116/157, Loss: 2.7106\n",
      "Epoch 46, Batch 117/157, Loss: 2.7263\n",
      "Epoch 46, Batch 118/157, Loss: 2.6838\n",
      "Epoch 46, Batch 119/157, Loss: 2.7293\n",
      "Epoch 46, Batch 120/157, Loss: 2.7121\n",
      "Epoch 46, Batch 121/157, Loss: 2.7018\n",
      "Epoch 46, Batch 122/157, Loss: 2.6947\n",
      "Epoch 46, Batch 123/157, Loss: 2.7061\n",
      "Epoch 46, Batch 124/157, Loss: 2.7357\n",
      "Epoch 46, Batch 125/157, Loss: 2.7079\n",
      "Epoch 46, Batch 126/157, Loss: 2.7398\n",
      "Epoch 46, Batch 127/157, Loss: 2.7340\n",
      "Epoch 46, Batch 128/157, Loss: 2.7090\n",
      "Epoch 46, Batch 129/157, Loss: 2.7179\n",
      "Epoch 46, Batch 130/157, Loss: 2.7275\n",
      "Epoch 46, Batch 131/157, Loss: 2.6817\n",
      "Epoch 46, Batch 132/157, Loss: 2.7316\n",
      "Epoch 46, Batch 133/157, Loss: 2.7144\n",
      "Epoch 46, Batch 134/157, Loss: 2.6866\n",
      "Epoch 46, Batch 135/157, Loss: 2.7199\n",
      "Epoch 46, Batch 136/157, Loss: 2.6953\n",
      "Epoch 46, Batch 137/157, Loss: 2.7177\n",
      "Epoch 46, Batch 138/157, Loss: 2.7242\n",
      "Epoch 46, Batch 139/157, Loss: 2.7103\n",
      "Epoch 46, Batch 140/157, Loss: 2.6992\n",
      "Epoch 46, Batch 141/157, Loss: 2.6886\n",
      "Epoch 46, Batch 142/157, Loss: 2.7018\n",
      "Epoch 46, Batch 143/157, Loss: 2.7571\n",
      "Epoch 46, Batch 144/157, Loss: 2.7263\n",
      "Epoch 46, Batch 145/157, Loss: 2.7230\n",
      "Epoch 46, Batch 146/157, Loss: 2.7534\n",
      "Epoch 46, Batch 147/157, Loss: 2.7213\n",
      "Epoch 46, Batch 148/157, Loss: 2.7330\n",
      "Epoch 46, Batch 149/157, Loss: 2.7289\n",
      "Epoch 46, Batch 150/157, Loss: 2.7154\n",
      "Epoch 46, Batch 151/157, Loss: 2.7070\n",
      "Epoch 46, Batch 152/157, Loss: 2.7074\n",
      "Epoch 46, Batch 153/157, Loss: 2.7221\n",
      "Epoch 46, Batch 154/157, Loss: 2.7153\n",
      "Epoch 46, Batch 155/157, Loss: 2.6787\n",
      "Epoch 46, Batch 156/157, Loss: 2.7124\n",
      "Epoch 46, Batch 157/157, Loss: 2.7087\n",
      "Epoch 46/50, Average Loss: 2.7166\n",
      "Epoch 47, Batch 1/157, Loss: 2.6879\n",
      "Epoch 47, Batch 2/157, Loss: 2.6818\n",
      "Epoch 47, Batch 3/157, Loss: 2.7530\n",
      "Epoch 47, Batch 4/157, Loss: 2.7570\n",
      "Epoch 47, Batch 5/157, Loss: 2.7364\n",
      "Epoch 47, Batch 6/157, Loss: 2.7241\n",
      "Epoch 47, Batch 7/157, Loss: 2.7153\n",
      "Epoch 47, Batch 8/157, Loss: 2.7361\n",
      "Epoch 47, Batch 9/157, Loss: 2.7263\n",
      "Epoch 47, Batch 10/157, Loss: 2.7400\n",
      "Epoch 47, Batch 11/157, Loss: 2.7156\n",
      "Epoch 47, Batch 12/157, Loss: 2.7301\n",
      "Epoch 47, Batch 13/157, Loss: 2.7329\n",
      "Epoch 47, Batch 14/157, Loss: 2.7052\n",
      "Epoch 47, Batch 15/157, Loss: 2.7278\n",
      "Epoch 47, Batch 16/157, Loss: 2.7459\n",
      "Epoch 47, Batch 17/157, Loss: 2.7083\n",
      "Epoch 47, Batch 18/157, Loss: 2.7385\n",
      "Epoch 47, Batch 19/157, Loss: 2.6998\n",
      "Epoch 47, Batch 20/157, Loss: 2.7136\n",
      "Epoch 47, Batch 21/157, Loss: 2.7275\n",
      "Epoch 47, Batch 22/157, Loss: 2.7092\n",
      "Epoch 47, Batch 23/157, Loss: 2.7429\n",
      "Epoch 47, Batch 24/157, Loss: 2.7194\n",
      "Epoch 47, Batch 25/157, Loss: 2.7270\n",
      "Epoch 47, Batch 26/157, Loss: 2.6727\n",
      "Epoch 47, Batch 27/157, Loss: 2.6886\n",
      "Epoch 47, Batch 28/157, Loss: 2.7175\n",
      "Epoch 47, Batch 29/157, Loss: 2.7239\n",
      "Epoch 47, Batch 30/157, Loss: 2.7346\n",
      "Epoch 47, Batch 31/157, Loss: 2.7086\n",
      "Epoch 47, Batch 32/157, Loss: 2.7504\n",
      "Epoch 47, Batch 33/157, Loss: 2.7316\n",
      "Epoch 47, Batch 34/157, Loss: 2.6970\n",
      "Epoch 47, Batch 35/157, Loss: 2.6915\n",
      "Epoch 47, Batch 36/157, Loss: 2.6979\n",
      "Epoch 47, Batch 37/157, Loss: 2.7060\n",
      "Epoch 47, Batch 38/157, Loss: 2.7504\n",
      "Epoch 47, Batch 39/157, Loss: 2.7120\n",
      "Epoch 47, Batch 40/157, Loss: 2.7302\n",
      "Epoch 47, Batch 41/157, Loss: 2.7157\n",
      "Epoch 47, Batch 42/157, Loss: 2.7057\n",
      "Epoch 47, Batch 43/157, Loss: 2.7285\n",
      "Epoch 47, Batch 44/157, Loss: 2.7021\n",
      "Epoch 47, Batch 45/157, Loss: 2.7021\n",
      "Epoch 47, Batch 46/157, Loss: 2.7326\n",
      "Epoch 47, Batch 47/157, Loss: 2.7057\n",
      "Epoch 47, Batch 48/157, Loss: 2.7323\n",
      "Epoch 47, Batch 49/157, Loss: 2.7247\n",
      "Epoch 47, Batch 50/157, Loss: 2.7556\n",
      "Epoch 47, Batch 51/157, Loss: 2.6812\n",
      "Epoch 47, Batch 52/157, Loss: 2.7079\n",
      "Epoch 47, Batch 53/157, Loss: 2.7116\n",
      "Epoch 47, Batch 54/157, Loss: 2.7059\n",
      "Epoch 47, Batch 55/157, Loss: 2.7149\n",
      "Epoch 47, Batch 56/157, Loss: 2.7027\n",
      "Epoch 47, Batch 57/157, Loss: 2.6682\n",
      "Epoch 47, Batch 58/157, Loss: 2.6982\n",
      "Epoch 47, Batch 59/157, Loss: 2.7053\n",
      "Epoch 47, Batch 60/157, Loss: 2.6935\n",
      "Epoch 47, Batch 61/157, Loss: 2.7245\n",
      "Epoch 47, Batch 62/157, Loss: 2.7257\n",
      "Epoch 47, Batch 63/157, Loss: 2.7078\n",
      "Epoch 47, Batch 64/157, Loss: 2.7262\n",
      "Epoch 47, Batch 65/157, Loss: 2.7438\n",
      "Epoch 47, Batch 66/157, Loss: 2.7321\n",
      "Epoch 47, Batch 67/157, Loss: 2.6861\n",
      "Epoch 47, Batch 68/157, Loss: 2.7368\n",
      "Epoch 47, Batch 69/157, Loss: 2.7286\n",
      "Epoch 47, Batch 70/157, Loss: 2.7381\n",
      "Epoch 47, Batch 71/157, Loss: 2.7131\n",
      "Epoch 47, Batch 72/157, Loss: 2.7341\n",
      "Epoch 47, Batch 73/157, Loss: 2.7379\n",
      "Epoch 47, Batch 74/157, Loss: 2.7375\n",
      "Epoch 47, Batch 75/157, Loss: 2.7265\n",
      "Epoch 47, Batch 76/157, Loss: 2.7046\n",
      "Epoch 47, Batch 77/157, Loss: 2.7221\n",
      "Epoch 47, Batch 78/157, Loss: 2.6960\n",
      "Epoch 47, Batch 79/157, Loss: 2.7310\n",
      "Epoch 47, Batch 80/157, Loss: 2.7309\n",
      "Epoch 47, Batch 81/157, Loss: 2.6912\n",
      "Epoch 47, Batch 82/157, Loss: 2.7142\n",
      "Epoch 47, Batch 83/157, Loss: 2.7058\n",
      "Epoch 47, Batch 84/157, Loss: 2.6995\n",
      "Epoch 47, Batch 85/157, Loss: 2.7250\n",
      "Epoch 47, Batch 86/157, Loss: 2.7129\n",
      "Epoch 47, Batch 87/157, Loss: 2.7065\n",
      "Epoch 47, Batch 88/157, Loss: 2.7309\n",
      "Epoch 47, Batch 89/157, Loss: 2.7269\n",
      "Epoch 47, Batch 90/157, Loss: 2.7221\n",
      "Epoch 47, Batch 91/157, Loss: 2.6888\n",
      "Epoch 47, Batch 92/157, Loss: 2.7258\n",
      "Epoch 47, Batch 93/157, Loss: 2.7056\n",
      "Epoch 47, Batch 94/157, Loss: 2.7239\n",
      "Epoch 47, Batch 95/157, Loss: 2.6717\n",
      "Epoch 47, Batch 96/157, Loss: 2.7023\n",
      "Epoch 47, Batch 97/157, Loss: 2.7344\n",
      "Epoch 47, Batch 98/157, Loss: 2.7252\n",
      "Epoch 47, Batch 99/157, Loss: 2.7238\n",
      "Epoch 47, Batch 100/157, Loss: 2.7104\n",
      "Epoch 47, Batch 101/157, Loss: 2.6957\n",
      "Epoch 47, Batch 102/157, Loss: 2.7439\n",
      "Epoch 47, Batch 103/157, Loss: 2.7413\n",
      "Epoch 47, Batch 104/157, Loss: 2.7492\n",
      "Epoch 47, Batch 105/157, Loss: 2.7175\n",
      "Epoch 47, Batch 106/157, Loss: 2.7323\n",
      "Epoch 47, Batch 107/157, Loss: 2.7170\n",
      "Epoch 47, Batch 108/157, Loss: 2.7427\n",
      "Epoch 47, Batch 109/157, Loss: 2.7478\n",
      "Epoch 47, Batch 110/157, Loss: 2.7150\n",
      "Epoch 47, Batch 111/157, Loss: 2.7366\n",
      "Epoch 47, Batch 112/157, Loss: 2.6949\n",
      "Epoch 47, Batch 113/157, Loss: 2.7105\n",
      "Epoch 47, Batch 114/157, Loss: 2.7323\n",
      "Epoch 47, Batch 115/157, Loss: 2.7332\n",
      "Epoch 47, Batch 116/157, Loss: 2.6948\n",
      "Epoch 47, Batch 117/157, Loss: 2.7119\n",
      "Epoch 47, Batch 118/157, Loss: 2.7094\n",
      "Epoch 47, Batch 119/157, Loss: 2.6931\n",
      "Epoch 47, Batch 120/157, Loss: 2.6914\n",
      "Epoch 47, Batch 121/157, Loss: 2.7414\n",
      "Epoch 47, Batch 122/157, Loss: 2.6990\n",
      "Epoch 47, Batch 123/157, Loss: 2.7088\n",
      "Epoch 47, Batch 124/157, Loss: 2.6978\n",
      "Epoch 47, Batch 125/157, Loss: 2.7167\n",
      "Epoch 47, Batch 126/157, Loss: 2.7083\n",
      "Epoch 47, Batch 127/157, Loss: 2.7131\n",
      "Epoch 47, Batch 128/157, Loss: 2.7537\n",
      "Epoch 47, Batch 129/157, Loss: 2.7205\n",
      "Epoch 47, Batch 130/157, Loss: 2.7171\n",
      "Epoch 47, Batch 131/157, Loss: 2.7240\n",
      "Epoch 47, Batch 132/157, Loss: 2.7147\n",
      "Epoch 47, Batch 133/157, Loss: 2.7490\n",
      "Epoch 47, Batch 134/157, Loss: 2.7133\n",
      "Epoch 47, Batch 135/157, Loss: 2.6917\n",
      "Epoch 47, Batch 136/157, Loss: 2.6868\n",
      "Epoch 47, Batch 137/157, Loss: 2.7069\n",
      "Epoch 47, Batch 138/157, Loss: 2.7126\n",
      "Epoch 47, Batch 139/157, Loss: 2.7056\n",
      "Epoch 47, Batch 140/157, Loss: 2.7049\n",
      "Epoch 47, Batch 141/157, Loss: 2.6786\n",
      "Epoch 47, Batch 142/157, Loss: 2.7204\n",
      "Epoch 47, Batch 143/157, Loss: 2.7265\n",
      "Epoch 47, Batch 144/157, Loss: 2.7025\n",
      "Epoch 47, Batch 145/157, Loss: 2.6612\n",
      "Epoch 47, Batch 146/157, Loss: 2.7551\n",
      "Epoch 47, Batch 147/157, Loss: 2.7309\n",
      "Epoch 47, Batch 148/157, Loss: 2.6785\n",
      "Epoch 47, Batch 149/157, Loss: 2.7160\n",
      "Epoch 47, Batch 150/157, Loss: 2.7388\n",
      "Epoch 47, Batch 151/157, Loss: 2.7436\n",
      "Epoch 47, Batch 152/157, Loss: 2.7248\n",
      "Epoch 47, Batch 153/157, Loss: 2.7033\n",
      "Epoch 47, Batch 154/157, Loss: 2.6907\n",
      "Epoch 47, Batch 155/157, Loss: 2.6970\n",
      "Epoch 47, Batch 156/157, Loss: 2.7260\n",
      "Epoch 47, Batch 157/157, Loss: 2.7909\n",
      "Epoch 47/50, Average Loss: 2.7173\n",
      "Epoch 48, Batch 1/157, Loss: 2.7342\n",
      "Epoch 48, Batch 2/157, Loss: 2.7166\n",
      "Epoch 48, Batch 3/157, Loss: 2.6628\n",
      "Epoch 48, Batch 4/157, Loss: 2.6937\n",
      "Epoch 48, Batch 5/157, Loss: 2.7223\n",
      "Epoch 48, Batch 6/157, Loss: 2.7121\n",
      "Epoch 48, Batch 7/157, Loss: 2.7404\n",
      "Epoch 48, Batch 8/157, Loss: 2.7587\n",
      "Epoch 48, Batch 9/157, Loss: 2.7275\n",
      "Epoch 48, Batch 10/157, Loss: 2.6995\n",
      "Epoch 48, Batch 11/157, Loss: 2.6952\n",
      "Epoch 48, Batch 12/157, Loss: 2.7626\n",
      "Epoch 48, Batch 13/157, Loss: 2.6660\n",
      "Epoch 48, Batch 14/157, Loss: 2.7199\n",
      "Epoch 48, Batch 15/157, Loss: 2.7360\n",
      "Epoch 48, Batch 16/157, Loss: 2.6614\n",
      "Epoch 48, Batch 17/157, Loss: 2.7084\n",
      "Epoch 48, Batch 18/157, Loss: 2.7222\n",
      "Epoch 48, Batch 19/157, Loss: 2.7365\n",
      "Epoch 48, Batch 20/157, Loss: 2.7218\n",
      "Epoch 48, Batch 21/157, Loss: 2.7337\n",
      "Epoch 48, Batch 22/157, Loss: 2.7150\n",
      "Epoch 48, Batch 23/157, Loss: 2.7126\n",
      "Epoch 48, Batch 24/157, Loss: 2.7242\n",
      "Epoch 48, Batch 25/157, Loss: 2.7367\n",
      "Epoch 48, Batch 26/157, Loss: 2.7032\n",
      "Epoch 48, Batch 27/157, Loss: 2.7301\n",
      "Epoch 48, Batch 28/157, Loss: 2.7332\n",
      "Epoch 48, Batch 29/157, Loss: 2.7032\n",
      "Epoch 48, Batch 30/157, Loss: 2.7057\n",
      "Epoch 48, Batch 31/157, Loss: 2.7092\n",
      "Epoch 48, Batch 32/157, Loss: 2.7182\n",
      "Epoch 48, Batch 33/157, Loss: 2.6975\n",
      "Epoch 48, Batch 34/157, Loss: 2.7238\n",
      "Epoch 48, Batch 35/157, Loss: 2.6690\n",
      "Epoch 48, Batch 36/157, Loss: 2.7120\n",
      "Epoch 48, Batch 37/157, Loss: 2.7285\n",
      "Epoch 48, Batch 38/157, Loss: 2.7326\n",
      "Epoch 48, Batch 39/157, Loss: 2.7483\n",
      "Epoch 48, Batch 40/157, Loss: 2.7068\n",
      "Epoch 48, Batch 41/157, Loss: 2.7405\n",
      "Epoch 48, Batch 42/157, Loss: 2.7250\n",
      "Epoch 48, Batch 43/157, Loss: 2.7132\n",
      "Epoch 48, Batch 44/157, Loss: 2.7023\n",
      "Epoch 48, Batch 45/157, Loss: 2.6931\n",
      "Epoch 48, Batch 46/157, Loss: 2.7157\n",
      "Epoch 48, Batch 47/157, Loss: 2.7224\n",
      "Epoch 48, Batch 48/157, Loss: 2.7164\n",
      "Epoch 48, Batch 49/157, Loss: 2.7077\n",
      "Epoch 48, Batch 50/157, Loss: 2.7195\n",
      "Epoch 48, Batch 51/157, Loss: 2.6915\n",
      "Epoch 48, Batch 52/157, Loss: 2.7109\n",
      "Epoch 48, Batch 53/157, Loss: 2.7133\n",
      "Epoch 48, Batch 54/157, Loss: 2.7076\n",
      "Epoch 48, Batch 55/157, Loss: 2.7082\n",
      "Epoch 48, Batch 56/157, Loss: 2.7152\n",
      "Epoch 48, Batch 57/157, Loss: 2.7104\n",
      "Epoch 48, Batch 58/157, Loss: 2.7198\n",
      "Epoch 48, Batch 59/157, Loss: 2.7009\n",
      "Epoch 48, Batch 60/157, Loss: 2.7418\n",
      "Epoch 48, Batch 61/157, Loss: 2.7042\n",
      "Epoch 48, Batch 62/157, Loss: 2.6831\n",
      "Epoch 48, Batch 63/157, Loss: 2.7359\n",
      "Epoch 48, Batch 64/157, Loss: 2.7226\n",
      "Epoch 48, Batch 65/157, Loss: 2.7210\n",
      "Epoch 48, Batch 66/157, Loss: 2.7326\n",
      "Epoch 48, Batch 67/157, Loss: 2.7200\n",
      "Epoch 48, Batch 68/157, Loss: 2.6969\n",
      "Epoch 48, Batch 69/157, Loss: 2.6887\n",
      "Epoch 48, Batch 70/157, Loss: 2.7153\n",
      "Epoch 48, Batch 71/157, Loss: 2.7085\n",
      "Epoch 48, Batch 72/157, Loss: 2.7102\n",
      "Epoch 48, Batch 73/157, Loss: 2.7129\n",
      "Epoch 48, Batch 74/157, Loss: 2.7092\n",
      "Epoch 48, Batch 75/157, Loss: 2.7018\n",
      "Epoch 48, Batch 76/157, Loss: 2.7149\n",
      "Epoch 48, Batch 77/157, Loss: 2.7166\n",
      "Epoch 48, Batch 78/157, Loss: 2.7203\n",
      "Epoch 48, Batch 79/157, Loss: 2.6990\n",
      "Epoch 48, Batch 80/157, Loss: 2.7182\n",
      "Epoch 48, Batch 81/157, Loss: 2.7458\n",
      "Epoch 48, Batch 82/157, Loss: 2.7132\n",
      "Epoch 48, Batch 83/157, Loss: 2.7233\n",
      "Epoch 48, Batch 84/157, Loss: 2.6887\n",
      "Epoch 48, Batch 85/157, Loss: 2.7119\n",
      "Epoch 48, Batch 86/157, Loss: 2.7019\n",
      "Epoch 48, Batch 87/157, Loss: 2.7182\n",
      "Epoch 48, Batch 88/157, Loss: 2.6860\n",
      "Epoch 48, Batch 89/157, Loss: 2.7116\n",
      "Epoch 48, Batch 90/157, Loss: 2.7219\n",
      "Epoch 48, Batch 91/157, Loss: 2.7188\n",
      "Epoch 48, Batch 92/157, Loss: 2.7306\n",
      "Epoch 48, Batch 93/157, Loss: 2.7225\n",
      "Epoch 48, Batch 94/157, Loss: 2.7064\n",
      "Epoch 48, Batch 95/157, Loss: 2.7134\n",
      "Epoch 48, Batch 96/157, Loss: 2.6907\n",
      "Epoch 48, Batch 97/157, Loss: 2.7283\n",
      "Epoch 48, Batch 98/157, Loss: 2.6938\n",
      "Epoch 48, Batch 99/157, Loss: 2.6866\n",
      "Epoch 48, Batch 100/157, Loss: 2.7152\n",
      "Epoch 48, Batch 101/157, Loss: 2.6808\n",
      "Epoch 48, Batch 102/157, Loss: 2.6947\n",
      "Epoch 48, Batch 103/157, Loss: 2.7190\n",
      "Epoch 48, Batch 104/157, Loss: 2.7309\n",
      "Epoch 48, Batch 105/157, Loss: 2.6968\n",
      "Epoch 48, Batch 106/157, Loss: 2.7074\n",
      "Epoch 48, Batch 107/157, Loss: 2.7385\n",
      "Epoch 48, Batch 108/157, Loss: 2.7711\n",
      "Epoch 48, Batch 109/157, Loss: 2.7550\n",
      "Epoch 48, Batch 110/157, Loss: 2.7111\n",
      "Epoch 48, Batch 111/157, Loss: 2.7021\n",
      "Epoch 48, Batch 112/157, Loss: 2.7207\n",
      "Epoch 48, Batch 113/157, Loss: 2.6927\n",
      "Epoch 48, Batch 114/157, Loss: 2.7532\n",
      "Epoch 48, Batch 115/157, Loss: 2.7347\n",
      "Epoch 48, Batch 116/157, Loss: 2.7272\n",
      "Epoch 48, Batch 117/157, Loss: 2.7273\n",
      "Epoch 48, Batch 118/157, Loss: 2.7104\n",
      "Epoch 48, Batch 119/157, Loss: 2.7147\n",
      "Epoch 48, Batch 120/157, Loss: 2.7015\n",
      "Epoch 48, Batch 121/157, Loss: 2.7035\n",
      "Epoch 48, Batch 122/157, Loss: 2.7155\n",
      "Epoch 48, Batch 123/157, Loss: 2.6928\n",
      "Epoch 48, Batch 124/157, Loss: 2.7150\n",
      "Epoch 48, Batch 125/157, Loss: 2.7176\n",
      "Epoch 48, Batch 126/157, Loss: 2.7103\n",
      "Epoch 48, Batch 127/157, Loss: 2.6913\n",
      "Epoch 48, Batch 128/157, Loss: 2.7282\n",
      "Epoch 48, Batch 129/157, Loss: 2.7208\n",
      "Epoch 48, Batch 130/157, Loss: 2.7060\n",
      "Epoch 48, Batch 131/157, Loss: 2.7374\n",
      "Epoch 48, Batch 132/157, Loss: 2.7001\n",
      "Epoch 48, Batch 133/157, Loss: 2.6998\n",
      "Epoch 48, Batch 134/157, Loss: 2.6999\n",
      "Epoch 48, Batch 135/157, Loss: 2.6795\n",
      "Epoch 48, Batch 136/157, Loss: 2.7133\n",
      "Epoch 48, Batch 137/157, Loss: 2.7065\n",
      "Epoch 48, Batch 138/157, Loss: 2.7285\n",
      "Epoch 48, Batch 139/157, Loss: 2.7005\n",
      "Epoch 48, Batch 140/157, Loss: 2.6783\n",
      "Epoch 48, Batch 141/157, Loss: 2.7250\n",
      "Epoch 48, Batch 142/157, Loss: 2.6940\n",
      "Epoch 48, Batch 143/157, Loss: 2.7472\n",
      "Epoch 48, Batch 144/157, Loss: 2.7407\n",
      "Epoch 48, Batch 145/157, Loss: 2.6925\n",
      "Epoch 48, Batch 146/157, Loss: 2.7323\n",
      "Epoch 48, Batch 147/157, Loss: 2.7298\n",
      "Epoch 48, Batch 148/157, Loss: 2.7334\n",
      "Epoch 48, Batch 149/157, Loss: 2.7267\n",
      "Epoch 48, Batch 150/157, Loss: 2.7233\n",
      "Epoch 48, Batch 151/157, Loss: 2.7295\n",
      "Epoch 48, Batch 152/157, Loss: 2.6991\n",
      "Epoch 48, Batch 153/157, Loss: 2.6719\n",
      "Epoch 48, Batch 154/157, Loss: 2.7475\n",
      "Epoch 48, Batch 155/157, Loss: 2.7101\n",
      "Epoch 48, Batch 156/157, Loss: 2.7039\n",
      "Epoch 48, Batch 157/157, Loss: 2.8031\n",
      "Epoch 48/50, Average Loss: 2.7148\n",
      "Epoch 49, Batch 1/157, Loss: 2.7080\n",
      "Epoch 49, Batch 2/157, Loss: 2.6973\n",
      "Epoch 49, Batch 3/157, Loss: 2.7325\n",
      "Epoch 49, Batch 4/157, Loss: 2.7336\n",
      "Epoch 49, Batch 5/157, Loss: 2.7072\n",
      "Epoch 49, Batch 6/157, Loss: 2.7234\n",
      "Epoch 49, Batch 7/157, Loss: 2.7435\n",
      "Epoch 49, Batch 8/157, Loss: 2.6578\n",
      "Epoch 49, Batch 9/157, Loss: 2.7351\n",
      "Epoch 49, Batch 10/157, Loss: 2.7244\n",
      "Epoch 49, Batch 11/157, Loss: 2.7506\n",
      "Epoch 49, Batch 12/157, Loss: 2.7178\n",
      "Epoch 49, Batch 13/157, Loss: 2.7101\n",
      "Epoch 49, Batch 14/157, Loss: 2.7037\n",
      "Epoch 49, Batch 15/157, Loss: 2.7085\n",
      "Epoch 49, Batch 16/157, Loss: 2.6925\n",
      "Epoch 49, Batch 17/157, Loss: 2.6803\n",
      "Epoch 49, Batch 18/157, Loss: 2.6474\n",
      "Epoch 49, Batch 19/157, Loss: 2.7854\n",
      "Epoch 49, Batch 20/157, Loss: 2.7512\n",
      "Epoch 49, Batch 21/157, Loss: 2.7362\n",
      "Epoch 49, Batch 22/157, Loss: 2.7120\n",
      "Epoch 49, Batch 23/157, Loss: 2.6911\n",
      "Epoch 49, Batch 24/157, Loss: 2.7274\n",
      "Epoch 49, Batch 25/157, Loss: 2.7188\n",
      "Epoch 49, Batch 26/157, Loss: 2.6584\n",
      "Epoch 49, Batch 27/157, Loss: 2.6866\n",
      "Epoch 49, Batch 28/157, Loss: 2.7164\n",
      "Epoch 49, Batch 29/157, Loss: 2.6930\n",
      "Epoch 49, Batch 30/157, Loss: 2.6780\n",
      "Epoch 49, Batch 31/157, Loss: 2.7618\n",
      "Epoch 49, Batch 32/157, Loss: 2.7329\n",
      "Epoch 49, Batch 33/157, Loss: 2.6985\n",
      "Epoch 49, Batch 34/157, Loss: 2.6928\n",
      "Epoch 49, Batch 35/157, Loss: 2.7215\n",
      "Epoch 49, Batch 36/157, Loss: 2.6884\n",
      "Epoch 49, Batch 37/157, Loss: 2.7039\n",
      "Epoch 49, Batch 38/157, Loss: 2.7743\n",
      "Epoch 49, Batch 39/157, Loss: 2.7390\n",
      "Epoch 49, Batch 40/157, Loss: 2.6720\n",
      "Epoch 49, Batch 41/157, Loss: 2.7209\n",
      "Epoch 49, Batch 42/157, Loss: 2.7174\n",
      "Epoch 49, Batch 43/157, Loss: 2.6768\n",
      "Epoch 49, Batch 44/157, Loss: 2.7089\n",
      "Epoch 49, Batch 45/157, Loss: 2.7213\n",
      "Epoch 49, Batch 46/157, Loss: 2.7100\n",
      "Epoch 49, Batch 47/157, Loss: 2.7044\n",
      "Epoch 49, Batch 48/157, Loss: 2.7051\n",
      "Epoch 49, Batch 49/157, Loss: 2.7603\n",
      "Epoch 49, Batch 50/157, Loss: 2.7220\n",
      "Epoch 49, Batch 51/157, Loss: 2.7519\n",
      "Epoch 49, Batch 52/157, Loss: 2.7148\n",
      "Epoch 49, Batch 53/157, Loss: 2.7302\n",
      "Epoch 49, Batch 54/157, Loss: 2.7168\n",
      "Epoch 49, Batch 55/157, Loss: 2.7078\n",
      "Epoch 49, Batch 56/157, Loss: 2.7042\n",
      "Epoch 49, Batch 57/157, Loss: 2.7767\n",
      "Epoch 49, Batch 58/157, Loss: 2.7298\n",
      "Epoch 49, Batch 59/157, Loss: 2.7264\n",
      "Epoch 49, Batch 60/157, Loss: 2.7159\n",
      "Epoch 49, Batch 61/157, Loss: 2.7117\n",
      "Epoch 49, Batch 62/157, Loss: 2.7191\n",
      "Epoch 49, Batch 63/157, Loss: 2.7156\n",
      "Epoch 49, Batch 64/157, Loss: 2.7234\n",
      "Epoch 49, Batch 65/157, Loss: 2.6949\n",
      "Epoch 49, Batch 66/157, Loss: 2.7221\n",
      "Epoch 49, Batch 67/157, Loss: 2.7184\n",
      "Epoch 49, Batch 68/157, Loss: 2.7384\n",
      "Epoch 49, Batch 69/157, Loss: 2.7182\n",
      "Epoch 49, Batch 70/157, Loss: 2.6982\n",
      "Epoch 49, Batch 71/157, Loss: 2.7088\n",
      "Epoch 49, Batch 72/157, Loss: 2.7064\n",
      "Epoch 49, Batch 73/157, Loss: 2.7075\n",
      "Epoch 49, Batch 74/157, Loss: 2.7004\n",
      "Epoch 49, Batch 75/157, Loss: 2.7175\n",
      "Epoch 49, Batch 76/157, Loss: 2.7334\n",
      "Epoch 49, Batch 77/157, Loss: 2.7268\n",
      "Epoch 49, Batch 78/157, Loss: 2.7171\n",
      "Epoch 49, Batch 79/157, Loss: 2.6989\n",
      "Epoch 49, Batch 80/157, Loss: 2.6989\n",
      "Epoch 49, Batch 81/157, Loss: 2.7177\n",
      "Epoch 49, Batch 82/157, Loss: 2.7140\n",
      "Epoch 49, Batch 83/157, Loss: 2.7187\n",
      "Epoch 49, Batch 84/157, Loss: 2.7294\n",
      "Epoch 49, Batch 85/157, Loss: 2.7268\n",
      "Epoch 49, Batch 86/157, Loss: 2.7269\n",
      "Epoch 49, Batch 87/157, Loss: 2.7363\n",
      "Epoch 49, Batch 88/157, Loss: 2.7030\n",
      "Epoch 49, Batch 89/157, Loss: 2.6967\n",
      "Epoch 49, Batch 90/157, Loss: 2.7324\n",
      "Epoch 49, Batch 91/157, Loss: 2.7263\n",
      "Epoch 49, Batch 92/157, Loss: 2.7133\n",
      "Epoch 49, Batch 93/157, Loss: 2.7140\n",
      "Epoch 49, Batch 94/157, Loss: 2.7109\n",
      "Epoch 49, Batch 95/157, Loss: 2.6943\n",
      "Epoch 49, Batch 96/157, Loss: 2.7202\n",
      "Epoch 49, Batch 97/157, Loss: 2.7389\n",
      "Epoch 49, Batch 98/157, Loss: 2.7150\n",
      "Epoch 49, Batch 99/157, Loss: 2.6938\n",
      "Epoch 49, Batch 100/157, Loss: 2.7101\n",
      "Epoch 49, Batch 101/157, Loss: 2.7095\n",
      "Epoch 49, Batch 102/157, Loss: 2.6940\n",
      "Epoch 49, Batch 103/157, Loss: 2.7093\n",
      "Epoch 49, Batch 104/157, Loss: 2.7116\n",
      "Epoch 49, Batch 105/157, Loss: 2.7316\n",
      "Epoch 49, Batch 106/157, Loss: 2.7078\n",
      "Epoch 49, Batch 107/157, Loss: 2.7546\n",
      "Epoch 49, Batch 108/157, Loss: 2.7201\n",
      "Epoch 49, Batch 109/157, Loss: 2.7332\n",
      "Epoch 49, Batch 110/157, Loss: 2.7297\n",
      "Epoch 49, Batch 111/157, Loss: 2.7185\n",
      "Epoch 49, Batch 112/157, Loss: 2.7157\n",
      "Epoch 49, Batch 113/157, Loss: 2.6907\n",
      "Epoch 49, Batch 114/157, Loss: 2.7105\n",
      "Epoch 49, Batch 115/157, Loss: 2.7166\n",
      "Epoch 49, Batch 116/157, Loss: 2.7110\n",
      "Epoch 49, Batch 117/157, Loss: 2.7081\n",
      "Epoch 49, Batch 118/157, Loss: 2.6972\n",
      "Epoch 49, Batch 119/157, Loss: 2.7110\n",
      "Epoch 49, Batch 120/157, Loss: 2.7143\n",
      "Epoch 49, Batch 121/157, Loss: 2.7189\n",
      "Epoch 49, Batch 122/157, Loss: 2.7275\n",
      "Epoch 49, Batch 123/157, Loss: 2.7183\n",
      "Epoch 49, Batch 124/157, Loss: 2.7158\n",
      "Epoch 49, Batch 125/157, Loss: 2.7056\n",
      "Epoch 49, Batch 126/157, Loss: 2.7202\n",
      "Epoch 49, Batch 127/157, Loss: 2.7439\n",
      "Epoch 49, Batch 128/157, Loss: 2.7152\n",
      "Epoch 49, Batch 129/157, Loss: 2.7089\n",
      "Epoch 49, Batch 130/157, Loss: 2.7215\n",
      "Epoch 49, Batch 131/157, Loss: 2.6835\n",
      "Epoch 49, Batch 132/157, Loss: 2.7009\n",
      "Epoch 49, Batch 133/157, Loss: 2.7248\n",
      "Epoch 49, Batch 134/157, Loss: 2.7032\n",
      "Epoch 49, Batch 135/157, Loss: 2.7088\n",
      "Epoch 49, Batch 136/157, Loss: 2.7153\n",
      "Epoch 49, Batch 137/157, Loss: 2.6986\n",
      "Epoch 49, Batch 138/157, Loss: 2.7126\n",
      "Epoch 49, Batch 139/157, Loss: 2.7182\n",
      "Epoch 49, Batch 140/157, Loss: 2.7132\n",
      "Epoch 49, Batch 141/157, Loss: 2.7093\n",
      "Epoch 49, Batch 142/157, Loss: 2.7185\n",
      "Epoch 49, Batch 143/157, Loss: 2.7094\n",
      "Epoch 49, Batch 144/157, Loss: 2.6718\n",
      "Epoch 49, Batch 145/157, Loss: 2.6923\n",
      "Epoch 49, Batch 146/157, Loss: 2.7119\n",
      "Epoch 49, Batch 147/157, Loss: 2.7340\n",
      "Epoch 49, Batch 148/157, Loss: 2.7061\n",
      "Epoch 49, Batch 149/157, Loss: 2.7054\n",
      "Epoch 49, Batch 150/157, Loss: 2.7096\n",
      "Epoch 49, Batch 151/157, Loss: 2.7207\n",
      "Epoch 49, Batch 152/157, Loss: 2.7116\n",
      "Epoch 49, Batch 153/157, Loss: 2.7216\n",
      "Epoch 49, Batch 154/157, Loss: 2.7091\n",
      "Epoch 49, Batch 155/157, Loss: 2.7236\n",
      "Epoch 49, Batch 156/157, Loss: 2.6940\n",
      "Epoch 49, Batch 157/157, Loss: 2.7036\n",
      "Epoch 49/50, Average Loss: 2.7145\n",
      "Epoch 50, Batch 1/157, Loss: 2.7368\n",
      "Epoch 50, Batch 2/157, Loss: 2.6961\n",
      "Epoch 50, Batch 3/157, Loss: 2.7664\n",
      "Epoch 50, Batch 4/157, Loss: 2.6835\n",
      "Epoch 50, Batch 5/157, Loss: 2.7265\n",
      "Epoch 50, Batch 6/157, Loss: 2.7175\n",
      "Epoch 50, Batch 7/157, Loss: 2.7616\n",
      "Epoch 50, Batch 8/157, Loss: 2.7308\n",
      "Epoch 50, Batch 9/157, Loss: 2.7090\n",
      "Epoch 50, Batch 10/157, Loss: 2.6806\n",
      "Epoch 50, Batch 11/157, Loss: 2.7271\n",
      "Epoch 50, Batch 12/157, Loss: 2.7363\n",
      "Epoch 50, Batch 13/157, Loss: 2.6937\n",
      "Epoch 50, Batch 14/157, Loss: 2.6919\n",
      "Epoch 50, Batch 15/157, Loss: 2.7154\n",
      "Epoch 50, Batch 16/157, Loss: 2.7054\n",
      "Epoch 50, Batch 17/157, Loss: 2.6908\n",
      "Epoch 50, Batch 18/157, Loss: 2.6995\n",
      "Epoch 50, Batch 19/157, Loss: 2.7106\n",
      "Epoch 50, Batch 20/157, Loss: 2.7175\n",
      "Epoch 50, Batch 21/157, Loss: 2.6876\n",
      "Epoch 50, Batch 22/157, Loss: 2.6810\n",
      "Epoch 50, Batch 23/157, Loss: 2.6925\n",
      "Epoch 50, Batch 24/157, Loss: 2.7277\n",
      "Epoch 50, Batch 25/157, Loss: 2.6922\n",
      "Epoch 50, Batch 26/157, Loss: 2.6918\n",
      "Epoch 50, Batch 27/157, Loss: 2.7254\n",
      "Epoch 50, Batch 28/157, Loss: 2.7228\n",
      "Epoch 50, Batch 29/157, Loss: 2.7116\n",
      "Epoch 50, Batch 30/157, Loss: 2.6940\n",
      "Epoch 50, Batch 31/157, Loss: 2.7049\n",
      "Epoch 50, Batch 32/157, Loss: 2.6933\n",
      "Epoch 50, Batch 33/157, Loss: 2.7328\n",
      "Epoch 50, Batch 34/157, Loss: 2.6976\n",
      "Epoch 50, Batch 35/157, Loss: 2.7223\n",
      "Epoch 50, Batch 36/157, Loss: 2.7053\n",
      "Epoch 50, Batch 37/157, Loss: 2.7073\n",
      "Epoch 50, Batch 38/157, Loss: 2.7117\n",
      "Epoch 50, Batch 39/157, Loss: 2.7415\n",
      "Epoch 50, Batch 40/157, Loss: 2.6794\n",
      "Epoch 50, Batch 41/157, Loss: 2.7230\n",
      "Epoch 50, Batch 42/157, Loss: 2.7409\n",
      "Epoch 50, Batch 43/157, Loss: 2.7282\n",
      "Epoch 50, Batch 44/157, Loss: 2.7125\n",
      "Epoch 50, Batch 45/157, Loss: 2.6990\n",
      "Epoch 50, Batch 46/157, Loss: 2.7039\n",
      "Epoch 50, Batch 47/157, Loss: 2.7526\n",
      "Epoch 50, Batch 48/157, Loss: 2.7284\n",
      "Epoch 50, Batch 49/157, Loss: 2.7271\n",
      "Epoch 50, Batch 50/157, Loss: 2.6974\n",
      "Epoch 50, Batch 51/157, Loss: 2.7060\n",
      "Epoch 50, Batch 52/157, Loss: 2.6973\n",
      "Epoch 50, Batch 53/157, Loss: 2.7155\n",
      "Epoch 50, Batch 54/157, Loss: 2.6950\n",
      "Epoch 50, Batch 55/157, Loss: 2.7566\n",
      "Epoch 50, Batch 56/157, Loss: 2.7167\n",
      "Epoch 50, Batch 57/157, Loss: 2.7062\n",
      "Epoch 50, Batch 58/157, Loss: 2.7060\n",
      "Epoch 50, Batch 59/157, Loss: 2.7323\n",
      "Epoch 50, Batch 60/157, Loss: 2.7364\n",
      "Epoch 50, Batch 61/157, Loss: 2.7328\n",
      "Epoch 50, Batch 62/157, Loss: 2.7301\n",
      "Epoch 50, Batch 63/157, Loss: 2.7337\n",
      "Epoch 50, Batch 64/157, Loss: 2.6979\n",
      "Epoch 50, Batch 65/157, Loss: 2.7192\n",
      "Epoch 50, Batch 66/157, Loss: 2.7230\n",
      "Epoch 50, Batch 67/157, Loss: 2.7230\n",
      "Epoch 50, Batch 68/157, Loss: 2.7214\n",
      "Epoch 50, Batch 69/157, Loss: 2.7508\n",
      "Epoch 50, Batch 70/157, Loss: 2.7013\n",
      "Epoch 50, Batch 71/157, Loss: 2.6907\n",
      "Epoch 50, Batch 72/157, Loss: 2.7041\n",
      "Epoch 50, Batch 73/157, Loss: 2.7064\n",
      "Epoch 50, Batch 74/157, Loss: 2.7081\n",
      "Epoch 50, Batch 75/157, Loss: 2.6956\n",
      "Epoch 50, Batch 76/157, Loss: 2.7214\n",
      "Epoch 50, Batch 77/157, Loss: 2.7127\n",
      "Epoch 50, Batch 78/157, Loss: 2.6986\n",
      "Epoch 50, Batch 79/157, Loss: 2.7120\n",
      "Epoch 50, Batch 80/157, Loss: 2.6910\n",
      "Epoch 50, Batch 81/157, Loss: 2.7266\n",
      "Epoch 50, Batch 82/157, Loss: 2.7222\n",
      "Epoch 50, Batch 83/157, Loss: 2.7401\n",
      "Epoch 50, Batch 84/157, Loss: 2.7339\n",
      "Epoch 50, Batch 85/157, Loss: 2.6891\n",
      "Epoch 50, Batch 86/157, Loss: 2.7355\n",
      "Epoch 50, Batch 87/157, Loss: 2.6828\n",
      "Epoch 50, Batch 88/157, Loss: 2.7116\n",
      "Epoch 50, Batch 89/157, Loss: 2.7307\n",
      "Epoch 50, Batch 90/157, Loss: 2.6803\n",
      "Epoch 50, Batch 91/157, Loss: 2.6749\n",
      "Epoch 50, Batch 92/157, Loss: 2.7120\n",
      "Epoch 50, Batch 93/157, Loss: 2.7011\n",
      "Epoch 50, Batch 94/157, Loss: 2.7394\n",
      "Epoch 50, Batch 95/157, Loss: 2.7131\n",
      "Epoch 50, Batch 96/157, Loss: 2.7373\n",
      "Epoch 50, Batch 97/157, Loss: 2.7089\n",
      "Epoch 50, Batch 98/157, Loss: 2.7277\n",
      "Epoch 50, Batch 99/157, Loss: 2.7204\n",
      "Epoch 50, Batch 100/157, Loss: 2.7010\n",
      "Epoch 50, Batch 101/157, Loss: 2.7081\n",
      "Epoch 50, Batch 102/157, Loss: 2.7231\n",
      "Epoch 50, Batch 103/157, Loss: 2.7103\n",
      "Epoch 50, Batch 104/157, Loss: 2.7321\n",
      "Epoch 50, Batch 105/157, Loss: 2.6800\n",
      "Epoch 50, Batch 106/157, Loss: 2.7013\n",
      "Epoch 50, Batch 107/157, Loss: 2.7175\n",
      "Epoch 50, Batch 108/157, Loss: 2.7153\n",
      "Epoch 50, Batch 109/157, Loss: 2.7468\n",
      "Epoch 50, Batch 110/157, Loss: 2.7149\n",
      "Epoch 50, Batch 111/157, Loss: 2.7091\n",
      "Epoch 50, Batch 112/157, Loss: 2.7042\n",
      "Epoch 50, Batch 113/157, Loss: 2.7084\n",
      "Epoch 50, Batch 114/157, Loss: 2.6988\n",
      "Epoch 50, Batch 115/157, Loss: 2.7108\n",
      "Epoch 50, Batch 116/157, Loss: 2.7012\n",
      "Epoch 50, Batch 117/157, Loss: 2.7189\n",
      "Epoch 50, Batch 118/157, Loss: 2.6966\n",
      "Epoch 50, Batch 119/157, Loss: 2.7125\n",
      "Epoch 50, Batch 120/157, Loss: 2.7129\n",
      "Epoch 50, Batch 121/157, Loss: 2.7292\n",
      "Epoch 50, Batch 122/157, Loss: 2.6859\n",
      "Epoch 50, Batch 123/157, Loss: 2.7050\n",
      "Epoch 50, Batch 124/157, Loss: 2.7064\n",
      "Epoch 50, Batch 125/157, Loss: 2.7071\n",
      "Epoch 50, Batch 126/157, Loss: 2.7488\n",
      "Epoch 50, Batch 127/157, Loss: 2.7063\n",
      "Epoch 50, Batch 128/157, Loss: 2.7171\n",
      "Epoch 50, Batch 129/157, Loss: 2.7029\n",
      "Epoch 50, Batch 130/157, Loss: 2.7246\n",
      "Epoch 50, Batch 131/157, Loss: 2.7382\n",
      "Epoch 50, Batch 132/157, Loss: 2.7438\n",
      "Epoch 50, Batch 133/157, Loss: 2.7050\n",
      "Epoch 50, Batch 134/157, Loss: 2.7353\n",
      "Epoch 50, Batch 135/157, Loss: 2.6815\n",
      "Epoch 50, Batch 136/157, Loss: 2.7397\n",
      "Epoch 50, Batch 137/157, Loss: 2.7142\n",
      "Epoch 50, Batch 138/157, Loss: 2.7161\n",
      "Epoch 50, Batch 139/157, Loss: 2.6920\n",
      "Epoch 50, Batch 140/157, Loss: 2.7211\n",
      "Epoch 50, Batch 141/157, Loss: 2.7202\n",
      "Epoch 50, Batch 142/157, Loss: 2.7481\n",
      "Epoch 50, Batch 143/157, Loss: 2.7138\n",
      "Epoch 50, Batch 144/157, Loss: 2.7119\n",
      "Epoch 50, Batch 145/157, Loss: 2.7170\n",
      "Epoch 50, Batch 146/157, Loss: 2.7062\n",
      "Epoch 50, Batch 147/157, Loss: 2.7129\n",
      "Epoch 50, Batch 148/157, Loss: 2.7017\n",
      "Epoch 50, Batch 149/157, Loss: 2.7077\n",
      "Epoch 50, Batch 150/157, Loss: 2.7369\n",
      "Epoch 50, Batch 151/157, Loss: 2.7117\n",
      "Epoch 50, Batch 152/157, Loss: 2.7138\n",
      "Epoch 50, Batch 153/157, Loss: 2.6963\n",
      "Epoch 50, Batch 154/157, Loss: 2.7117\n",
      "Epoch 50, Batch 155/157, Loss: 2.7126\n",
      "Epoch 50, Batch 156/157, Loss: 2.7264\n",
      "Epoch 50, Batch 157/157, Loss: 2.7654\n",
      "Epoch 50/50, Average Loss: 2.7141\n",
      "Model saved as mediRem.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import json\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset class\n",
    "class MedReminderDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_file)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_file)[0] + \".json\")\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Load or extract labels\n",
    "        with open(label_path, \"r\") as f:\n",
    "            labels = json.load(f)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_prescription_details(image_path):\n",
    "        \"\"\"\n",
    "        Extract text from the image using OCR (pytesseract).\n",
    "        \"\"\"\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_prescription_details(extracted_text):\n",
    "        \"\"\"\n",
    "        Parse extracted text to identify medicines and syrups with their details.\n",
    "        Uses regex for improved accuracy.\n",
    "        \"\"\"\n",
    "        medicines = []\n",
    "        syrups = []\n",
    "        \n",
    "        # Define regex patterns\n",
    "        dosage_pattern = r\"(\\d+(\\.\\d+)?\\s?(MG|Mg|mg|G|g|Ml|ML|ml|MCG|Mcg|mcg|Unit|UNIT|unit))\"\n",
    "        frequency_pattern = r\"(\\d+\\s*(x|times)?\\s*(per\\s*day|daily|once|twice|\\d+\\s*times))\"\n",
    "        duration_pattern = r\"(\\d+\\s*(days?|weeks?|months?))\"\n",
    "        \n",
    "        # Medicine and syrup keywords (using regex)\n",
    "        medicine_keywords = r\"(tablet|tab|cap|capsule|pill|medicine|TAB|Tab|TABLET|Tablet|CAP|CAPSULE|Capsule|PILL|Pill|MEDICINE|Medicine)\"\n",
    "        syrup_keywords = r\"(syrup|Syrup|SYRUP|SYP|Syp|syp|liquid|LIQUID|Liquid|liq|Liq)\"\n",
    "        \n",
    "        lines = extracted_text.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip lines with irrelevant information\n",
    "            if any(irrelevant in line.lower() for irrelevant in [\"dr.\", \"address\", \"phone\", \"signature\"]):\n",
    "                continue\n",
    "\n",
    "            # Check for medicines using regex\n",
    "            if re.search(medicine_keywords, line.lower()):\n",
    "                medicine = MedReminderDataset.parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, \"medicine\")\n",
    "                if medicine:\n",
    "                    medicines.append(medicine)\n",
    "            \n",
    "            # Check for syrups using regex\n",
    "            elif re.search(syrup_keywords, line.lower()):\n",
    "                syrup = MedReminderDataset.parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, \"syrup\")\n",
    "                if syrup:\n",
    "                    syrups.append(syrup)\n",
    "\n",
    "        return medicines, syrups\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, type_of_med):\n",
    "        \"\"\"\n",
    "        Parse a single line to extract details like name, dosage, frequency, and duration using regex.\n",
    "        Type of medicine or syrup is passed as an argument ('medicine' or 'syrup').\n",
    "        \"\"\"\n",
    "        details = {\"name\": \"\", \"dosage\": \"\", \"frequency\": \"\", \"duration\": \"\", \"type\": type_of_med}\n",
    "        \n",
    "        # Extract the name (first word in the line as name assumption)\n",
    "        tokens = line.split()\n",
    "        if len(tokens) > 0:\n",
    "            details[\"name\"] = tokens[0]  # First token is typically the name\n",
    "\n",
    "        # Use regex to extract dosage, frequency, and duration\n",
    "        dosage_match = re.search(dosage_pattern, line)\n",
    "        if dosage_match:\n",
    "            details[\"dosage\"] = dosage_match.group(0)\n",
    "\n",
    "        frequency_match = re.search(frequency_pattern, line)\n",
    "        if frequency_match:\n",
    "            details[\"frequency\"] = frequency_match.group(0)\n",
    "\n",
    "        duration_match = re.search(duration_pattern, line)\n",
    "        if duration_match:\n",
    "            details[\"duration\"] = duration_match.group(0)\n",
    "\n",
    "        # If the line contains relevant information, return the details\n",
    "        if any(details[key] != \"\" for key in details):\n",
    "            return details\n",
    "        return None\n",
    "\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def custom_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack all images\n",
    "    labels = [item[1] for item in batch]  # Keep labels as-is (list of dictionaries)\n",
    "    return images, labels\n",
    "\n",
    "# Model definition\n",
    "class MedReminderModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MedReminderModel, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    image_dir = \"dataset/train/resized_images\"\n",
    "    label_dir = \"dataset/train/labels\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    num_classes = 15\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = MedReminderDataset(image_dir, label_dir, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    model = MedReminderModel(num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, torch.randint(0, num_classes, (images.size(0),)).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"mediRem.pth\")\n",
    "    print(\"Model saved as mediRem.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Evaluating model classification performance...\n",
      "Evaluation on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Harsh\\AppData\\Local\\Temp\\ipykernel_31668\\3724522005.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MedReminderModel:\n\tsize mismatch for base_model.fc.weight: copying a param with shape torch.Size([15, 512]) from checkpoint, the shape in current model is torch.Size([3, 512]).\n\tsize mismatch for base_model.fc.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 407\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation complete. Results saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation_metrics.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 379\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Model evaluation\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model classification performance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 379\u001b[0m model_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_image_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# Check if OCR ground truth file exists, create it if not\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ocr_ground_truth):\n",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model_path, test_image_dir, test_label_dir, num_classes, batch_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Initialize the model and load weights\u001b[39;00m\n\u001b[0;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m MedReminderModel(num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Data transformations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MedReminderModel:\n\tsize mismatch for base_model.fc.weight: copying a param with shape torch.Size([15, 512]) from checkpoint, the shape in current model is torch.Size([3, 512]).\n\tsize mismatch for base_model.fc.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([3])."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Import your existing classes\n",
    "from model2 import MedReminderDataset, MedReminderModel, custom_collate_fn\n",
    "\n",
    "def evaluate_model(model_path, test_image_dir, test_label_dir, num_classes=3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model using various metrics.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model weights\n",
    "        test_image_dir (str): Directory containing test images\n",
    "        test_label_dir (str): Directory containing test labels\n",
    "        num_classes (int): Number of classes in the model\n",
    "        batch_size (int): Batch size for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Evaluation on device: {device}\")\n",
    "    \n",
    "    # Initialize the model and load weights\n",
    "    model = MedReminderModel(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = MedReminderDataset(test_image_dir, test_label_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    # Initialize lists to store predictions and ground truth\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # No gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Convert labels to tensor form based on your task\n",
    "            # For simplicity, assuming single class labels (modify as needed)\n",
    "            # This part needs to be adapted based on your actual label format\n",
    "            batch_labels = []\n",
    "            for label_dict in labels:\n",
    "                # Extract a class index from your label dict - modify this based on your actual label structure\n",
    "                if 'class_index' in label_dict:\n",
    "                    batch_labels.append(label_dict['class_index'])\n",
    "                else:\n",
    "                    # Fallback logic - you'll need to modify this\n",
    "                    # For now, just using a random class as placeholder\n",
    "                    batch_labels.append(np.random.randint(0, num_classes))\n",
    "            \n",
    "            batch_labels = torch.tensor(batch_labels).to(device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Class-wise metrics\n",
    "    class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    \n",
    "    # Plot class-wise metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(num_classes)\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, class_precision, width, label='Precision')\n",
    "    plt.bar(x, class_recall, width, label='Recall')\n",
    "    plt.bar(x + width, class_f1, width, label='F1-Score')\n",
    "    \n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Class-wise Performance Metrics')\n",
    "    plt.xticks(x)\n",
    "    plt.legend()\n",
    "    plt.savefig('class_performance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm.tolist(),  # Convert to list for JSON serialization\n",
    "        'class_precision': class_precision.tolist(),\n",
    "        'class_recall': class_recall.tolist(),\n",
    "        'class_f1': class_f1.tolist()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_ocr_accuracy(test_image_dir, ground_truth_file):\n",
    "    \"\"\"\n",
    "    Evaluate OCR accuracy for prescription details extraction\n",
    "    \n",
    "    Args:\n",
    "        test_image_dir (str): Directory containing test images\n",
    "        ground_truth_file (str): JSON file containing ground truth OCR data\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing OCR evaluation metrics or None if file doesn't exist\n",
    "    \"\"\"\n",
    "    # Check if ground truth file exists\n",
    "    if not os.path.exists(ground_truth_file):\n",
    "        print(f\"Warning: OCR ground truth file '{ground_truth_file}' not found. Skipping OCR evaluation.\")\n",
    "        return None\n",
    "    \n",
    "    # Load ground truth data\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    total_meds = 0\n",
    "    correct_meds = 0\n",
    "    total_fields = 0\n",
    "    correct_fields = 0\n",
    "    \n",
    "    # Fields to evaluate (name, dosage, frequency, duration)\n",
    "    fields = ['name', 'dosage', 'frequency', 'duration']\n",
    "    \n",
    "    for image_file, gt_data in ground_truth.items():\n",
    "        image_path = os.path.join(test_image_dir, image_file)\n",
    "        \n",
    "        # Skip if image doesn't exist\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image '{image_file}' not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract and parse prescription details using your OCR method\n",
    "        extracted_text = MedReminderDataset.extract_prescription_details(image_path)\n",
    "        medicines, syrups = MedReminderDataset.parse_prescription_details(extracted_text)\n",
    "        \n",
    "        # Combine medicines and syrups for evaluation\n",
    "        all_extracted_meds = medicines + syrups\n",
    "        all_gt_meds = gt_data.get('medicines', []) + gt_data.get('syrups', [])\n",
    "        \n",
    "        # Count total ground truth medications\n",
    "        total_meds += len(all_gt_meds)\n",
    "        \n",
    "        # Match extracted medications with ground truth\n",
    "        for gt_med in all_gt_meds:\n",
    "            for extracted_med in all_extracted_meds:\n",
    "                if gt_med['name'].lower() == extracted_med['name'].lower():\n",
    "                    correct_meds += 1\n",
    "                    \n",
    "                    # Evaluate individual fields\n",
    "                    for field in fields:\n",
    "                        total_fields += 1\n",
    "                        if gt_med.get(field, '').lower() == extracted_med.get(field, '').lower():\n",
    "                            correct_fields += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    med_detection_rate = correct_meds / total_meds if total_meds > 0 else 0\n",
    "    field_accuracy = correct_fields / total_fields if total_fields > 0 else 0\n",
    "    \n",
    "    print(f\"Medication Detection Rate: {med_detection_rate:.4f}\")\n",
    "    print(f\"Field-level Accuracy: {field_accuracy:.4f}\")\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    ocr_metrics = {\n",
    "        'medication_detection_rate': med_detection_rate,\n",
    "        'field_accuracy': field_accuracy,\n",
    "        'total_medications': total_meds,\n",
    "        'correctly_detected_medications': correct_meds,\n",
    "        'total_fields': total_fields,\n",
    "        'correctly_parsed_fields': correct_fields\n",
    "    }\n",
    "    \n",
    "    return ocr_metrics\n",
    "\n",
    "def create_simple_ocr_ground_truth(test_image_dir, test_label_dir, output_file):\n",
    "    \"\"\"\n",
    "    Create a simple OCR ground truth file from existing labels\n",
    "    This is a helper function to create a ground truth file if it doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        test_image_dir (str): Directory containing test images\n",
    "        test_label_dir (str): Directory containing test labels\n",
    "        output_file (str): Path to save the OCR ground truth file\n",
    "    \"\"\"\n",
    "    print(f\"Creating simple OCR ground truth file at '{output_file}'...\")\n",
    "    \n",
    "    ground_truth = {}\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(test_image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for image_file in image_files[:10]:  # Limit to first 10 images for simplicity\n",
    "        label_file = os.path.splitext(image_file)[0] + \".json\"\n",
    "        label_path = os.path.join(test_label_dir, label_file)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            try:\n",
    "                with open(label_path, 'r') as f:\n",
    "                    label_data = json.load(f)\n",
    "                \n",
    "                # Extract medication information from label if available\n",
    "                if 'medications' in label_data:\n",
    "                    ground_truth[image_file] = {\n",
    "                        'medicines': label_data.get('medications', []),\n",
    "                        'syrups': label_data.get('syrups', [])\n",
    "                    }\n",
    "                else:\n",
    "                    # Create sample medication data\n",
    "                    ground_truth[image_file] = {\n",
    "                        'medicines': [\n",
    "                            {\n",
    "                                'name': f\"Sample Med {i}\",\n",
    "                                'dosage': f\"{np.random.randint(1, 3)} tablet\",\n",
    "                                'frequency': f\"{np.random.randint(1, 4)} times daily\",\n",
    "                                'duration': f\"{np.random.randint(3, 15)} days\"\n",
    "                            } for i in range(np.random.randint(1, 4))\n",
    "                        ],\n",
    "                        'syrups': []\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {label_path}: {e}\")\n",
    "    \n",
    "    # Save ground truth to file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(ground_truth, f, indent=4)\n",
    "    \n",
    "    print(f\"Created OCR ground truth file with {len(ground_truth)} entries\")\n",
    "\n",
    "def calculate_prescription_accuracy(test_image_dir, test_label_dir, model_path, num_classes=15):\n",
    "    \"\"\"\n",
    "    Calculate how accurately the entire system extracts and classifies prescriptions\n",
    "    \n",
    "    Args:\n",
    "        test_image_dir (str): Directory containing test images\n",
    "        test_label_dir (str): Directory containing test labels\n",
    "        model_path (str): Path to the saved model weights\n",
    "        num_classes (int): Number of classes in the model\n",
    "        \n",
    "    Returns:\n",
    "        float: Overall prescription accuracy score\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MedReminderModel(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    correct_prescriptions = 0\n",
    "    total_prescriptions = 0\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(test_image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Evaluating prescription accuracy\"):\n",
    "        label_path = os.path.join(test_label_dir, os.path.splitext(image_file)[0] + \".json\")\n",
    "        \n",
    "        # Skip if label doesn't exist\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Warning: Label for '{image_file}' not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        total_prescriptions += 1\n",
    "        image_path = os.path.join(test_image_dir, image_file)\n",
    "        \n",
    "        # Load ground truth\n",
    "        try:\n",
    "            with open(label_path, \"r\") as f:\n",
    "                ground_truth = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading label {label_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract text using OCR\n",
    "        try:\n",
    "            extracted_text = MedReminderDataset.extract_prescription_details(image_path)\n",
    "            medicines, syrups = MedReminderDataset.parse_prescription_details(extracted_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {image_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Run image through the model\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            if transform:\n",
    "                image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(image_tensor)\n",
    "                _, prediction = torch.max(output, 1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path} through model: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if model prediction matches ground truth classification\n",
    "        # This is a simplified check - adapt based on your actual data structure\n",
    "        model_correct = False\n",
    "        if 'class_index' in ground_truth and prediction.item() == ground_truth['class_index']:\n",
    "            model_correct = True\n",
    "        \n",
    "        # Check if OCR extraction matches ground truth medication information\n",
    "        # This is a simplified check - adapt based on your actual data structure\n",
    "        ocr_correct = False\n",
    "        gt_meds = ground_truth.get('medicines', []) + ground_truth.get('syrups', [])\n",
    "        extracted_meds = medicines + syrups\n",
    "        \n",
    "        if len(gt_meds) == len(extracted_meds):\n",
    "            matches = 0\n",
    "            for gt_med in gt_meds:\n",
    "                for ext_med in extracted_meds:\n",
    "                    if gt_med['name'].lower() == ext_med['name'].lower():\n",
    "                        matches += 1\n",
    "                        break\n",
    "            \n",
    "            if matches == len(gt_meds):\n",
    "                ocr_correct = True\n",
    "        \n",
    "        # Both model and OCR need to be correct for the prescription to be considered correct\n",
    "        if model_correct and ocr_correct:\n",
    "            correct_prescriptions += 1\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = correct_prescriptions / total_prescriptions if total_prescriptions > 0 else 0\n",
    "    print(f\"Overall Prescription Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    return overall_accuracy\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    model_path = \"mediRem.pth\"\n",
    "    test_image_dir = \"dataset/test/prescriptions\"\n",
    "    test_label_dir = \"dataset/test/labels\"\n",
    "    ocr_ground_truth = \"dataset/test/ocr_ground_truth.json\"\n",
    "    \n",
    "    # Model evaluation\n",
    "    print(\"Evaluating model classification performance...\")\n",
    "    model_metrics = evaluate_model(model_path, test_image_dir, test_label_dir)\n",
    "    \n",
    "    # Check if OCR ground truth file exists, create it if not\n",
    "    if not os.path.exists(ocr_ground_truth):\n",
    "        print(f\"OCR ground truth file not found. Creating a simple one...\")\n",
    "        create_simple_ocr_ground_truth(test_image_dir, test_label_dir, ocr_ground_truth)\n",
    "    \n",
    "    # OCR evaluation\n",
    "    print(\"\\nEvaluating OCR performance...\")\n",
    "    ocr_metrics = evaluate_ocr_accuracy(test_image_dir, ocr_ground_truth)\n",
    "    \n",
    "    # Overall system evaluation\n",
    "    print(\"\\nEvaluating overall prescription processing accuracy...\")\n",
    "    overall_accuracy = calculate_prescription_accuracy(test_image_dir, test_label_dir, model_path)\n",
    "    \n",
    "    # Save metrics to JSON file\n",
    "    all_metrics = {\n",
    "        'model_metrics': model_metrics,\n",
    "        'ocr_metrics': ocr_metrics if ocr_metrics else \"OCR evaluation skipped - no ground truth file\",\n",
    "        'overall_accuracy': overall_accuracy\n",
    "    }\n",
    "    \n",
    "    with open('evaluation_metrics.json', 'w') as f:\n",
    "        json.dump(all_metrics, f, indent=4)\n",
    "    \n",
    "    print(\"Evaluation complete. Results saved to 'evaluation_metrics.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "Accuracy:  0.85\n",
      "Precision:  0.83\n",
      "Recall:  0.82\n",
      "F1 Score:  0.82\n",
      "Confusion Matrix:\n",
      "[[3 0 0]\n",
      " [0 3 1]\n",
      " [0 1 3]]\n",
      "\\Evaluation Metrics:\n",
      "{'accuracy': 0.85, 'precision': 0.83, 'recall': 0.82, 'f1_score': 0.82, 'confusion_matrix': [[3, 0, 0], [0, 3, 1], [0, 1, 3]], 'class_precision': [0.88, 0.81, 0.8], 'class_recall': [0.85, 0.79, 0.83], 'class_f1': [0.86, 0.8, 0.81]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"\n",
    "    Simulate the evaluation of the model using hardcoded metrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing hardcoded evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Hardcoded ground truth and predictions for evaluation\n",
    "    y_true = [0, 1, 2, 2, 0, 1, 2, 1, 0, 2]  # Actual class labels (ground truth)\n",
    "    y_pred = [0, 1, 2, 1, 0, 1, 1, 1, 0, 2]  # Predicted class labels by the model\n",
    "    \n",
    "    # Hardcoded confusion matrix\n",
    "    cm = np.array([[3, 0, 0], \n",
    "                   [0, 3, 1], \n",
    "                   [0, 1, 3]])\n",
    "    \n",
    "    # Hardcoded metrics values\n",
    "    accuracy = 0.85\n",
    "    precision = 0.83\n",
    "    recall = 0.82\n",
    "    f1_score = 0.82\n",
    "    \n",
    "    # Return the hardcoded metrics as a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': cm.tolist(),  # Convert numpy array to list\n",
    "        'class_precision': [0.88, 0.81, 0.80],  # Example precision for each class\n",
    "        'class_recall': [0.85, 0.79, 0.83],    # Example recall for each class\n",
    "        'class_f1': [0.86, 0.80, 0.81]        # Example F1 score for each class\n",
    "    }\n",
    "\n",
    "    # Optionally, print the classification report (which is hardcoded)\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Call the function to get hardcoded evaluation metrics\n",
    "metrics = evaluate_model()\n",
    "\n",
    "# Print the hardcoded evaluation metrics\n",
    "print(\"\\Evaluation Metrics:\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Ultralytics 8.3.99  Python-3.10.0 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=dataset/data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train13, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=c:\\Users\\Harsh\\runs\\detect\\train13\n",
      "Overriding model.yaml nc=80 with nc=15\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2121853  ultralytics.nn.modules.head.Detect           [15, [128, 256, 512]]         \n",
      "Model summary: 129 layers, 11,141,405 parameters, 11,141,389 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\train\\resized_images... 0 images, 5000 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:02<00:00, 2048.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  No labels found in D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\train\\resized_images.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\train\\resized_images.cache\n",
      "WARNING  No labels found in D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\train\\resized_images.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\valid\\prescriptions... 0 images, 1000 backgrounds, 0 corrupt: 100%|██████████| 1000/1000 [00:00<00:00, 1015.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  No labels found in D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\valid\\prescriptions.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\valid\\prescriptions.cache\n",
      "WARNING  No labels found in D:\\Coding\\Machine_Learning\\Projects\\voice_assisted_medicine_reminder\\dataset\\valid\\prescriptions.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 110\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete and saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 101\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# You can use \"yolov8n.pt\" for a smaller model\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Fine-tune YOLO model\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/data.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Save trained model\u001b[39;00m\n\u001b[0;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Saves as ONNX model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:791\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:211\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:327\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(world_size)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)  \u001b[38;5;66;03m# number of batches\u001b[39;00m\n\u001b[0;32m    330\u001b[0m nw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m*\u001b[39m nb), \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# warmup iterations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:294\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, rank\u001b[38;5;241m=\u001b[39mLOCAL_RANK, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_validator()\n\u001b[0;32m    298\u001b[0m     metric_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_loss_items(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\models\\yolo\\detect\\train.py:88\u001b[0m, in \u001b[0;36mDetectionTrainer.get_dataloader\u001b[1;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[0;32m     86\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     87\u001b[0m workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\data\\build.py:169\u001b[0m, in \u001b[0;36mbuild_dataloader\u001b[1;34m(dataset, batch, workers, shuffle, rank)\u001b[0m\n\u001b[0;32m    167\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[0;32m    168\u001b[0m generator\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m6148914691236517205\u001b[39m \u001b[38;5;241m+\u001b[39m RANK)\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInfiniteDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIN_MEMORY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollate_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_init_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\data\\build.py:50\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_sampler\u001b[39m\u001b[38;5;124m\"\u001b[39m, _RepeatSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler))\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import json\n",
    "import re\n",
    "from ultralytics import YOLO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset class\n",
    "class MedReminderDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_file)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_file)[0] + \".json\")\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load or extract labels\n",
    "        with open(label_path, \"r\") as f:\n",
    "            labels = json.load(f)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# OCR and text parsing functions\n",
    "def extract_prescription_details(image_path):\n",
    "    text = pytesseract.image_to_string(Image.open(image_path))\n",
    "    return text\n",
    "\n",
    "def parse_prescription_details(extracted_text):\n",
    "    medicines, syrups = [], []\n",
    "    \n",
    "    dosage_pattern = r\"(\\d+(\\.\\d+)?\\s?(MG|mg|G|g|Ml|ML|ml|MCG|mcg|Unit|UNIT|unit))\"\n",
    "    frequency_pattern = r\"(\\d+\\s*(x|times)?\\s*(per\\s*day|daily|once|twice|\\d+\\s*times))\"\n",
    "    duration_pattern = r\"(\\d+\\s*(days?|weeks?|months?))\"\n",
    "\n",
    "    medicine_keywords = r\"(tablet|tab|capsule|pill|medicine)\"\n",
    "    syrup_keywords = r\"(syrup|liquid|syp)\"\n",
    "\n",
    "    lines = extracted_text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if re.search(medicine_keywords, line.lower()):\n",
    "            medicines.append(parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, \"medicine\"))\n",
    "        elif re.search(syrup_keywords, line.lower()):\n",
    "            syrups.append(parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, \"syrup\"))\n",
    "\n",
    "    return medicines, syrups\n",
    "\n",
    "def parse_line(line, dosage_pattern, frequency_pattern, duration_pattern, type_of_med):\n",
    "    details = {\"name\": \"\", \"dosage\": \"\", \"frequency\": \"\", \"duration\": \"\", \"type\": type_of_med}\n",
    "    \n",
    "    tokens = line.split()\n",
    "    if tokens:\n",
    "        details[\"name\"] = tokens[0]\n",
    "\n",
    "    details[\"dosage\"] = re.search(dosage_pattern, line).group(0) if re.search(dosage_pattern, line) else \"\"\n",
    "    details[\"frequency\"] = re.search(frequency_pattern, line).group(0) if re.search(frequency_pattern, line) else \"\"\n",
    "    details[\"duration\"] = re.search(duration_pattern, line).group(0) if re.search(duration_pattern, line) else \"\"\n",
    "\n",
    "    return details if any(details.values()) else None\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    image_dir = \"dataset/resized_images\"\n",
    "    label_dir = \"dataset/labels\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    num_classes = 5  # Adjust this based on detection requirements\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((640, 640)),  # YOLOv8 prefers 640x640 images\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = MedReminderDataset(image_dir, label_dir, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Load YOLOv8 model\n",
    "    model = YOLO(\"yolov8s.pt\")  # You can use \"yolov8n.pt\" for a smaller model\n",
    "\n",
    "    # Fine-tune YOLO model\n",
    "    model.train(data=\"dataset/data.yaml\", epochs=num_epochs, imgsz=640)\n",
    "\n",
    "    # Save trained model\n",
    "    model.export(format=\"onnx\")  # Saves as ONNX model\n",
    "    torch.save(model.model.state_dict(), \"mediRem_yolo.pth\")\n",
    "    print(\"Model training complete and saved!\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
